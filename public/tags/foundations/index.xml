<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>foundations | On Call Life</title>
    <link>/tags/foundations/</link>
      <atom:link href="/tags/foundations/index.xml" rel="self" type="application/rss+xml" />
    <description>foundations</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 05 Mar 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>foundations</title>
      <link>/tags/foundations/</link>
    </image>
    
    <item>
      <title>Breaking Down the Components of a Post-incident Review</title>
      <link>/post/breaking-down-the-components-of-a-post-incident-review/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/breaking-down-the-components-of-a-post-incident-review/</guid>
      <description>&lt;p&gt;You now know what a post-incident review is, its role in the incident response
process, and when it should be conducted. In this unit, you’ll dive a little
deeper into the details of what makes a post-incident review most effective.&lt;/p&gt;
&lt;p&gt;Because incidents differ, the exact makeup of post-incident reviews can be
different, too. But there are some common characteristics and components of a
good review that can provide you with a solid foundation for carrying out the
process.&lt;/p&gt;
&lt;h2 id=&#34;what-its-not&#34;&gt;What it’s not&lt;/h2&gt;
&lt;p&gt;Before you can understand the characteristics that make for a good post-incident
review, you should consider what it’s &lt;em&gt;not.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;It’s not a document or report.&lt;/strong&gt; It’s easy to think of a “review” as a
written summary, and indeed, a summary report often follows a post-incident
review. However, these are two different and distinct parts of the Analysis
phase of the incident response lifecycle.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;It’s not a determination of causality.&lt;/strong&gt; Your review will look at the
factors that contributed to the failure, but the purpose isn’t to pinpoint a
culprit. It’s to think about and share information about all aspects of the
incident in order to learn and improve.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;It’s not a list of action items.&lt;/strong&gt; You may end up with such a list as a
result of what you learn in the review, but this isn’t the focus. If you
don’t come away with a stepwise punch list but you do know more about your
systems than before, the review was successful.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The incident review is, more than anything, a &lt;em&gt;conversation.&lt;/em&gt; It’s a defined
space within which your team can review what they knew at the time and what they
know now, and explore and better understand how the parts of the system –
including the human parts – do or don’t work together in response to problems.&lt;/p&gt;
&lt;h2 id=&#34;characteristics-and-components&#34;&gt;Characteristics and components&lt;/h2&gt;
&lt;p&gt;A good incident review is, first and foremost, &lt;em&gt;blameless.&lt;/em&gt; Although you need to
examine how the human parts of the system interacted with it, you don’t do this
in order to label anyone “at fault.” The focus should be on the failures of the
technology and the process, not of the people.&lt;/p&gt;
&lt;p&gt;Frame your questions to reflect this:&lt;/p&gt;
&lt;p&gt;“What was the deficit in your monitoring that failed to give the person at the
keyboard the necessary context to make the right decision?”&lt;/p&gt;
&lt;p&gt;“Why was there a &lt;strong&gt;delete the entire database&lt;/strong&gt; option in the tool that didn’t ask
for confirmation?”&lt;/p&gt;
&lt;p&gt;When things go wrong, it can be tempting to point fingers. However, you must
remember this key point: &lt;em&gt;You can’t fire your way to reliability.&lt;/em&gt; Shaming and
blaming and an investigation that’s aimed at finding and firing the person who
is “responsible” won’t lead to more reliable systems. Instead, it will lead to
an inexperienced operations team and personnel who are afraid to act.&lt;/p&gt;
&lt;p&gt;Approach the review as a search for knowledge and context, not a hunt for who
did what and a reaction to that.&lt;/p&gt;
&lt;p&gt;Although the review is about the failures of the technology, it’s not a
technical process as much as it is a people process. Talk – and more important,
listen – to the people who were involved in the incident. Keep an open mind.
Different people have different perspectives and not everyone will agree, and
that mix of perspectives is invaluable to the learning process.&lt;/p&gt;
&lt;p&gt;A post-incident review is an honest inquiry. As such, it embraces these key
components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Discussion&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Discourse&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dissent&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Discovery&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These “4 Ds” create a framework on which you can build a post-incident learning
review that can result in more reliable systems and more productive teams that
work together.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/running-a-post-incident-review/&#34;&gt;Running a Post-incident Review&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Defining Alerts</title>
      <link>/post/defining-alerts/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/defining-alerts/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&amp;quot;An alert is something which requires a human to perform an action.&amp;rdquo; - Pagerduty 
&lt;a href=&#34;https://response.pagerduty.com/oncall/alerting_principles/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Alerting Principles&amp;rdquo;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To understand why alerting can create a problem, you need to think about the purpose of alerts and how they differ from other monitoring components.&lt;/p&gt;
&lt;p&gt;Actionable alerts are &lt;em&gt;not&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Logs.&lt;/strong&gt; Alerts are not records of events; that’s the role of logs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Notifications.&lt;/strong&gt; Alerts are not intended to announce non-critical
occurrences such as the completion of a software build.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Heartbeats.&lt;/strong&gt; Alerts shouldn’t be used to document failure of a heartbeat
signal periodically sent between two systems at regular intervals.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Actionable alerts are used for situations in which you need a human to
investigate and intervene to remediate the problem. Alerts should be
communications that something exceptional or unexpected has happened that
requires someone’s attention.&lt;/p&gt;
&lt;p&gt;A lot of thought should be put in to how alerts are delivered and when it is necessary. 
&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/azure-monitor/platform/alerts-overview/?wt.mc_id=oncalllife-blog-jahand&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;More on how to configure alerts in Azure can be found here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If the event is something that the system can handle through automated
processes, such as scaling resources within a preset limit, an alert is not
necessary. A simple line in a log should suffice.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/understanding-actionable-alerts&#34;&gt;Understanding Actionable Alerts&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Establishing Communication Channels</title>
      <link>/post/establishing-communications-channel/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/establishing-communications-channel/</guid>
      <description>&lt;p&gt;To address some of the challenges around how we communicate we also want to find a way to create a unique channel or space for engineers to discuss the details of the incident - a “conversation bridge” in our persistent group chat tool -which for Tailwind Traders is Microsoft Teams.&lt;/p&gt;
&lt;p&gt;We want a channel that is unique to the incident only. We do not want conversations about other engineering efforts.We don’t want conversations about what people are doing for lunch. We ONLY want conversations related to the incident. Because then we can take that text (or data) and analyze later in a Post-incident review.&lt;/p&gt;
&lt;h2 id=&#34;next-steps&#34;&gt;Next steps&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;/post/understanding-why-we-learn-from-incidents/&#34;&gt;Understanding Why We Learn From Incidents&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Establishing On-call Roles</title>
      <link>/post/establishing-oncall-roles/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/establishing-oncall-roles/</guid>
      <description>&lt;p&gt;Creating a repeatable response plan means establishing who does what when something goes wrong. We don&amp;rsquo;t want there to be any question around who is supposed to be doing what.&lt;/p&gt;
&lt;p&gt;Because of this, it is important to establish roles and the associated expectations. This isn&amp;rsquo;t a separation of duties exercise. In fact, we want to encourage less of that. It is however, a way of establishing better coordination and communication. It prevents people from stepping on each others toes while enabling cross-collaboration amongst not only on-call rosters, but an entire organization.&lt;/p&gt;
&lt;p&gt;The first role we need to talk about is the &lt;strong&gt;Primary Responder&lt;/strong&gt; – the Primary “On-call” engineer.&lt;/p&gt;
&lt;p&gt;This person is expected to acknowledge their awareness of an incident once the alert has been received.&lt;/p&gt;
&lt;p&gt;Then we have the &lt;strong&gt;secondary responder&lt;/strong&gt; – who is there as back up -Another engineer who can step in if the primary responder is unavailable or unreachable. Or if they just need another pair of eyes.&lt;/p&gt;
&lt;p&gt;Another key role to identify, in many cases, is the &lt;strong&gt;incident commander&lt;/strong&gt;. An incident commander can be extremely helpful when you have got a large-scale outage that effects a lot of different components or requires coordination across many teams and different systems. They are great for making sure that engineers stay focused and they are working on their own remediation efforts&amp;hellip; Ensuring people are not stepping on each other or undoing each other&amp;rsquo;s work.  It is good to have a central person who can track what is going on and who is doing what.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Communication Coordinator&lt;/strong&gt; is meant to be the person working in conjunction with the incident commander to share more information beyond those who are in the firefight actively working to recover from the incident itself. That could be customers. It could be the sales and marketing teams. Maybe your customer support. There are many people within an organization who need to be made aware of what’s taking place and the status around how things are progressing. It&amp;rsquo;s always good to put someone in charge of managing that communication and making sure that other stakeholders are aware of what is happening and what’s being done.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;scribe&lt;/strong&gt;’s role is to document the conversation in as much detail as possible. Teams commonly use phone bridges, conference calls, or video chat to get everyone together and try to understand what is going on, which can certainly help create space for the conversation. However, it is difficult for us to go through and understand in detail what the engineers were saying and doing unless it is transcribed. As a result, a scribe is that person who can help us document as much as possible to review later. What were people saying, doing, feeling, and even experiencing?  It is all data to be analyzed – but only if we capture it.&lt;/p&gt;
&lt;p&gt;It’s quite common within on-call rosters to identify &lt;strong&gt;subject matter experts&lt;/strong&gt;, so that early responders know who to escalate too quickly. These people should not be on call all the time, of course, but we do want to be able to identify who is our database expert. Who is our front-end expert? Who are the people that we can reach out to if our primary and secondary responders are not able to diagnose and resolve the issue themselves?&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s take a closer look at each of these roles to better understand their place within our incident response efforts.&lt;/p&gt;
&lt;h2 id=&#34;next-steps&#34;&gt;Next steps&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;/post/identifying-the-primary-responder/&#34;&gt;Identifying the Primary Responder&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Establishing On-call Rosters</title>
      <link>/post/establishing-oncall-rosters/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/establishing-oncall-rosters/</guid>
      <description>&lt;p&gt;Rosters establish a framework around who is on-call at any given point. A roster, or team, is made up of multiple engineers. Rosters can also contain multiple rotations. I&amp;rsquo;m testing out how to edit a page.&lt;/p&gt;
&lt;h2 id=&#34;next-steps&#34;&gt;Next steps&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;/post/creating-an-oncall-roster-using-azure-table-storage/&#34;&gt;Creating an on-call roster using Azure Table Storage&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Establishing On-call Rotations</title>
      <link>/post/establishing-oncall-rotations/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/establishing-oncall-rotations/</guid>
      <description>&lt;p&gt;People shouldn&amp;rsquo;t have to be &lt;em&gt;on-call&lt;/em&gt; for long periods of time. It&amp;rsquo;s detrimental to their health and therefore the systems they create and look after.&lt;/p&gt;
&lt;p&gt;In order to make sure people aren&amp;rsquo;t expected to respond to problems 24-hours a day, indefinitely, we use rotations. Also referred to as scheduled shifts. That way people can take turns.&lt;/p&gt;
&lt;p&gt;Being on-call requires a heavy cognitive task which then negatively impacts many aspects of a person&amp;rsquo;s life. Shifts allow engineers to take “on-call” responsibility for their own specific recurring rotation and share the load in a way that keeps the human element a priority.&lt;/p&gt;
&lt;p&gt;Site Reliability Engineering is an engineering discipline devoted to helping an organization &lt;strong&gt;sustainably&lt;/strong&gt; achieve the appropriate level of reliability in their systems, services, and products. Long periods of on-call responsibilities is &lt;strong&gt;not&lt;/strong&gt; sustainable.&lt;/p&gt;
&lt;p&gt;When creating shifts there are a number of common approaches.&lt;/p&gt;
&lt;h2 id=&#34;24-x-7&#34;&gt;24 x 7&lt;/h2&gt;
&lt;p&gt;The majority of rotations used by teams are known as 27 x 7 shifts where engineers will be “on-call” for several days in a row. However, most “Elite/High performers” would agree that rotations longer than 3 or 4 days are detrimental to the overall health of engineering staff and therefore the entire system.&lt;/p&gt;
&lt;h2 id=&#34;follow-the-sun&#34;&gt;Follow the Sun&lt;/h2&gt;
&lt;p&gt;Follow the sun shifts are nice for distributed teams. These allow for engineers to schedule their “on-call” shifts only during their normal working office hours. As they end their day and go home, engineers in a different time zone can take over.&lt;/p&gt;
&lt;p&gt;And of course, there are many ways to customize shifts, especially for weekends when engineers need more flexibility. Engineers should be able to hand off the role to someone when personal conflicts arise.&lt;/p&gt;
&lt;p&gt;Once roles, rosters, and rotations have been determined and put in place, we can now focus our attion on methods of 
&lt;a href=&#34;/post/tracking-incident-details/&#34;&gt;Tracking Incident Details&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sending Actionable Alerts</title>
      <link>/post/sending-actionable-alerts/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/sending-actionable-alerts/</guid>
      <description>&lt;p&gt;Alerts play an important role in your reliability monitoring strategy, but in
order to be helpful, they must be properly constructed for situations that
warrant immediate human attention, and they should be devised with simplicity,
scope, and context in mind.&lt;/p&gt;
&lt;p&gt;Preferences on how alerts are delivered can be designed using 
&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/azure-monitor/platform/action-groups/?wt.mc_id=oncalllife-blog-jahand&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Action Groups&lt;/a&gt; in Azure.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://docs.microsoft.com/en-us/azure/azure-monitor/platform/media/action-groups/action-group-define.png&#34; alt=&#34;Action Group&#34;&gt;&lt;/p&gt;
&lt;p&gt;You have learned how to monitor and interact with indicators of
the reliability of your systems and create reliability goals, but there is also
an important way by which reliability interacts with you. That’s through Azure
Monitor’s log alerts feature.&lt;/p&gt;
&lt;p&gt;It’s easy to create log alerts using Azure Monitor where the signal is a log
query in Log Analytics or Application Insights. However, there is a pitfall that
you’ll want to avoid, to prevent derailing all the effort you have put into
bringing SLIs and SLOs into your organization.&lt;/p&gt;
&lt;p&gt;To understand this potential pitfall, review the definition of SRE:&lt;/p&gt;
&lt;p&gt;“Site Reliability Engineering is an engineering discipline devoted to helping
organizations &lt;strong&gt;sustainably&lt;/strong&gt; achieve the appropriate level of reliability in
their systems, services, and products.”&lt;/p&gt;
&lt;p&gt;Alerts are designed to notify you when there is a problem with your systems.
However, when alerts are improperly configured, this can undermine your goal of
sustainability. Log alert rules are stateless; they work only on the logic that
you build into the query and they send an alert whenever the alert condition is
“true.” Thus, it’s important to put some thoughts into constructing your alerts.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/defining-alerts&#34;&gt;Defining Alerts&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Actionable Alerts</title>
      <link>/post/understanding-actionable-alerts/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/understanding-actionable-alerts/</guid>
      <description>&lt;p&gt;To create effective actionable alerts, you must understand their components and
characteristics. Actionable alerts have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Simplicity&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Scope&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Context&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Simplicity is self-explanatory: make your alerts easy for you and others to
understand, even if you’re reading them after being awakened at 2:00 a.m. Scope
and context should be included in the content of the alert.&lt;/p&gt;
&lt;p&gt;Let’s look at some elements that an actionable alert should always include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The source:&lt;/strong&gt; information about where the alert is coming from. Many
organizations have multiple monitoring systems in use at any one time and a
large number of interconnected systems. It can save someone a tremendous
amount of time if the alert says &amp;ldquo;This alert for payroll system thx-1138 is
coming from Azure monitor subscription prod.&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt; information about what expectation has been violated. For
example, &amp;ldquo;This server has been returning an error 30% of the time when it
should have been returning errors less than 1% of the time.&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Impact and scope:&lt;/strong&gt; information about the effect or impact the situation
has had or potentially will have and the scope of that impact (ideally,
stated from the customer’s point of view).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Recommended action:&lt;/strong&gt; if possible, the alert should include what the
person responding should do next, even if that is a pointer to a
troubleshooting guide or some other documentation to find help in diagnosing
and remediating this problem.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Including such helpful context will make operations practices around monitoring
more sustainable and make responding to alerts easier.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/defining-incidents&#34;&gt;Defining Incidents&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding the Foundations of Incident Response</title>
      <link>/post/understanding-the-foundations-of-incident-response/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/understanding-the-foundations-of-incident-response/</guid>
      <description>&lt;p&gt;The foundations of building reliable systems including a good incident response plan, have to start with determining “Who is expected to respond to problems?” and “How do let them know?”.&lt;/p&gt;
&lt;p&gt;The best place to start, is to design what is to establish roles, rosters, and rotations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Roles :&lt;/strong&gt; Well defined responsibilities and expectations of individuals on an on-call team (or roster). The &lt;strong&gt;Primary Responder&lt;/strong&gt;, for example.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Rosters :&lt;/strong&gt; A group of individuals, each with their own assigned role and understood responsibilities and expectations. The mobile &lt;em&gt;&amp;ldquo;on-call&amp;rdquo;&lt;/em&gt; team, consisting of multiple members each with thier own assigned role.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Rotations :&lt;/strong&gt; A scheduled shift for individuals where they are &lt;em&gt;&amp;ldquo;on-call&amp;rdquo;&lt;/em&gt; for a defined period of time. A 24 x n rotation where someone is the** Primary Responder**, for example.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s worth looking at each of these groups a little closer, so let&amp;rsquo;s do that now.&lt;/p&gt;
&lt;h2 id=&#34;next-steps&#34;&gt;Next steps&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;/post/establishing-oncall-roles/&#34;&gt;Establishing On-call Roles&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding the Full Lifecycle of an Incident</title>
      <link>/post/understanding-the-full-lifecycle-of-an-incident/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/understanding-the-full-lifecycle-of-an-incident/</guid>
      <description>&lt;p&gt;If we start to think of incidents as a normal part of a system, then we can also build some formality around the patterns and practices we inevitably see when people instinctually do what they do when something goes wrong.&lt;/p&gt;
&lt;p&gt;From the beginning of a problem to analyzing what and how things happened, we can measure them independantly of each other. By doing so, we can look for improvements in each phase.&lt;/p&gt;
&lt;p&gt;For example, monitoring systems may be working as expected but because an alert was sent to a email distribution group, once people were aware of the alert, most assumed someone else was investigating the problem. The problem persisted for hours.&lt;/p&gt;
&lt;p&gt;An incident can be divided into 5 phases. Detection, response, remediation, analysis, and readiness.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jhandcdn.blob.core.windows.net/blob/LifecycleOfAnIncident.png&#34; alt=&#34;Lifecycle Of An Incident&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Phase 1 - Detection:&lt;/strong&gt;
A problem has been detected through various tooling and practices&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Phase 2 - Response:&lt;/strong&gt;
A coordinated effort to get the right people and tooling in place to diagnose, theorize, and triage.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Phase 3 - Remediation:&lt;/strong&gt;
Efforts made to change the system to either restore service or confirm theories.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Phase 4 - Analysis:&lt;/strong&gt;
Post-incident retrospective exercise to understand the the full the lifecycle of the incident including the human response.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Phase 5 - Readiness:&lt;/strong&gt;
Implementing knew knowledge and changes to improve and shorten the time and effects of future incidents.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s touch on the detection phase just a little more in depth. This is often the best place to start improving your incident response practices. Solid monitoring is the foundation of building reliable systems.&lt;/p&gt;
&lt;h2 id=&#34;next-steps&#34;&gt;Next steps&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;/post/identifying-the-incident-detection-phase/&#34;&gt;Identifying the Detection Phase of an Incident&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>incident | On Call Life</title>
    <link>/tags/incident/</link>
      <atom:link href="/tags/incident/index.xml" rel="self" type="application/rss+xml" />
    <description>incident</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 24 Mar 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>incident</title>
      <link>/tags/incident/</link>
    </image>
    
    <item>
      <title>Distinguishing Prevention and Preparation</title>
      <link>/post/distinguishing-prevention-and-preparation/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/distinguishing-prevention-and-preparation/</guid>
      <description>&lt;p&gt;You’ve probably heard all your life the adage, attributed to Benjamin Franklin,
that “an ounce of prevention is worth a pound of cure.” The accepted meaning is
that it’s better to keep a problem from happening than to fix it after it’s
happened.&lt;/p&gt;
&lt;p&gt;In your efforts to achieve a high level of reliability for your systems and
services, you should do everything possible to prevent incidents from occurring.
However, due to the complexity of those systems, as explained above, prevention
isn’t always possible.&lt;/p&gt;
&lt;p&gt;Thus, you must take a two-pronged approach to failure: prevention on the one
hand, and when that isn’t possible, preparation to respond – quickly and
effectively. These two are interlinked.&lt;/p&gt;
&lt;p&gt;Here’s an example of why it’s essential to do both: Sometimes an organization
will deploy an automated system. It works well. In fact, it works almost too
well – because people may take for granted that it will always work. They don’t
make proper preparation for the day that it doesn’t work. When that day comes –
as it inevitably will, sooner or later – and the system fails, it fails
spectacularly, and it’s much harder for operators to understand what went wrong.&lt;/p&gt;
&lt;p&gt;The systems you work on are made up of more than the technology. In fact, you
don’t work “on” or “with” a system; you work &lt;em&gt;in&lt;/em&gt; the system. You are part of
the system. Complex systems include both technical components (hardware,
software) and human components (people – and their personalities, training, and
knowledge).&lt;/p&gt;
&lt;p&gt;How the humans respond when things go wrong is as important as preventing things
from going wrong in the first place. We learn from failure to respond faster and
better.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/defining-the-post-incident-review/&#34;&gt;Defining the Post-incident Review&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Identifyig the Incident Remediation Phase</title>
      <link>/post/identifying-the-incident-remediation-phase/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/identifying-the-incident-remediation-phase/</guid>
      <description>&lt;p&gt;The remediation phase is the blurriest of them all. A big reason is that sometimes there&amp;rsquo;s no difference between what takes place during the response and an action intended to improve the situation (i.e. remediation step).&lt;/p&gt;
&lt;p&gt;Much of incident response is just trial and error, quite honestly. We quickly think through what to do, we do it, we hope for quick feedback, we examine if things improved, and we iterate.&lt;/p&gt;
&lt;p&gt;Because of this, measuring the remediation phase is a bit trickier.&lt;/p&gt;
&lt;p&gt;What we are looking for is to determine the distinction between when we have &lt;em&gt;identified the fix&lt;/em&gt; (or series of fixes) that will result in recovery of services and the end of the incident. What is the time between? Good or bad, it&amp;rsquo;s data to potentially discuss and discover revelations.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s not until the analysis phase that engineers can definitively determine the exact point along the incident timeline that everyone agrees the problem and solution were both understood.&lt;/p&gt;
&lt;p&gt;This underlines the importance of not only capturing the timeline of events, including conversations and actions taken but also analyzing it in retrospect with a diverse audience encouraged to ask questions. Questions that help radiate a broader and more informed knowledge base across an organization.&lt;/p&gt;
&lt;p&gt;One reason for measuring this way is to set aside the time between definitively knowing what will restore service and when services were actually back.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s say the payment process thing from before was pretty easy to determine. It probably took less than 5 minutes to know it was something with the backend talking to an API and that after someone followed a specific series of steps everything would be fine.&lt;/p&gt;
&lt;p&gt;However, the process to do this is not only complicated and requires administrative access, it&amp;rsquo;s not well documented, and what is documented is extremely dated.&lt;/p&gt;
&lt;p&gt;If this type of problem occurs again, we could shorten the total time of the incident, and therefore cost to the business, simply by making a few small adjustments.&lt;/p&gt;
&lt;p&gt;These types of opportunities begin to surface when we can ask questions like, &amp;ldquo;how long does the backup script take?&amp;quot;. &amp;ldquo;Was the documentation helpful?&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Just because you can figure out what the problem is in an acceptable amount of time, does not mean your system will recover as quickly as the business needs it to.&lt;/p&gt;
&lt;p&gt;Once service is restored and things return to normal it&amp;rsquo;s important to set aside time to reflect on what took place, discuss it openly, broadcast what has been learned, and prepare for the future.&lt;/p&gt;
&lt;p&gt;This takes us to our next phase of the incident lifecycle - analysis.&lt;/p&gt;
&lt;h2 id=&#34;next&#34;&gt;Next&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;/post/identifying-the-incident-analysis-phase/&#34;&gt;Identifying the Analysis Phase of an Incident&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Identifying the Incident Detection Phase</title>
      <link>/post/identifying-the-incident-detection-phase/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/identifying-the-incident-detection-phase/</guid>
      <description>&lt;p&gt;The methods used to determine when we have a problem have changed over the years.&lt;/p&gt;
&lt;p&gt;Alerting a person to a spike in CPU usage isn&amp;rsquo;t as valuable these days. Especially those in the process of adopting the cloud. Instead, we want to know when our customer is experiencing a problem while using our system.&lt;/p&gt;
&lt;p&gt;The problems will vary but the methods used to determine when a human needs to get involved have evolved.&lt;/p&gt;
&lt;p&gt;By monitoring systems in a way that matches the customer&amp;rsquo;s perspective we can see when they experience a problem rather than we &lt;em&gt;think&lt;/em&gt; we experienced a problem.&lt;/p&gt;
&lt;p&gt;If the customer is experiencing an issue, that&amp;rsquo;s far more important to the business than any spike in CPU usage.&lt;/p&gt;
&lt;p&gt;In today&amp;rsquo;s connected world, no matter how complex or simple a system appears to be there is much more that goes in to what a user experiences.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s entirely possible that all systems appear healthy and no alarms are going off when in reality users aren&amp;rsquo;t able to complete a shopping purchase due to a third party payment processor. No amount of monitoring for memory or network performance would have tipped off engineers or leadership to this business impacting problem.&lt;/p&gt;
&lt;p&gt;Every system is different and while there may be legitimate reasons to set up alerts for problems at the component level. However, by and large if we are planning to get engineers involved (especially outside of office hours) then we need to make sure the problem is real, it&amp;rsquo;s impacting the business, and it requires human intervention immediately.&lt;/p&gt;
&lt;p&gt;If an alert isn&amp;rsquo;t actionable - meaning it requires a person or group of people to respond and investigate right away then it&amp;rsquo;s not an incident.&lt;/p&gt;
&lt;p&gt;If we can measure some minor details about when amd how we detect problems in the first place then we can look for opportunities to improve.&lt;/p&gt;
&lt;p&gt;In conversations about what took place with the payment processor incident it is reasonable to ask &amp;ldquo;how could we have detected this sooner?&amp;quot;.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;How could we have detected this &amp;hellip; at all?&amp;rdquo; may be a better question.&lt;/p&gt;
&lt;h2 id=&#34;next-steps&#34;&gt;Next steps&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;/post/identifying-the-incident-response-phase/&#34;&gt;Identifying the Response Phase of an Incident&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Identifying the Incident Readiness Phase</title>
      <link>/post/identifying-the-incident-readiness-phase/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/identifying-the-incident-readiness-phase/</guid>
      <description>&lt;p&gt;During and after a post-incident review many ideas will surface around how to improve not only various aspects of each phase of the lifecycle but also how the team can improve in other areas. Communication for example.&lt;/p&gt;
&lt;p&gt;During the review, engineers might have pointed out that there were long gaps in the conversation timeline where nobody said anything. It&amp;rsquo;s helpful to be verbose in what engineers are doing, thinking, even feeling. If someone isn&amp;rsquo;t completely comfortable following the steps from documentation, we should address that. Who else on the team carries fears about performing actions on the system during an incident? We want our team to be confident and ready.&lt;/p&gt;
&lt;p&gt;So, what did we learn that helps improve that readiness?&lt;/p&gt;
&lt;p&gt;Action items aren&amp;rsquo;t really the point of a post-incident reivew but inevitably, creative ideas will emerge. Some engineering efforts will make sense to schedule and implement as a result of the conversations. Adding telemetry to help keep a better eye on the credit card processing system, for example.&lt;/p&gt;
&lt;p&gt;Product and engineering teams should work together to prioritize and schedule work for those enhancements. Tradeoffs will be made since the uptime of a new feature is just as important as the feature itself. Ultimately, what&amp;rsquo;s best for the business is what leadership will have to wrestle with.&lt;/p&gt;
&lt;p&gt;The bigger win that helps with our readiness efforts is that we have measurements by which we can set goals around.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/understanding-the-foundations-of-incident-response/&#34;&gt;Understanding the Foundations of Incident Response&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Identifying the Response Phase of an Incident</title>
      <link>/post/identifying-the-incident-response-phase/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/identifying-the-incident-response-phase/</guid>
      <description>&lt;p&gt;Once our detection efforts have been configured to send actionable alerts to the people who build the systems, we need to make sure they are sending those alerts to the &lt;em&gt;right&lt;/em&gt; people.&lt;/p&gt;
&lt;h2 id=&#34;right-people&#34;&gt;Right People&lt;/h2&gt;
&lt;p&gt;How do you know who the right people are? In most cases it is situational. A few things that can be done to help establish some formatlity and standard around responding to incidents is through the use of roles, rosters, and rotations. We&amp;rsquo;ll go more in depth on what each of those are soon.&lt;/p&gt;
&lt;h2 id=&#34;tooling&#34;&gt;Tooling&lt;/h2&gt;
&lt;p&gt;The right person for the job needs the right tools for the job. If someone is responding to an issue they need to get busy immediately. Making sure the right monitoring, communications, access, and documentation is provided is also important. People should be familiar with the tooling and know how and where to find additional resources to help diagnose, theorize, and triage.&lt;/p&gt;
&lt;h3 id=&#34;diagnose&#34;&gt;Diagnose&lt;/h3&gt;
&lt;p&gt;Everyone experiences problems. Sometimes routinely throughout the day in fact. When something doesn&amp;rsquo;t go as expected or breaks entirely our impulse is to fix it. In order to do so we must first have a look at what&amp;rsquo;s currently observable. What is the status? Who and what is impacted? What hints or clues are there? What information do we have to work with?&lt;/p&gt;
&lt;p&gt;What do we know &lt;em&gt;right now?&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;theorize&#34;&gt;Theorize&lt;/h3&gt;
&lt;p&gt;Once information has been obtained, we begin to theorize next best steps.
What action can we take to minimize or stop the impact? What are the repurcussions of that action? Will something else go wrong? If we take one action, what result do we expect? In very brief moments we are creatively thinking through as many possible scenarios to restore service as we can. And then stack ranking them based on our own calculations on the probability of success.&lt;/p&gt;
&lt;h3 id=&#34;triage&#34;&gt;Triage&lt;/h3&gt;
&lt;p&gt;At some point we all need help. That could be access to an admin account, theories from subject matter experts, someone to amplify updates to a broader audience. Rarely are incidents viewed as a success if only a single person was involved.&lt;/p&gt;
&lt;p&gt;Regardless of the size of your response team, by isolating it as a phase in the incident lifecycle, we can examine this section of the timeline for improvements on how we coordinate our response. If it took an excessive amount of time for the engineering team to correct the payment processor problem simply because it took too long to find the right person, with the right tool, and with the appropriate level of access then there are some clear opportunities for improvement right there.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/identifying-the-incident-remediation-phase/&#34;&gt;Identifying the Remediation Phase of an Incident&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Improving the Remediation of Incidents</title>
      <link>/post/improving-the-remediation-of-incidents/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/improving-the-remediation-of-incidents/</guid>
      <description>&lt;p&gt;Although thinking of incidents in terms of phases allows for us to shorten each in their own unique ways, responding to and remediating an incident often begin to blur. Especially when actions to mitigate or improve the situation, have the opposite result.&lt;/p&gt;
&lt;p&gt;Now that we’ve covered the foundations of building a good incident response plan, let&amp;rsquo;s talk about remediation efforts and how 
&lt;a href=&#34;/post/supplying-context-and-guidance/&#34;&gt;Supplying Context &amp;amp; Guidance&lt;/a&gt; to on-call engineers rather than step by step procedures can dramatically help reduce the impact of an incident.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Reviewing Azure&#39;s Monitoring Tools</title>
      <link>/post/reviewing-azures-monitoring-tools/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/reviewing-azures-monitoring-tools/</guid>
      <description>&lt;p&gt;Azure Monitor includes a number of features and tools you can use to monitor and
measure reliability, using data from many different sources.&lt;/p&gt;
&lt;p&gt;This article’s focus is to help you keep your “eyes on the prize” – the goal of
improving reliability in your organization. You should now understand what
reliability is and why it’s important. You know that operational awareness is an
essential precursor to monitoring for reliability, and that you need to
establish a baseline of “normal” behavior as a first step.&lt;/p&gt;
&lt;p&gt;Now you’ll look at the practical question of how to go about doing that. As with
all complex tasks, the right tools can help you get the job done more quickly
and easily. In this unit, you’ll get a quick introduction to Azure Monitor,
Application Insights, and Log Analytics.&lt;/p&gt;
&lt;h2 id=&#34;azure-monitor&#34;&gt;Azure Monitor&lt;/h2&gt;
&lt;p&gt;Azure Monitor is a comprehensive platform for monitoring Azure resources to gain
insights into your applications, infrastructure, and network. In this module,
the focus will be on the Azure Monitor tools that you can use to monitor and
measure reliability.&lt;/p&gt;
&lt;p&gt;A look at Azure Monitor starts with the data that comes into the system. Azure
Monitor takes in data from a number of different sources. These include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Data from applications&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Data from the various operating systems running in Azure&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Information fed from Azure resources, subscriptions, and tenants&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Custom data of any sort and from any source&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are many things we can do with that information: analyze it, visualize it,
integrate it, and respond to it when things go wrong.&lt;/p&gt;
&lt;h2 id=&#34;data-types&#34;&gt;Data types&lt;/h2&gt;
&lt;p&gt;The data that comes into Azure Monitor can be divided into two types:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Metrics:&lt;/strong&gt; small numerical pieces of information from counters, gauges,
and so forth that are collected on a regular basis&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Log data:&lt;/strong&gt; information gathered from many different logs such as Windows
event logs, Linux syslog, agents running on virtual machines, custom logs,
telemetry from Application Insights, and more.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this module, we will be focusing primarily on log data.&lt;/p&gt;
&lt;p&gt;You can create, import, and export dashboards, using JSON files, to help you
better visualize the data and provide you with operational awareness about all
aspects of your Azure subscription, including reliability information. You can
also set access controls on the dashboards and share them with others.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/examining-common-traps/&#34;&gt;Examining Common Traps&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Running a Post-incident Review</title>
      <link>/post/running-a-post-incident-review/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/running-a-post-incident-review/</guid>
      <description>&lt;p&gt;We want to develop a sense making approach to the present situation in order to act upon it better in the future&amp;hellip; in the moment.&lt;/p&gt;
&lt;p&gt;There are different ways to conduct a useful post-incident review, but there are
some common practices that can make the process easier and more effective. If
you run a facilitated review meeting, keep your review and planning meetings
separate, ask better questions, and identify how things went right, you can
learn more and use what you learn to improve the reliability of your systems,
services, and processes.&lt;/p&gt;
&lt;p&gt;Now you’re aware of some of the common pitfalls that can sabotage your
post-incident review, and you know what &lt;em&gt;not&lt;/em&gt; to do. The next logical question
is “what should you do instead?”&lt;/p&gt;
&lt;p&gt;In this unit, you’ll learn about four helpful practices that can make your
response team better and improve the post-incident analysis process.&lt;/p&gt;
&lt;h2 id=&#34;practice-1-run-a-facilitated-post-incident-review&#34;&gt;Practice 1: Run a facilitated post-incident review&lt;/h2&gt;
&lt;p&gt;You already know that a post-incident review is not a document or report, so it
follows that having one person write up a “postmortem” of what happened doesn’t
make for a helpful post-incident review. No matter how knowledgeable or how
deeply involved in the incident that person might be, not much will be learned
if everything comes from a single viewpoint.&lt;/p&gt;
&lt;p&gt;Getting those who were involved in the incident together at the same time is the
first step. But there is something else you can do to increase the useful
learning that comes out of the review even more.&lt;/p&gt;
&lt;p&gt;A &lt;em&gt;facilitator&lt;/em&gt; is someone or something that makes an action or process easier.
To make the post-incident review process both easier and more effective, you
should plan to have a post-incident review meeting with a &lt;em&gt;neutral facilitator&lt;/em&gt;
– someone who was not involved in the response to this incident.&lt;/p&gt;
&lt;p&gt;Everyone will learn more if the facilitator cannot be seen to have any
preconceived ideas or a personal agenda in telling the story of the incident.&lt;/p&gt;
&lt;p&gt;The exact format for the meeting will depend on your team, scheduling, and the
nature of the incident, but here are some more basic guidelines:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Meetings, not marathons:&lt;/strong&gt; the meetings don’t have to be long. Generally,
sixty to ninety minutes is the maximum length of time most people can fully
concentrate and participate effectively, so limit the meeting to not more
than an hour and a half.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Pre&lt;/strong&gt;-&lt;strong&gt;meeting prep:&lt;/strong&gt; to make better use of the meeting time, it can be
helpful for the facilitator to prepare by conducting one-to-one interviews
with some of the members of the response team to get an overview of the
incident and ideas about which topics to talk about in the meeting.
Individual interviews are also appropriate with certain personalities, when
presenting in front of the room is a cause for discomfort.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Not required for every incident&lt;/strong&gt;: this is a learning process, and you’re
“learning to learn,” so start small. You don’t have to do this for every
incident. You can pick and choose. You might want to start with smaller
incidents or start with a review meeting only once per month.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The post-incident review meeting is an opportunity to find out what went wrong,
what was done right, and how failures can be handled better in the future. The
ultimate goal is to improve reliability.&lt;/p&gt;
&lt;h2 id=&#34;practice-2-ask-better-questions&#34;&gt;Practice 2: Ask better questions&lt;/h2&gt;
&lt;p&gt;You already know that language matters, and in the post-incident review, this
applies especially to the questions you ask. Objective questions will usually
elicit more useful answers.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In particular, it’s better to ask people “how” or “what” instead of “why.”&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;When people are asked to explain “why” they did something or “why” something
happened, it tends to put them on the defensive. Beginning a question with “why”
often comes across as a judgement, criticism, or accusation. It forces people to
justify their actions, and people don’t always know why they did something, or
why something happened as a result of their actions.&lt;/p&gt;
&lt;p&gt;This doesn’t mean you can’t explore the reasons the incident occurred or the
reasoning a person used to decide what to do in response to it. It just means
you should pay attention to how you word those questions:&lt;/p&gt;
&lt;p&gt;Don’t ask “why did you do that?”&lt;/p&gt;
&lt;p&gt;Instead, ask “&lt;em&gt;what factored into your decision to make that change?&lt;/em&gt;”&lt;/p&gt;
&lt;p&gt;Don’t ask “why wasn’t this caught in canary?”&lt;/p&gt;
&lt;p&gt;Instead, ask “&lt;em&gt;how effective is canary at catching this sort of issue,
usually?&lt;/em&gt;”&lt;/p&gt;
&lt;p&gt;Remember that the post-incident review is about learning. Each participant in
the incident is likely to have had a slightly different view on events. You’ll
learn more if you ask questions that expose these multiple views and
interpretations.&lt;/p&gt;
&lt;p&gt;Don’t get tunnel vision. Sometimes focusing too narrowly on the incident in
question will cause you to miss valuable information. You will often learn as
much by asking about how work “normally” happens as you will by asking about the
specific incident.&lt;/p&gt;
&lt;p&gt;To learn more about how to ask better questions, check out this resource:&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://extfiles.etsy.com/DebriefingFacilitationGuide.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Etsy Debriefing Facilitation
Guide&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;practice-3-ask-how-things-went-right&#34;&gt;Practice 3: Ask how things went right&lt;/h2&gt;
&lt;p&gt;When you think about learning from failure, you may forget that even within a
major outage or other incident, in addition to the things that go wrong there
are also things that go right. Far from our view of incidents as one-offs or
products of extreme conditions, in most complex systems things go wrong for many
of the same reasons they go right.&lt;/p&gt;
&lt;p&gt;It’s human nature to focus your questions on the negative side of the equation.
However, also asking about how things went right will provide you with insights
that you would not have gotten otherwise.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Don’t just ask how the outage happened.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Ask how your team recovered the systems.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;You want to know what insights, tools, skills, and people helped in the recovery
effort. These are things you want to be able to reproduce, so this information
will be valuable in planning what to do going forward.&lt;/p&gt;
&lt;p&gt;In this context, you want to ask how people came to know what they knew and on
what basis they made the decisions they made. Was there a critical moment when
someone shared a piece of information that helped unlock the puzzle of what was
happening? How did they know to do that? Where did the information come from?&lt;/p&gt;
&lt;p&gt;Look for themes and patterns. Finally, as part of recognizing what went right,
ask &lt;em&gt;What do you know now that you didn’t know previously?&lt;/em&gt; If learning resulted
from the incident and from the incident response and review processes, that’s
another thing that went right.&lt;/p&gt;
&lt;h2 id=&#34;practice-4-keep-review-and-planning-meetings-separate&#34;&gt;Practice 4: Keep review and planning meetings separate&lt;/h2&gt;
&lt;p&gt;After you’ve resolved the immediate incident, you’re naturally going to want to
talk about repair items and future mitigation – and you should – but these
topics should &lt;em&gt;not&lt;/em&gt; be a part of your post-incident review meeting. Your
post-incident review meeting has a purpose and allowing the discussion of repair
items into that meeting will distract from that purpose.&lt;/p&gt;
&lt;p&gt;The best plan is to discuss repair items and planning issues in a separate
meeting a day or two after your post-incident review. You might want to do this
with a smaller group. Both of the meetings will be more productive as a result
of this separation.&lt;/p&gt;
&lt;p&gt;It will help you keep the focus of the post-incident review where it belongs, on
what actually happened. In addition, allowing a day or two of “soak time” will
result in better “synthesis,” giving your subconscious time to work on the
issues. This will help you to identify the most “energy-efficient” repair items
– those that require minimum energy for maximum impact.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/gathering-data-for-the-post-incident-review/&#34;&gt;Gathering Data for the Post-incident Review&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding the Full Lifecycle of an Incident</title>
      <link>/post/understanding-the-full-lifecycle-of-an-incident/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/understanding-the-full-lifecycle-of-an-incident/</guid>
      <description>&lt;p&gt;If we start to think of incidents as a normal part of a system, then we can also build some formality around the patterns and practices we inevitably see when people instinctually do what they do when something goes wrong.&lt;/p&gt;
&lt;p&gt;From the beginning of a problem to analyzing what and how things happened, we can measure them independantly of each other. By doing so, we can look for improvements in each phase.&lt;/p&gt;
&lt;p&gt;For example, monitoring systems may be working as expected but because an alert was sent to a email distribution group, once people were aware of the alert, most assumed someone else was investigating the problem. The problem persisted for hours.&lt;/p&gt;
&lt;p&gt;An incident can be divided into 5 phases. Detection, response, remediation, analysis, and readiness.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jhandcdn.blob.core.windows.net/blob/LifecycleOfAnIncident.png&#34; alt=&#34;Lifecycle Of An Incident&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Phase 1 - Detection:&lt;/strong&gt;
A problem has been detected through various tooling and practices&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Phase 2 - Response:&lt;/strong&gt;
A coordinated effort to get the right people and tooling in place to diagnose, theorize, and triage.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Phase 3 - Remediation:&lt;/strong&gt;
Efforts made to change the system to either restore service or confirm theories.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Phase 4 - Analysis:&lt;/strong&gt;
Post-incident retrospective exercise to understand the the full the lifecycle of the incident including the human response.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Phase 5 - Readiness:&lt;/strong&gt;
Implementing knew knowledge and changes to improve and shorten the time and effects of future incidents.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s touch on the detection phase just a little more in depth. This is often the best place to start improving your incident response practices. Solid monitoring is the foundation of building reliable systems.&lt;/p&gt;
&lt;h2 id=&#34;next-steps&#34;&gt;Next steps&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;/post/identifying-the-incident-detection-phase/&#34;&gt;Identifying the Detection Phase of an Incident&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Why We Learn From Incidents</title>
      <link>/post/understanding-why-we-learn-from-incidents/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/understanding-why-we-learn-from-incidents/</guid>
      <description>&lt;p&gt;Incident response doesn’t stop when the incident is over. They say those who
don’t study history are doomed to repeat it. Likewise, those who don’t study,
analyze, and learn from the incidents they resolved are doomed to keep repeating
the process, as well.&lt;/p&gt;
&lt;p&gt;Your most important means of learning from incidents is the post-incident
review.&lt;/p&gt;
&lt;p&gt;When an incident occurs, your first reaction probably isn’t, “Hurray – a
learning opportunity!” Your immediate priority is figuring out what went wrong
and fixing it as quickly as possible, to reduce the impact on your customers and
end users – as it should be.&lt;/p&gt;
&lt;p&gt;However, once the incident has been resolved, it’s important to follow up and
benefit from the discovery of whatever mistakes or circumstances led to the
failure. Doing so will help you prevent the same thing from happening again and
will also help you understand what tactics do and don’t work best when
responding to any kind of incident in the future.&lt;/p&gt;
&lt;p&gt;The post-incident review is part of the analysis phase of the incident response
lifecycle. Not all post-incident reviews are created equal. There are different
ways to approach the process, and too much focus on certain aspects of the
problem or framing questions in the wrong way can reduce the value of the
review.&lt;/p&gt;
&lt;p&gt;We want to now explore the best path towards generating actionable introspection about the systems we co-create.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/understanding-how-complex-systems-fail/&#34;&gt;Understanding How Complex Systems Fail&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

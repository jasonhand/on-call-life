[{"authors":["admin"],"categories":null,"content":"Jason Hand is an author and speaker on the subjects of site reliability engineering, incident management, post-incident analysis, and chatops. Co-organizer and supporter of several tech communities including DevOpsDays Rockies, Jason enjoys connecting story tellers and actionable ideas with those who are hungry to learn. Jason also loves to bring together ideas and expertise around building communities in tech through his podcast “Community Pulse”.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Jason Hand is an author and speaker on the subjects of site reliability engineering, incident management, post-incident analysis, and chatops. Co-organizer and supporter of several tech communities including DevOpsDays Rockies, Jason enjoys connecting story tellers and actionable ideas with those who are hungry to learn. Jason also loves to bring together ideas and expertise around building communities in tech through his podcast “Community Pulse”.","tags":null,"title":"Jason Hand","type":"authors"},{"authors":null,"categories":null,"content":"Also referred to as Workbooks in Azure Monitor, interactive reports can be quickly built to provide critical information to first responders as they begin to investigate and take early remediation efforts.\nGuides can include data from sources such as logs, metrics, and more through visualizations like charts, grids, graphs, and text.\nEverything displayed in the troubleshooting guide can be customized on the fly through parameters making it interactive.\nThe name Troubleshooting Guide is specific to the workbooks made available through Azure\u0026rsquo;s Application Insights. To view a gallery of templates or to create a new guide, select Troubleshooting Guide from the Investigate section of the left navigation pane.\nTroubleshooting guides are ultimately just JSON files, which means they can be handled just like code. You can put them in a repository right next to your code, infrastructure, deployment pipelines, and more.\nTo view an example of a Troubleshooting Guide represented in JSON, take a look at this sample.\nTroubleshooting Guides can often provide the jumpstart first responders need to begin diagnosing. By freeing up their cognitive tasks for something other than remembering where resources are, engineers can make a positive difference faster.\nSupplying context and guidance rather than rigid step-by-step processes allows engineers to be creative in novel situations. If an engineer can follow each step of a check list to recover from the problem then it is time to automate that \u0026ldquo;known\u0026rdquo; fix and free up incident response for scenarios that couldn\u0026rsquo;t have been predicted or fixed through a linear process.\nKeeping people informed is a big driving force behind efforts during incident response. Troubleshooting guides are no exception. When engineers (also stakeholders in the incident) are informed with objective data, they can take appropriate action based on information they have available to them.\n Updating Stakeholders can be done in many ways. Let\u0026rsquo;s take a look at a few examples.\n","date":1585785600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585785600,"objectID":"967d0c6d78b89a2030798a7d2c9ea31d","permalink":"/post/building-troubleshooting-guides-in-azure-monitor/","publishdate":"2020-04-02T00:00:00Z","relpermalink":"/post/building-troubleshooting-guides-in-azure-monitor/","section":"post","summary":"Also referred to as Workbooks in Azure Monitor, interactive reports can be quickly built to provide critical information to first responders as they begin to investigate and take early remediation efforts.\nGuides can include data from sources such as logs, metrics, and more through visualizations like charts, grids, graphs, and text.\nEverything displayed in the troubleshooting guide can be customized on the fly through parameters making it interactive.\nThe name Troubleshooting Guide is specific to the workbooks made available through Azure\u0026rsquo;s Application Insights.","tags":["troubleshooting","guide","context","runbook","azure monitor"],"title":"Building Troubleshooting Guides in Azure Monitor","type":"post"},{"authors":null,"categories":null,"content":"For the most part, incidents are unique.\nTherefore the lessons learned will vary from problem to problem. However, it\u0026rsquo;s helpful to spot trends in response efforts to both identify what is working and what needs improvement.\nIt\u0026rsquo;s also helpful for engineering teams to have a sense of how frequent problems are arising and how quickly they are addressed and resolved.\nWhen tracking incidents using Azure Boards, it\u0026rsquo;s quite simple to build reports and charts provide a high level snapshot of incident management efforts.\nCreate a Query 1. Select the Queries option in the left navigation 2. Customize the query\nSelect the clauses, fields, and values of interest to easily build queries and report back information such as \u0026ldquo;Show me ANY incident\u0026rdquo; in this example. You\u0026rsquo;ll see the results at the bottom of the screen. In our case we only have two incidents. One is resolved and one is acknowledged.\n3. Save the query\nPress the Save query menu option and give it a name such as \u0026ldquo;All Incidents\u0026rdquo;.\nCreate a Graph To create visualizations of the queries, begin by changing to the Charts option.\n1. Click on Charts 2. Click on New Charts 3. Configure the chart\nChoose the type of chart you want to use (Pie in this example). Give the chart a name, such as \u0026ldquo;All Incidents - Chart\u0026rdquo;. Choose how you want the data grouped, like \u0026ldquo;State\u0026rdquo; (New Incident, Resolved, Acknowledged, etc.)\nChoose a color to represent the data and press Ok. Now you have a high level visual to communicate the state of incidents.\nNow that we\u0026rsquo;ve discussed ways to help track the response of an incident, let\u0026rsquo;s take a moment to discuss ways of Improving the Remediation of Incidents.\n","date":1585008000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585008000,"objectID":"26ff8b5c1d92f4a9d6fa63ea229a4960","permalink":"/post/creating-custom-incident-reports-and-charts-in-azure-boards/","publishdate":"2020-03-24T00:00:00Z","relpermalink":"/post/creating-custom-incident-reports-and-charts-in-azure-boards/","section":"post","summary":"For the most part, incidents are unique.\nTherefore the lessons learned will vary from problem to problem. However, it\u0026rsquo;s helpful to spot trends in response efforts to both identify what is working and what needs improvement.\nIt\u0026rsquo;s also helpful for engineering teams to have a sense of how frequent problems are arising and how quickly they are addressed and resolved.\nWhen tracking incidents using Azure Boards, it\u0026rsquo;s quite simple to build reports and charts provide a high level snapshot of incident management efforts.","tags":["incidents","tracking","azure","boards","reports","charts"],"title":"Creating Custom Incident Reports and Charts in Azure Boards","type":"post"},{"authors":null,"categories":null,"content":"    Understanding Service Level Objectives (SLOs)\n","date":1584316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584316800,"objectID":"bade3e0743bb04636fe95f7adc4b27c9","permalink":"/post/building-service-level-indicators-with-log-analytics/","publishdate":"2020-03-16T00:00:00Z","relpermalink":"/post/building-service-level-indicators-with-log-analytics/","section":"post","summary":"Understanding Service Level Objectives (SLOs)","tags":["measurements","reliability","sli","service","log analytics"],"title":"Building Service Level Indicators With Log Analytics","type":"post"},{"authors":null,"categories":null,"content":"    Sending Actionable Alerts\n","date":1584316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584316800,"objectID":"1c2f4a60258ca014e17ab9cccd903647","permalink":"/post/building-service-level-objectives-with-log-analytics/","publishdate":"2020-03-16T00:00:00Z","relpermalink":"/post/building-service-level-objectives-with-log-analytics/","section":"post","summary":"Sending Actionable Alerts","tags":["measurements","reliability","slo","service","log analytics"],"title":"Building Service Level Objectives With Log Analytics","type":"post"},{"authors":null,"categories":null,"content":"On-call rosters allow teams to identify who is responsible for acknowledging and addressing incidents as they occur.\nThey are made up of the names and contact information of everyone expected to take part in the response and remediation of service disruptions.\nOn-call Roster\n   Name Email Service On-call     Jason Hand jason@xyz.com API Yes   Chris Smith chris@xyz.com API No   Lauren Jones lauren@xyz.com Mobile Yes   Ryan Boggs ryan@xyz.com Database Yes    Depending on the make up of your teams and services, on-call rosters can remain quite simple or become extremely complex.\nOne way of creating an on-call roster is with a basic storage table in Azure.\nSetup  First, login or create a free Azure account.   1. Create a new resource From the home screen in Azure, select the option to create a resource. 2. Search for and select \u0026ldquo;Storage Account\u0026rdquo;\n3. Select a subscription, resource group, etc.\nChoose where you want the storage account created, what name you want to give it, as well as the type of disk and access tiers and then choose Review \u0026amp; Create, followed by Create.\n4. Go to the (Storage Account) resource\n5. Click on the Tables card 6. Click the + Table option and give it a name of oncall\n7. Click on Storage Exploror\n8. Click on Tables, then oncall, then + Add 9. Add entity\nThe PartitionKey field will remain the same. Add new properties for name, email, service, and oncall. All properties should be set to a string type, except for oncall. It is a boolean. Once all fields have been added, press Insert.\n10. Repeat the process for new entities. Make sure to use the same PartitionKey but a unique rowkey. Also be sure to use true or false for the oncall field.\nAfter a couple of entries, it should look as follows. That\u0026rsquo;s it. You\u0026rsquo;ve taken your first steps towards building a basic on-call roster. This will help us identify who to initially alert when an incident occurs.\n This example roster tracks only the \u0026ldquo;Primary Responder\u0026rdquo; (i.e. oncall=true or false). It doesn\u0026rsquo;t include any alternative contact information. Nor does it identify what rotations someone is associated with. Try expanding your roster to contain more of the roles previously discussed.   Next, we will take a look at Establishing On-call Rotations\n","date":1584057600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584057600,"objectID":"414184d4912ce055bbba46b248a3e695","permalink":"/post/creating-an-oncall-roster-using-azure-table-storage/","publishdate":"2020-03-13T00:00:00Z","relpermalink":"/post/creating-an-oncall-roster-using-azure-table-storage/","section":"post","summary":"On-call rosters allow teams to identify who is responsible for acknowledging and addressing incidents as they occur.\nThey are made up of the names and contact information of everyone expected to take part in the response and remediation of service disruptions.\nOn-call Roster\n   Name Email Service On-call     Jason Hand jason@xyz.com API Yes   Chris Smith chris@xyz.com API No   Lauren Jones lauren@xyz.","tags":["on-call","roster","azure","table storage"],"title":"Creating an On-call Roster Using Azure Table Storage","type":"post"},{"authors":null,"categories":null,"content":"Now that we have a tool to track the incident details, we need to ensure we are tracking all of the important aspects. Such as When did we know about the problem and more.\nLet\u0026rsquo;s now take a look at how we can customize Azure Boards to track additional incident details.\nWhen did we know? When a new record (or incident) is created in Azure Boards we will automatically have the date and time as well as a change log throughout the incident\u0026rsquo;s lifecycle\u0026hellip;\nHowever, it would be good to have a separate field where we can also capture when our systems were detecting the issue. It is possible that while no alerts were triggered for humans to respond, the data was there if only we were monitoring and alerting on it.\nHow did we know? The default fields do not provide anything suitable to capture this information. We will need to create a new field to start tracking it.\nWho is aware? Using the Assigned To field we can attempt to communicate some awareness. If the first responder updates the State (to Acknowledged), and sets the Assigned To field to themself, it acknowledges to others that someone has been alerted and is investigating.\nWhat is the impact? When possible, it\u0026rsquo;s best to include context such as \u0026ldquo;customer impacting\u0026rdquo; in the alert itself. This allows immediate contextual awareness and a sense of urgency. We can create a new field to track the official severity level.\nWhat is being done? Naturally, anyone looking at the details of the incident are going to want to know what is being done. The description and discussion fields provide great places for responders or others assisting in the efforts to update the larger audience on what is being done and what expectations to have regarding future updates.\nHowever, real-time updates on what is taking place can be found within the chat channel where engineers are collaborating during the response.\nIt would be useful to provide a link or information for others to follow if they would like to observe the conversation as it unfolds. We will need to create a field specifically related to the conversation.\nLet\u0026rsquo;s now customize Azure Boards to satisfy our basic needs of tracking an incident.\nCreating New Fields 1. Click the \u0026ldquo;Customize\u0026rdquo; option under the (\u0026hellip;) in the upper right.\nBefore we create new fields, let\u0026rsquo;s first make changes to the \u0026ldquo;State\u0026rdquo; of incidents to better align with industry terminology.\n2. Click on + New State\nDelete the existing ones and create new states that better align with terminology used. Such as Acknowledged (ACK), Escalated, etc.\n3. Click on Layout\nThen add new fields using the button below\n4. Confirm changes\nWhen you return to the incident, you\u0026rsquo;ll notice a warning message pointing out that the current state of \u0026ldquo;To do\u0026rdquo; does not match the existing options.\nYou\u0026rsquo;ll also see the new fields that we have added (Time to detection, source of alert, etc.)\nThis should be a good place to start capturing information that will be helpful and important to analyze in a post-incident review later.\nWith the information tracked in Azure Boards along with the conversations taking place in Microsoft Teams we will have a lot of great data to analyze.\nFrom that analysis we will measure and establish baselines and expectations around incident response.\nIt\u0026rsquo;s important to discuss measuring incident response now. However, on our path to continuous improvement there are certain traps we want to avoid when examining incidents, especially in aggregate.\n","date":1584057600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584057600,"objectID":"31fc032cc158de2b92eb66b8c0d8f995","permalink":"/post/customizing-azure-boards-for-incident-tracking/","publishdate":"2020-03-13T00:00:00Z","relpermalink":"/post/customizing-azure-boards-for-incident-tracking/","section":"post","summary":"Now that we have a tool to track the incident details, we need to ensure we are tracking all of the important aspects. Such as When did we know about the problem and more.\nLet\u0026rsquo;s now take a look at how we can customize Azure Boards to track additional incident details.\nWhen did we know? When a new record (or incident) is created in Azure Boards we will automatically have the date and time as well as a change log throughout the incident\u0026rsquo;s lifecycle\u0026hellip;","tags":["incidents","tracking","azure","boards"],"title":"Customizing Azure Boards for Incident Tracking","type":"post"},{"authors":null,"categories":null,"content":"Useful links:\n  azure.microsoft.com  docs.microsoft.com  channel9.msdn.com  code.visualstudio.com  cloudblogs.microsoft.com  devblogs.microsoft.com  marketplace.visualstudio.com  techcommunity.microsoft.com  ","date":1583798400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583798400,"objectID":"477813931710146f41d6177009f4258a","permalink":"/project/detection/","publishdate":"2020-03-10T00:00:00Z","relpermalink":"/project/detection/","section":"project","summary":"Foundations of monitoring","tags":["Devops"],"title":"1. Detection","type":"project"},{"authors":null,"categories":null,"content":"Useful links:\n  azure.microsoft.com  docs.microsoft.com  channel9.msdn.com  code.visualstudio.com  cloudblogs.microsoft.com  devblogs.microsoft.com  marketplace.visualstudio.com  techcommunity.microsoft.com  ","date":1583798400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583798400,"objectID":"1c6e63df68ad1a36a694e23c275e42d0","permalink":"/project/response/","publishdate":"2020-03-10T00:00:00Z","relpermalink":"/project/response/","section":"project","summary":"Foundations of Incident Management","tags":["devops, incident, foundations"],"title":"2. Response","type":"project"},{"authors":null,"categories":null,"content":"Useful links:\n  azure.microsoft.com  docs.microsoft.com  channel9.msdn.com  code.visualstudio.com  cloudblogs.microsoft.com  devblogs.microsoft.com  marketplace.visualstudio.com  techcommunity.microsoft.com  ","date":1583798400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583798400,"objectID":"7f44fdb2e5368eb659c27df69096d5ba","permalink":"/project/remediation/","publishdate":"2020-03-10T00:00:00Z","relpermalink":"/project/remediation/","section":"project","summary":"Foundations of a post-incident review","tags":["Devops"],"title":"3. Remediation","type":"project"},{"authors":null,"categories":null,"content":"Useful links:\n  azure.microsoft.com  docs.microsoft.com  channel9.msdn.com  code.visualstudio.com  cloudblogs.microsoft.com  devblogs.microsoft.com  marketplace.visualstudio.com  techcommunity.microsoft.com  ","date":1583798400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583798400,"objectID":"50068b7e213536ddfa1624237247c86b","permalink":"/project/analysis/","publishdate":"2020-03-10T00:00:00Z","relpermalink":"/project/analysis/","section":"project","summary":"Foundations of a post-incident review","tags":["Devops"],"title":"4. Analysis","type":"project"},{"authors":null,"categories":null,"content":"Useful links:\n  azure.microsoft.com  docs.microsoft.com  channel9.msdn.com  code.visualstudio.com  cloudblogs.microsoft.com  devblogs.microsoft.com  marketplace.visualstudio.com  techcommunity.microsoft.com  ","date":1583798400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583798400,"objectID":"5688ea709c63eaef27862d67bdb38aef","permalink":"/project/readiness/","publishdate":"2020-03-10T00:00:00Z","relpermalink":"/project/readiness/","section":"project","summary":"Foundations of a post-incident review","tags":["Devops"],"title":"5. Readiness","type":"project"},{"authors":null,"categories":null,"content":"What Is This? The systems we work in eventually have problems.\nThey are built, maintained, and supported by technolgists such as yourself. And when an issue inevitably occurs, someone needs to take action to restore services.\nResponding to those problems helps maintain functionality and operational abilities of an organization\u0026rsquo;s IT services, serving both internal and external users.\nMany organizations don\u0026rsquo;t currently have an incident response plan in place. In fact, efforts to recover from service disruptions rarely follow any kind of repeatable and measured framework at all. Engineers react rather than respond.\nWith the increased reliance on digital services and their underlying technology it\u0026rsquo;s more important than ever to establish an explicit response plan. There are small steps that you could take immediately so that when the next problem occurs, everyone knows what to do. The incident itself can be viewed not just as an outage but an opportunity to learn.\n On-call Life is dedicated to providing foundational concepts and information related to being on-call including monitoring, incident response, and the post-incident review process.\nThis is a live site with new information added regularly. Much of the content is syndicated from presentations created for and delivered during Microsoft\u0026rsquo;s Ignite the Tour.\nThroughout these articles, demonstrations and resources specific to Azure will be used, but the foundations of monitoring, incident response, and retrospectives are agnostic to tooling. Demonstrations on Azure is done to illustrate rather than to suggest \u0026ldquo;best practice\u0026rdquo; implementations.\nHow Do I Use This? Begin, by examining why the responsibilities of on-call have become so critical to nearly every business, group, and government.\n Monitoring For Reliability\nWho Is Behind This?  Jason Hand - @jasonhand\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"499d05d5817b2679f60603796462be8a","permalink":"/post/answers-to-questions/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/answers-to-questions/","section":"post","summary":"What Is This? The systems we work in eventually have problems.\nThey are built, maintained, and supported by technolgists such as yourself. And when an issue inevitably occurs, someone needs to take action to restore services.\nResponding to those problems helps maintain functionality and operational abilities of an organization\u0026rsquo;s IT services, serving both internal and external users.\nMany organizations don\u0026rsquo;t currently have an incident response plan in place. In fact, efforts to recover from service disruptions rarely follow any kind of repeatable and measured framework at all.","tags":[],"title":"Answers To Your Questions","type":"post"},{"authors":null,"categories":null,"content":"You now know what a post-incident review is, its role in the incident response process, and when it should be conducted. In this unit, you’ll dive a little deeper into the details of what makes a post-incident review most effective.\nBecause incidents differ, the exact makeup of post-incident reviews can be different, too. But there are some common characteristics and components of a good review that can provide you with a solid foundation for carrying out the process.\nWhat it’s not Before you can understand the characteristics that make for a good post-incident review, you should consider what it’s not.\n  It’s not a document or report. It’s easy to think of a “review” as a written summary, and indeed, a summary report often follows a post-incident review. However, these are two different and distinct parts of the Analysis phase of the incident response lifecycle.\n  It’s not a determination of causality. Your review will look at the factors that contributed to the failure, but the purpose isn’t to pinpoint a culprit. It’s to think about and share information about all aspects of the incident in order to learn and improve.\n  It’s not a list of action items. You may end up with such a list as a result of what you learn in the review, but this isn’t the focus. If you don’t come away with a stepwise punch list but you do know more about your systems than before, the review was successful.\n  The incident review is, more than anything, a conversation. It’s a defined space within which your team can review what they knew at the time and what they know now, and explore and better understand how the parts of the system – including the human parts – do or don’t work together in response to problems.\nCharacteristics and components A good incident review is, first and foremost, blameless. Although you need to examine how the human parts of the system interacted with it, you don’t do this in order to label anyone “at fault.” The focus should be on the failures of the technology and the process, not of the people.\nFrame your questions to reflect this:\n“What was the deficit in your monitoring that failed to give the person at the keyboard the necessary context to make the right decision?”\n“Why was there a delete the entire database option in the tool that didn’t ask for confirmation?”\nWhen things go wrong, it can be tempting to point fingers. However, you must remember this key point: You can’t fire your way to reliability. Shaming and blaming and an investigation that’s aimed at finding and firing the person who is “responsible” won’t lead to more reliable systems. Instead, it will lead to an inexperienced operations team and personnel who are afraid to act.\nApproach the review as a search for knowledge and context, not a hunt for who did what and a reaction to that.\nAlthough the review is about the failures of the technology, it’s not a technical process as much as it is a people process. Talk – and more important, listen – to the people who were involved in the incident. Keep an open mind. Different people have different perspectives and not everyone will agree, and that mix of perspectives is invaluable to the learning process.\nA post-incident review is an honest inquiry. As such, it embraces these key components:\n  Discussion\n  Discourse\n  Dissent\n  Discovery\n  These “4 Ds” create a framework on which you can build a post-incident learning review that can result in more reliable systems and more productive teams that work together.\n Running a Post-incident Review\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"0db7c8236d7a4afa1098c9f299004b7b","permalink":"/post/breaking-down-the-components-of-a-post-incident-review/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/breaking-down-the-components-of-a-post-incident-review/","section":"post","summary":"You now know what a post-incident review is, its role in the incident response process, and when it should be conducted. In this unit, you’ll dive a little deeper into the details of what makes a post-incident review most effective.\nBecause incidents differ, the exact makeup of post-incident reviews can be different, too. But there are some common characteristics and components of a good review that can provide you with a solid foundation for carrying out the process.","tags":["analysis","on-call","foundations"],"title":"Breaking Down the Components of a Post-incident Review","type":"post"},{"authors":null,"categories":null,"content":" \nTech Used The brains behind this solution is an Azure Function (running Node.js) that is triggered via outgoing webhook (from Microsoft Teams). The function modifies an index.html file stored in a \u0026ldquo;web server\u0026rdquo; served from a serverless SMB file share in Azure Storage.\nUsers can open, update, and close \u0026ldquo;status updates\u0026rdquo; by invoking them from within a chat channel.\nThe text that follows the command will be stored and displayed on the site below the colored (Red or Green) header.\nIn addition to the website files, an Azure Table will be used for storing the history of each status update.\n NOTE: The table is not provisioned with the deployment script. It needs to be manually created in Step 2 below.\n SignalR manages refreshing the client so that changes made to the HTML are immediately visible without any end user interaction.\nApplication Insights is used to provide observability on the operation, behavior, and usage of the solution and is \u0026ldquo;best practice\u0026rdquo; for building highly available and reliable system\u0026hellip; which we expect from any Status Page solution.\nPrerequisites You will need an account with the following services:\n  Microsoft Azure  Microsoft Teams   Deployment Instructions The blue button below will deploy all resources needed for this solution in to the Resource Group and Azure region of your choice. The name you choose also determines the URL used to view the Status Page as well as the incoming URL used to trigger updates.\nSteps To Deploy 1. Create the Azure Function app, Storage account, and SignalR Service with this button: \n This will begin deploying everything needed for the solution and will provide a link to the public facing URL of the Status Page as well as a link to your new resource group where you will continue with step 2.\n  2. In the Azure portal, open the Storage account and add a table named statuses. You do not need to set any properties or add records.\n 3. Navigate to the function app, and open the teams-webhook function. Click \u0026ldquo;Get Function URL\u0026rdquo; and copy the URL.\n4. Open the URL of the Function in a new tab. This is \u0026ldquo;Status Page\u0026rdquo; that will change automatically when updated. It is NOT the same as the \u0026ldquo;Get Function URL\u0026rdquo; used in the next step.\n 5. Open Microsoft Teams and navigate to the \u0026ldquo;Apps\u0026rdquo; page of the team in which you want to create the bot. Click \u0026ldquo;Create outgoing webhook\u0026rdquo;.\n Use StatusPage as the bot name (this is hardcoded, for now). Paste in the function URL, and enter a description and press the create button.   You will be prompted with a secret code for validating webhook calls from Teams. We currently do not use this. Close the dialog box.\n  \n6. In Microsoft Teams, update the status page by typing @StatusPage to summon the bot followed by open We are experiencing a problem. Standby for more information\nAvailable commands are:\n@StatusPage open [message] @StatusPage update [message] @StatusPage close [message] @StatusPage help`   This solution is based on the on-stage demonstrations built for Microsoft Ignite The Tour.\nTo learn more about the full demonstration, view the repo for \u0026ldquo; OPS20 - Responding To Incidents\u0026quot;. Huge Thanks to Anthony Chu in bringing this to life.\nNow, let\u0026rsquo;s move on to Understanding Why We Learn From Incidents.\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"157c45e28d4528538979674fb20fbd09","permalink":"/post/building-a-serverless-status-page-solution/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/building-a-serverless-status-page-solution/","section":"post","summary":"Tech Used The brains behind this solution is an Azure Function (running Node.js) that is triggered via outgoing webhook (from Microsoft Teams). The function modifies an index.html file stored in a \u0026ldquo;web server\u0026rdquo; served from a serverless SMB file share in Azure Storage.\nUsers can open, update, and close \u0026ldquo;status updates\u0026rdquo; by invoking them from within a chat channel.\nThe text that follows the command will be stored and displayed on the site below the colored (Red or Green) header.","tags":["chatops","devops","communication","collaboration","stakeholders","serverless","status"],"title":"Building a Serverless Status Page Solution","type":"post"},{"authors":null,"categories":null,"content":"Tracking incidents is as easy as setting up a datastore, like the table storage used for the on-call roster. However, why reinvent the wheel? Why not use something already available, customizable, extensible, and free?\nAzure Boards is my tool of choice in this tutorial, but honestly this could probably be done with any popular project management tool with an API.\n First, login or create a free Azure Devops account.   1. Select an existing or create a new project in Azure Devops\n2. Click on Work Items\nIn the left navigation menu, choose Work Items under the Boards group. This is what we will use to track our incidents.\nBut before we do, we need to create a custom work item type and then start a new project based on different default options.\n3. Navigate to the Project Configuration settings\nThe Project Settings icon is in the lower left. Then select Project Configuration.\nFrom here we will create a custom work item type for incidents.\n4. Click the \u0026ldquo;go to the process customization page\u0026rdquo; link in the upper right\nThis is where we can view existing Work Item Types as well as create new ones. But first, we have to create a new inherited process in order to enable customizations.\n5. Click on the \u0026ldquo;create an inherited process\u0026rdquo; link near the top\nGive it a name, such as on-call and a description.\nOnce you click \u0026ldquo;Create process\u0026rdquo; you\u0026rsquo;ll be returned to the previous screen where you can now select the + New work item type option.\nGive it a name, like Incident, a description, icon, and icon color. Then press create.\nFrom the next screen click on Process found in the Boards section of the left navigation pane. You\u0026rsquo;ll see the new inherited process \u0026ldquo;on-call\u0026rdquo;.\n6. Click the settings elipses (\u0026hellip;) followed by + New team project.\n7. Click create new project\nGive the project a name, description, and a few other selections and press create.\nWe will use this new project for all incident tracking leaving the original project for something else.\nLet\u0026rsquo;s check to see if our new work item type is available. Return to Azure Boards.\n8. Click the + New Work Item option\nYou\u0026rsquo;ll now see an additional work item type of \u0026ldquo;incident\u0026quot;.\nSelect the Incident option to view the default fields available to track details.\nWe can see that we have a number of fields available to us by default such as Assigned To, State, Discussion, and more.\nWe now have a basic way of tracking incidents. Any time an incident is detected, we can add a new work item type of \u0026ldquo;incident\u0026rdquo; along with the important details so that there is a central place for everyone to stay informed.\nHowever, the default settings only provide us a few important things that we want to track. We mentioned before that we\u0026rsquo;d like to track when, how, who, and what we know about the incident.\nWe can easily add and modify fields within Azure Boards to allow for tracking additional details we know are important to our incident response efforts.\nNow, let\u0026rsquo;s talk about ways of Customizing Azure Boards for Incident Tracking\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"b5f5765781c3d1626819d9b0f38f3111","permalink":"/post/creating-an-incident-tracking-tool-with-azure-boards/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/creating-an-incident-tracking-tool-with-azure-boards/","section":"post","summary":"Tracking incidents is as easy as setting up a datastore, like the table storage used for the on-call roster. However, why reinvent the wheel? Why not use something already available, customizable, extensible, and free?\nAzure Boards is my tool of choice in this tutorial, but honestly this could probably be done with any popular project management tool with an API.\n First, login or create a free Azure Devops account.   1.","tags":["incidents","tracking","azure","boards"],"title":"Creating an Incident Tracking Tool with Azure Boards","type":"post"},{"authors":null,"categories":null,"content":" \u0026quot;An alert is something which requires a human to perform an action.\u0026rdquo; - Pagerduty \u0026ldquo;Alerting Principles\u0026rdquo;\n To understand why alerting can create a problem, you need to think about the purpose of alerts and how they differ from other monitoring components.\nActionable alerts are not:\n  Logs. Alerts are not records of events; that’s the role of logs.\n  Notifications. Alerts are not intended to announce non-critical occurrences such as the completion of a software build.\n  Heartbeats. Alerts shouldn’t be used to document failure of a heartbeat signal periodically sent between two systems at regular intervals.\n  Actionable alerts are used for situations in which you need a human to investigate and intervene to remediate the problem. Alerts should be communications that something exceptional or unexpected has happened that requires someone’s attention.\nA lot of thought should be put in to how alerts are delivered and when it is necessary. More on how to configure alerts in Azure can be found here.\nIf the event is something that the system can handle through automated processes, such as scaling resources within a preset limit, an alert is not necessary. A simple line in a log should suffice.\n Understanding Actionable Alerts\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"727c5667a04463688ba7fecffab7f7df","permalink":"/post/defining-alerts/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/defining-alerts/","section":"post","summary":"\u0026quot;An alert is something which requires a human to perform an action.\u0026rdquo; - Pagerduty \u0026ldquo;Alerting Principles\u0026rdquo;\n To understand why alerting can create a problem, you need to think about the purpose of alerts and how they differ from other monitoring components.\nActionable alerts are not:\n  Logs. Alerts are not records of events; that’s the role of logs.\n  Notifications. Alerts are not intended to announce non-critical occurrences such as the completion of a software build.","tags":["on-call","alerts","foundations"],"title":"Defining Alerts","type":"post"},{"authors":null,"categories":null,"content":"If you search online for \u0026ldquo;Incident Response\u0026rdquo; a majority of what you\u0026rsquo;ll find is information related to security threats and breaches. What doesn\u0026rsquo;t show up in the results is stuff about how to properly respond to threats related to something else entirely.\nHow should a business respond to technical challenges and failures as they come up? The ones that affect reliability concerns such as availability, latency, correctness, and others. What happens when service level expectations are breached and it\u0026rsquo;s time for a human to get involved?\nServices such as VictorOps, PagerDuty, and others provide \u0026ldquo;on-call\u0026rdquo; solutions as well as documentation and best practices regarding this type of incident management. Service Now has opinions as well but the language is aimed more for those who follow ITSM guidance regarding service management. Ticketing with a tiered support structure doesn\u0026rsquo;t provide the fasted path to uptime for many companies however.\nIn the devops and web operations space, the idea of anyone but the engineers building the system responding to customer impacting problems is completely unacceptable. Irresponsible even. Time is of the essence and those who helped build the applications and underlying infrastructure are the best suited to maintain it\u0026rsquo;s health and upgrades in a production environment.\nExactly when an engineer should be expected to take action is why we need to define what we mean by an incident.\nWe can all agree that an incident is a “service disruption” - something that is affecting our user\u0026rsquo;s ability to use the services they have come to rely on.\nThat\u0026rsquo;s a given. However, there are other things about incidents that are often overlooked or never considered. For example incidents are commonly subjective, feared, and unexpected\n From Microsoft\u0026rsquo;s IcM Documentation What is an incident? An incident is any unplanned interruption or degradation of a product or service that is causing customer impact. For example, a bad HTTP request, slow connection, security vulnerability, or customer-reported error message could constitute an incident. Every service across Microsoft has a different definition of what an incident is and when one should be triggered.\nSometimes, an incident can be severe enough to affect many different services and customers. For example, a datacenter power failure may impact dozens of Microsoft products. Severe incidents with broad impact are called outages.\nTypically any customer impacting incident must be mitigated as soon as possible to minimize the customer impact. Organizations track Time-To-Mitigate (TTM) metrics for their services. For example: In Azure, TTM must be \u0026lt;= 30mins for any customer impacting incident.\nWhat is incident management? Incident Management is the process of detecting a live-site problem with a service, creating an incident, determining the cause, restoring the service to full operation, and capturing learnings to prevent it from happening again.\nWhat is IcM? IcM is the one incident management system for all Microsoft services. IcM provides tools for managing live site and on call rotations. IcM runs around the clock to keep services working across the world. Incident Types supported are \u0026ldquo;Livesite Incident\u0026rdquo;, \u0026ldquo;Deployment Incident\u0026rdquo; and \u0026ldquo;Customer Reported Incidents\u0026rdquo;. No additional types will be supported.\nWhat IcM is not? IcM is not intended to be a ticketing solution. IcM was designed for incidents that must be mitigated within minutes to minimize the customer impact. Here are some differences:\nIn a ticketing solution, a ticket can take \u0026lsquo;n\u0026rsquo; days to resolve. In IcM, incidents need to be resolved as soon as possible to minimize customer impact.\nIn a ticketing solution, teams might want to pause or have a count down timer for tickets. In IcM, this will not be supported because the incident has to be resolved in minutes. In a ticketing solution, teams might want to customize the workflows. In IcM, incident workflow is fixed: Create -\u0026gt; Acknowledge -\u0026gt; Mitigate -\u0026gt; Resolve.\nIn a ticketing solution, teams might want different \u0026lsquo;Types\u0026rsquo; and \u0026lsquo;Sub-Types\u0026rsquo;. In IcM, only types that will be supported are \u0026ldquo;Livesite Incident\u0026rdquo;, \u0026ldquo;Deployment Incident\u0026rdquo; and \u0026ldquo;Customer Reported Incidents\u0026rdquo;. This is to ensure that Service teams are focused on addressing the customer impacting incidents as soon as possible.\n Exploring Subjectivity of Incidents\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"165555fa559ea22e41603d012c3f671d","permalink":"/post/defining-incidents/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/defining-incidents/","section":"post","summary":"If you search online for \u0026ldquo;Incident Response\u0026rdquo; a majority of what you\u0026rsquo;ll find is information related to security threats and breaches. What doesn\u0026rsquo;t show up in the results is stuff about how to properly respond to threats related to something else entirely.\nHow should a business respond to technical challenges and failures as they come up? The ones that affect reliability concerns such as availability, latency, correctness, and others. What happens when service level expectations are breached and it\u0026rsquo;s time for a human to get involved?","tags":["incidents","on-call"],"title":"Defining Incidents","type":"post"},{"authors":null,"categories":null,"content":"Site Reliability Engineering is an engineering discipline devoted to helping organizations sustainably achieve the appropriate level of reliability in their systems, services, and products.\nThe key concepts to take away from this definition are:\n  Reliability. You learned in the introductory module that there are multiple aspects to reliability and later in this module, you’ll examine each in more detail. You also learned about the importance of reliability – why it matters.\n  Sustainability. In this context, “sustainable” refers to the role of people in creating a sustainable operations practice. Reliable systems, services, products are built by people. It is crucial to the concept of SRE to implement an operations practice that is sustainable over time, so that people are able to bring their best to the job.\n  Appropriateness. It’s important to understand that 100% reliability isn’t often possible, especially in today’s complex systems with dependencies on other systems. 100% reliability means zero down time, which also means no opportunity to make any changes or improvements that could create some down time. Instead of striving for a goal of absolute reliability, determine the appropriate level of reliability for a particular application or service.\n  The role of a site reliability engineer bridges the span between operations and development but also goes beyond DevOps, and the SRE philosophy is the basis of a new and more efficient and effective approach to building and operating reliable systems and applications.\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"d2a060afbbef89467bb6e01313b438a3","permalink":"/post/defining-site-reliability-engineering/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/defining-site-reliability-engineering/","section":"post","summary":"Site Reliability Engineering is an engineering discipline devoted to helping organizations sustainably achieve the appropriate level of reliability in their systems, services, and products.\nThe key concepts to take away from this definition are:\n  Reliability. You learned in the introductory module that there are multiple aspects to reliability and later in this module, you’ll examine each in more detail. You also learned about the importance of reliability – why it matters.","tags":["incidents","on-call","sre","reliability"],"title":"Defining Site Reliability Engineering","type":"post"},{"authors":null,"categories":null,"content":"To conduct a good post-incident review, you must first ensure that everyone is on the same page. Toward that end, remember that language matters; there are terms you should use and some you shouldn’t. Key points in conducting the review are who to include (everyone) and when to do it (within twenty-four to thirty-six hours). This will help you accomplish the purpose of the review: to learn and improve.\nWe learn from incidents by conducting a post-incident review, which happens during the analysis phase. You should do a post-incident review after every significant incident.\n(Placeholder: graphic of incident response lifecycle in Slide #28)\nAlthough the formal review takes place after the response and remediation phases, you begin to set the stage for your analysis as soon as you receive an actionable alert that an incident has occurred, inform team members, and begin a conversation around the incident.\nLanguage matters In conducting that conversation, and as you go through this unit, there is an important point that you need to keep in mind: language matters. There are specific terms that you should use and others that you should deliberately avoid using. That’s because the words you use affect how you think about what happened and can dramatically impact what and how much you learn from the incident.\nThis has been shown by research in safety-critical industries such as aviation, medicine, search and rescue, firefighting, and more. This field of research is known as resilience engineering.\nResilience engineering principles are applicable to the technology sector, as well. In this module, you’ll learn some of the useful concepts from resilience engineering, including four of the most common traps people fall into when attempting to learn from failure.\nDefining the post-incident review Not everyone uses exactly the same language to refer to the results of the analysis phase. You might call it:\n  Post-incident review\n  Post-incident learning review\n  Postmortem\n  Retrospective\n  In addition, not everyone goes about it in exactly the same way. For example, many people start by getting everyone who had any connection to the incident into a room, while other people choose to create the review via individual interviews and then report back to the group.\nThe latter method often works better when group settings in your organization are more difficult because of group dynamics or personalities or may be based on how distributed the group member are, or the type of incident. You should do what works best for your team and the circumstances.\nWhatever you call it and however you organize it, there are a couple of key points:\n  Inclusiveness: You should try to include in the post-incident review everyone who was involved in the incident response.\n  Timing: you should perform the post-incident review within twenty-four to thirty-six hours after the event if at all possible. Neuroscience has confirmed that human memory is notoriously unreliable; people forget things. The more time that passes after an event, the less detailed and specific memories tend to be.\n  Purpose of the post-incident review The goal of the post-incident review is so your team can learn and improve. You will want to learn about the systems and about the things that you had put in place that worked or didn’t work, so you can make improvements.\nAt the same time, you should remember that action items that you generate – reports, tasks, feedback – are useful but are peripheral to the point of the process, which is to learn and improve. Action items are a secondary effect.\n Breaking Down the Components of a Post-incident Review\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"498fb1cd577f45c3705f50557a039610","permalink":"/post/defining-the-post-incident-review/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/defining-the-post-incident-review/","section":"post","summary":"To conduct a good post-incident review, you must first ensure that everyone is on the same page. Toward that end, remember that language matters; there are terms you should use and some you shouldn’t. Key points in conducting the review are who to include (everyone) and when to do it (within twenty-four to thirty-six hours). This will help you accomplish the purpose of the review: to learn and improve.\nWe learn from incidents by conducting a post-incident review, which happens during the analysis phase.","tags":["incidents","analysis"],"title":"Defining the Post-incident Review","type":"post"},{"authors":null,"categories":null,"content":"You’ve probably heard all your life the adage, attributed to Benjamin Franklin, that “an ounce of prevention is worth a pound of cure.” The accepted meaning is that it’s better to keep a problem from happening than to fix it after it’s happened.\nIn your efforts to achieve a high level of reliability for your systems and services, you should do everything possible to prevent incidents from occurring. However, due to the complexity of those systems, as explained above, prevention isn’t always possible.\nThus, you must take a two-pronged approach to failure: prevention on the one hand, and when that isn’t possible, preparation to respond – quickly and effectively. These two are interlinked.\nHere’s an example of why it’s essential to do both: Sometimes an organization will deploy an automated system. It works well. In fact, it works almost too well – because people may take for granted that it will always work. They don’t make proper preparation for the day that it doesn’t work. When that day comes – as it inevitably will, sooner or later – and the system fails, it fails spectacularly, and it’s much harder for operators to understand what went wrong.\nThe systems you work on are made up of more than the technology. In fact, you don’t work “on” or “with” a system; you work in the system. You are part of the system. Complex systems include both technical components (hardware, software) and human components (people – and their personalities, training, and knowledge).\nHow the humans respond when things go wrong is as important as preventing things from going wrong in the first place. We learn from failure to respond faster and better.\n Defining the Post-incident Review\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"a8bb5fa830884dead0bc7bcd3e013c42","permalink":"/post/distinguishing-prevention-and-preparation/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/distinguishing-prevention-and-preparation/","section":"post","summary":"You’ve probably heard all your life the adage, attributed to Benjamin Franklin, that “an ounce of prevention is worth a pound of cure.” The accepted meaning is that it’s better to keep a problem from happening than to fix it after it’s happened.\nIn your efforts to achieve a high level of reliability for your systems and services, you should do everything possible to prevent incidents from occurring. However, due to the complexity of those systems, as explained above, prevention isn’t always possible.","tags":["incident","analysis","prevention","readiness","preparation"],"title":"Distinguishing Prevention and Preparation","type":"post"},{"authors":null,"categories":null,"content":"To address some of the challenges around how we communicate we also want to find a way to create a unique channel or space for engineers to discuss the details of the incident - a “conversation bridge” in our persistent group chat tool -which for Tailwind Traders is Microsoft Teams.\nWe want a channel that is unique to the incident only. We do not want conversations about other engineering efforts.We don’t want conversations about what people are doing for lunch. We ONLY want conversations related to the incident. Because then we can take that text (or data) and analyze later in a Post-incident review.\nNext steps  Understanding Why We Learn From Incidents\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"4b242d816984c70d4837699150625f40","permalink":"/post/establishing-communications-channel/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/establishing-communications-channel/","section":"post","summary":"To address some of the challenges around how we communicate we also want to find a way to create a unique channel or space for engineers to discuss the details of the incident - a “conversation bridge” in our persistent group chat tool -which for Tailwind Traders is Microsoft Teams.\nWe want a channel that is unique to the incident only. We do not want conversations about other engineering efforts.We don’t want conversations about what people are doing for lunch.","tags":["chatops","on-call","coordination","foundations"],"title":"Establishing Communication Channels","type":"post"},{"authors":null,"categories":null,"content":"Creating a repeatable response plan means establishing who does what when something goes wrong. We don\u0026rsquo;t want there to be any question around who is supposed to be doing what.\nBecause of this, it is important to establish roles and the associated expectations. This isn\u0026rsquo;t a separation of duties exercise. In fact, we want to encourage less of that. It is however, a way of establishing better coordination and communication. It prevents people from stepping on each others toes while enabling cross-collaboration amongst not only on-call rosters, but an entire organization.\nThe first role we need to talk about is the Primary Responder – the Primary “On-call” engineer.\nThis person is expected to acknowledge their awareness of an incident once the alert has been received.\nThen we have the secondary responder – who is there as back up -Another engineer who can step in if the primary responder is unavailable or unreachable. Or if they just need another pair of eyes.\nAnother key role to identify, in many cases, is the incident commander. An incident commander can be extremely helpful when you have got a large-scale outage that effects a lot of different components or requires coordination across many teams and different systems. They are great for making sure that engineers stay focused and they are working on their own remediation efforts\u0026hellip; Ensuring people are not stepping on each other or undoing each other\u0026rsquo;s work. It is good to have a central person who can track what is going on and who is doing what.\nThe Communication Coordinator is meant to be the person working in conjunction with the incident commander to share more information beyond those who are in the firefight actively working to recover from the incident itself. That could be customers. It could be the sales and marketing teams. Maybe your customer support. There are many people within an organization who need to be made aware of what’s taking place and the status around how things are progressing. It\u0026rsquo;s always good to put someone in charge of managing that communication and making sure that other stakeholders are aware of what is happening and what’s being done.\nThe scribe’s role is to document the conversation in as much detail as possible. Teams commonly use phone bridges, conference calls, or video chat to get everyone together and try to understand what is going on, which can certainly help create space for the conversation. However, it is difficult for us to go through and understand in detail what the engineers were saying and doing unless it is transcribed. As a result, a scribe is that person who can help us document as much as possible to review later. What were people saying, doing, feeling, and even experiencing? It is all data to be analyzed – but only if we capture it.\nIt’s quite common within on-call rosters to identify subject matter experts, so that early responders know who to escalate too quickly. These people should not be on call all the time, of course, but we do want to be able to identify who is our database expert. Who is our front-end expert? Who are the people that we can reach out to if our primary and secondary responders are not able to diagnose and resolve the issue themselves?\nLet\u0026rsquo;s take a closer look at each of these roles to better understand their place within our incident response efforts.\nNext steps  Identifying the Primary Responder\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"9f57a7869f6655c3f949c09f6f7be28a","permalink":"/post/establishing-oncall-roles/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/establishing-oncall-roles/","section":"post","summary":"Creating a repeatable response plan means establishing who does what when something goes wrong. We don\u0026rsquo;t want there to be any question around who is supposed to be doing what.\nBecause of this, it is important to establish roles and the associated expectations. This isn\u0026rsquo;t a separation of duties exercise. In fact, we want to encourage less of that. It is however, a way of establishing better coordination and communication. It prevents people from stepping on each others toes while enabling cross-collaboration amongst not only on-call rosters, but an entire organization.","tags":["role","on-call","coordination","foundations"],"title":"Establishing On-call Roles","type":"post"},{"authors":null,"categories":null,"content":"Rosters establish a framework around who is on-call at any given point. A roster, or team, is made up of multiple engineers. Rosters can also contain multiple rotations. I\u0026rsquo;m testing out how to edit a page.\nNext steps  Creating an on-call roster using Azure Table Storage\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"3225eb21ef7ca365f80a106f1fbaa474","permalink":"/post/establishing-oncall-rosters/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/establishing-oncall-rosters/","section":"post","summary":"Rosters establish a framework around who is on-call at any given point. A roster, or team, is made up of multiple engineers. Rosters can also contain multiple rotations. I\u0026rsquo;m testing out how to edit a page.\nNext steps  Creating an on-call roster using Azure Table Storage","tags":["roster","on-call","coordination","foundations"],"title":"Establishing On-call Rosters","type":"post"},{"authors":null,"categories":null,"content":"People shouldn\u0026rsquo;t have to be on-call for long periods of time. It\u0026rsquo;s detrimental to their health and therefore the systems they create and look after.\nIn order to make sure people aren\u0026rsquo;t expected to respond to problems 24-hours a day, indefinitely, we use rotations. Also referred to as scheduled shifts. That way people can take turns.\nBeing on-call requires a heavy cognitive task which then negatively impacts many aspects of a person\u0026rsquo;s life. Shifts allow engineers to take “on-call” responsibility for their own specific recurring rotation and share the load in a way that keeps the human element a priority.\nSite Reliability Engineering is an engineering discipline devoted to helping an organization sustainably achieve the appropriate level of reliability in their systems, services, and products. Long periods of on-call responsibilities is not sustainable.\nWhen creating shifts there are a number of common approaches.\n24 x 7 The majority of rotations used by teams are known as 27 x 7 shifts where engineers will be “on-call” for several days in a row. However, most “Elite/High performers” would agree that rotations longer than 3 or 4 days are detrimental to the overall health of engineering staff and therefore the entire system.\nFollow the Sun Follow the sun shifts are nice for distributed teams. These allow for engineers to schedule their “on-call” shifts only during their normal working office hours. As they end their day and go home, engineers in a different time zone can take over.\nAnd of course, there are many ways to customize shifts, especially for weekends when engineers need more flexibility. Engineers should be able to hand off the role to someone when personal conflicts arise.\nOnce roles, rosters, and rotations have been determined and put in place, we can now focus our attion on methods of Tracking Incident Details\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"494aaf44e935dbea77a385b391364bf4","permalink":"/post/establishing-oncall-rotations/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/establishing-oncall-rotations/","section":"post","summary":"People shouldn\u0026rsquo;t have to be on-call for long periods of time. It\u0026rsquo;s detrimental to their health and therefore the systems they create and look after.\nIn order to make sure people aren\u0026rsquo;t expected to respond to problems 24-hours a day, indefinitely, we use rotations. Also referred to as scheduled shifts. That way people can take turns.\nBeing on-call requires a heavy cognitive task which then negatively impacts many aspects of a person\u0026rsquo;s life.","tags":["rotations","on-call","coordination","foundations"],"title":"Establishing On-call Rotations","type":"post"},{"authors":null,"categories":null,"content":"When deciding what to monitor for reliability it becomes clear that there are many approaches because there are many aspects to reliability. It\u0026rsquo;s important to examine the reliability of your service from the user\u0026rsquo;s perspective to determine what is important and how to prioritize.\nWhat Does Reliability Mean to the User? To determine the reliability of a system, service, application, or process many look to a combination of the following eight aspects of reliability.\n Availability Latency Throughput Coverage Correctness Fidelity Freshness Durability  Once it is determined which aspects to measure and how, we can begin understanding the behavior of our systems under \u0026ldquo;normal\u0026rdquo; conditions.\nEstablish a Baseline Establishing a baseline of reliable performance helps set expectations and recognize deviations from what you might call \u0026ldquo;normal\u0026rdquo;. Once you have a baseline, you can do a comparison.\n Exploring the Availability Aspect of Reliability\n NOTE: Look in to \u0026ldquo;The Golden Signals\u0026rdquo; - Jason Hand\n ","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"979ce27d57b87e9b503de5afc1f24e8a","permalink":"/post/examining-all-aspects-of-reliability/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/examining-all-aspects-of-reliability/","section":"post","summary":"When deciding what to monitor for reliability it becomes clear that there are many approaches because there are many aspects to reliability. It\u0026rsquo;s important to examine the reliability of your service from the user\u0026rsquo;s perspective to determine what is important and how to prioritize.\nWhat Does Reliability Mean to the User? To determine the reliability of a system, service, application, or process many look to a combination of the following eight aspects of reliability.","tags":["reliability"],"title":"Examining All Aspects of Reliability","type":"post"},{"authors":null,"categories":null,"content":"As we delve into our post-incident review, we need to be on guard against some human tendencies that can lead us to inaccurate or incomplete conclusions and distract us from accomplishing the core purpose of the review: learning about our systems so as to improve their reliability.\nNow you have a roadmap help you get started on the post-incident review process, but it would also be useful to know about some of the obstacles you might encounter on this journey.\nLearning from your own mistakes is something everyone should all do, but you don’t always have to experience something first-hand to learn from it. Learning from the mistakes of others, without criticizing or judging them, allows you to benefit from those lessons without personally suffering the consequences.\nIn this unit, you’ll find out about some common traps that others have fallen into during the post-incident review process and how to avoid them.\nTrap 1: Attribution to “human error” Trap 2: Counterfactual reasoning Trap 3: Normative language Trap 4: Mechanistic reasoning  Examining Attribution of Human Error\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"92164ad1733bf0acf919735648a9cdef","permalink":"/post/examining-common-traps/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/examining-common-traps/","section":"post","summary":"As we delve into our post-incident review, we need to be on guard against some human tendencies that can lead us to inaccurate or incomplete conclusions and distract us from accomplishing the core purpose of the review: learning about our systems so as to improve their reliability.\nNow you have a roadmap help you get started on the post-incident review process, but it would also be useful to know about some of the obstacles you might encounter on this journey.","tags":["incidents","human","analysis"],"title":"Examining Common Traps","type":"post"},{"authors":null,"categories":null,"content":"In the field of psychology, counterfactual thinking is a concept that’s associated with the human tendency to invent possible alternatives to past events – how things might have turned out differently.\nCounterfactual means “contrary to facts,” and counterfactual reasoning refers to telling a story about events that did not happen, in order to explain the events that did. You can identify counterfactual statements by key phrases:\n  Could have\n  Should have\n  Would have\n  Failed to\n  Did not\n  If only\n  Some examples of counterfactual statements related to post-incident reviews:\n“The monitoring system failed to detect the problem.”\n“The engineer did not check the validity of the configuration before enacting it.”\n“This could have been picked up in the canary environment.”\nThe problem with this type of reasoning is that you’re talking about things that didn’t happen instead of taking the time to understand how what did happen, happened. You don’t learn anything from this speculation.\n Examining Mechanistic Reasoning\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"6cc743838bf294f2c14549b23289a8a1","permalink":"/post/examining-counter-factual-reasoning/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/examining-counter-factual-reasoning/","section":"post","summary":"In the field of psychology, counterfactual thinking is a concept that’s associated with the human tendency to invent possible alternatives to past events – how things might have turned out differently.\nCounterfactual means “contrary to facts,” and counterfactual reasoning refers to telling a story about events that did not happen, in order to explain the events that did. You can identify counterfactual statements by key phrases:\n  Could have","tags":["incidents","human","analysis"],"title":"Examining Counter-factual Reasoning","type":"post"},{"authors":null,"categories":null,"content":"Mechanistic reasoning refers to the concept that a particular outcome can be inferred from intervention. It’s sometimes called the meddling kids syndrome based on the premise that “Our system would have worked fine … if it hadn’t been for those meddling kids.”\nWhen you use mechanistic reasoning in your post-incident review, you build your conclusions on the fallacy that the systems you work with and within are basically working correctly, and if only those “meddling kids” hadn’t done whatever they did, the failure would not have occurred.\nHowever, that’s not how systems work.\nTo illustrate this point, imagine the following scenario: You work on a production service. Now you’re told that you are not allowed to touch or do anything to that service. Everything outside your team continues as before – customers continue to use the service, external dependencies continue to change, the Internet functions normally.\nBut you can’t make any changes to the code or configuration. No deployments, no control plane operations, nothing.\nDo you think your service would still be running as expected after a day? After a week? After a month? After a year? How long could you realistically expect your service to keep running without human intervention?\nHuman adaptive capacity is necessary to keep our systems up and running.\nThe only reason your systems are up and running in the first place is because of the actions of humans in the control loop. It’s only through human action and ability to adapt to changing circumstances that the system continues to work.\nTherefore, it’s erroneous to conclude the system was “basically working… if it hadn’t been for those meddling kids.” In fact, the reliability of your service is not independent of the humans who work on it. Instead, it’s a direct result of the work that the humans do on it every day.\nThe problem with mechanistic reasoning is that it leads you down a path where you believe that finding the faulty human is equivalent to finding the problem. However, that same faulty human has been improvising and adapting to keep the system running for weeks and months.\n Examining Normative Language\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"3cb47b00118598c86247ba56bee6ecce","permalink":"/post/examining-mechanistic-reasoning/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/examining-mechanistic-reasoning/","section":"post","summary":"Mechanistic reasoning refers to the concept that a particular outcome can be inferred from intervention. It’s sometimes called the meddling kids syndrome based on the premise that “Our system would have worked fine … if it hadn’t been for those meddling kids.”\nWhen you use mechanistic reasoning in your post-incident review, you build your conclusions on the fallacy that the systems you work with and within are basically working correctly, and if only those “meddling kids” hadn’t done whatever they did, the failure would not have occurred.","tags":["incidents","human","analysis"],"title":"Examining Mechanistic Reasoning","type":"post"},{"authors":null,"categories":null,"content":"Normativity relates to the human tendency to designate some actions and outcomes as desirable or good and other actions or outcomes as undesirable or bad. A norm is a standard of correctness agreed upon by a society.\nWhen you use normative language in a post-incident review, you judge the decisions and actions of those responding to the incident with the benefit of hindsight. This language implies that there was an obviously correct course of action that the operator should have followed.\nNormative language can usually be identified by adverbs such as “inadequately,” “carelessly,” “hastily,” and so forth.\nNormative thinking leads you to judge decisions based on their outcomes. This isn’t logical because the outcome is the only piece of information that was not available to those who made the decisions and judgments.\nThe problem with normative thinking is that you neglect to understand how the actions of the operators made sense to them at the time.\n Home\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"bbcc6765b9c561195061e0e2bb530eb5","permalink":"/post/examining-normative-language/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/examining-normative-language/","section":"post","summary":"Normativity relates to the human tendency to designate some actions and outcomes as desirable or good and other actions or outcomes as undesirable or bad. A norm is a standard of correctness agreed upon by a society.\nWhen you use normative language in a post-incident review, you judge the decisions and actions of those responding to the incident with the benefit of hindsight. This language implies that there was an obviously correct course of action that the operator should have followed.","tags":["incidents","human","analysis"],"title":"Examining Normative Language","type":"post"},{"authors":null,"categories":null,"content":"Humans make mistakes. However, human error is not a diagnosis; it’s a symptom. When human error is deemed to be the reason for a failure, you may stop there instead of further analyzing the incident to determine the root cause – which is a deeper, systemic issue.\nSystem design, organizational context, and personal context all affect when, how and with what impact people make mistakes. “Human error” is a label that causes you to quit investigating at precisely the moment when you’re about to discover something interesting about your system.\nThe problem with the “human error” conclusion in investigations is that it causes you to lose sight of the fact that what the humans did made sense to them at the time. Mistakes, by definition, aren’t deliberate, so they didn’t intend to make a mistake.\nWhen we see or hear “human error”, it is a signal that we need to look deeper. Root cause analysis is needed to identify the sequence of events that resulted in the human error.\n Examining Counter-factural Reasoning\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"acc58b612b34239e925f313bfc07a0bf","permalink":"/post/examining-attribution-of-human-error/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/examining-attribution-of-human-error/","section":"post","summary":"Humans make mistakes. However, human error is not a diagnosis; it’s a symptom. When human error is deemed to be the reason for a failure, you may stop there instead of further analyzing the incident to determine the root cause – which is a deeper, systemic issue.\nSystem design, organizational context, and personal context all affect when, how and with what impact people make mistakes. “Human error” is a label that causes you to quit investigating at precisely the moment when you’re about to discover something interesting about your system.","tags":["incidents","human","analysis"],"title":"Examining the Attribution of Human Error","type":"post"},{"authors":null,"categories":null,"content":" \u0026quot;There is also a subtler effect, which is that the fear of on-call is often enough by itself to radically change people’s behavior. Entire development teams reject outright the notion of going on call, because of the impact on their personal lives, family, and in-hours effectiveness.\u0026rdquo; - Niall Murphy, Microsoft \u0026ldquo;Seeking SRE\u0026rdquo;(O\u0026rsquo;Reilly)\n In some cases, we downplay the significance of an outage \u0026hellip; or worse \u0026hellip; intentionally mis-label or not report a disruption in service for fear of reprimand.\nHistorically, we have felt that incidents reflected poorly in several areas of our engineering efforts and the organization.\nIt has not been until more recently through many of the conversations around devops and site reliability engineering, that we are starting to rethink incidents and now view them as opportunities to learn and improve our systems.\n Exploring Why Incidents Are Unexpected\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"20c3a2e809e4b9691d07b03d85438b80","permalink":"/post/exploring-fear-of-incidents/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/exploring-fear-of-incidents/","section":"post","summary":"\u0026quot;There is also a subtler effect, which is that the fear of on-call is often enough by itself to radically change people’s behavior. Entire development teams reject outright the notion of going on call, because of the impact on their personal lives, family, and in-hours effectiveness.\u0026rdquo; - Niall Murphy, Microsoft \u0026ldquo;Seeking SRE\u0026rdquo;(O\u0026rsquo;Reilly)\n In some cases, we downplay the significance of an outage \u0026hellip; or worse \u0026hellip; intentionally mis-label or not report a disruption in service for fear of reprimand.","tags":["incidents","human","analysis"],"title":"Exploring Fear of Incidents","type":"post"},{"authors":null,"categories":null,"content":"If you ask engineers across different organizations and industries, you will get many different answers about what an incident is.\nSometimes it is only when a customer is affected.\nOthers will label disruptions as incidents even if a customer never experienced a thing.\nSubjectivity is an unfortunate property of incidents in a lot of cases, even when it comes to identifying severity levels.\n Exploring Fear of Incidents\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"8bf5287def32a4306641abb608612757","permalink":"/post/exploring-subjectivity-of-incidents/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/exploring-subjectivity-of-incidents/","section":"post","summary":"If you ask engineers across different organizations and industries, you will get many different answers about what an incident is.\nSometimes it is only when a customer is affected.\nOthers will label disruptions as incidents even if a customer never experienced a thing.\nSubjectivity is an unfortunate property of incidents in a lot of cases, even when it comes to identifying severity levels.\n Exploring Fear of Incidents","tags":["incidents","human","analysis"],"title":"Exploring Subjectivity of Incidents","type":"post"},{"authors":null,"categories":null,"content":"Can people reach a website or service?\nThis is typically the first thing people think of when talking about reliability. Can \u0026ldquo;the thing\u0026rdquo; be reached (and used) when expected?\nThis is important from the perspective of both external and internal users who depend on a service. Because of this, availability is a good place to start discussing reliability.\nAvailability If a service can\u0026rsquo;t be reached at all then it is of no value to its intended user and in some cases detrimental. Availability lets us communicate whether or not our service is ready to go.\nIn terms of monitoring for availability, we could set alerts for low disk space of service. This could tip us off to an imminent problem that is known to impact the availability of a service.\nWe could also examine logs that tell us in aggregate what the customer is experiencing when attempting to communicate with our service. Are they gaining access or receiving responses? Or are they seeing 404 and 501 error codes?\nAnswering those questions gives a more accurate picture of the availability of your service. Just because everything appears to be working, does not mean that is what your user is seeing.\nYou can\u0026rsquo;t always do something about availability. The number of hops between a website and a person\u0026rsquo;s mobile device is astounding. Failure could happen at any point between them.\nThe closer we can measure that experience from their perspective, the better off we\u0026rsquo;ll be in measuring our availability.\nThe next aspect of reliability that often gets a lot of attention is latency. Despite your service being available, if it takes too long to communicate back and forth, there\u0026rsquo;s no denying it is a bad experience for the user. In the high speed world, patience for slow services is nearly non-existent.\n Exploring the Latency Aspect of Reliability\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"7322b6d462b0a2a0854d93729efc5f14","permalink":"/post/exploring-the-availability-aspect-of-reliability/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/exploring-the-availability-aspect-of-reliability/","section":"post","summary":"Can people reach a website or service?\nThis is typically the first thing people think of when talking about reliability. Can \u0026ldquo;the thing\u0026rdquo; be reached (and used) when expected?\nThis is important from the perspective of both external and internal users who depend on a service. Because of this, availability is a good place to start discussing reliability.\nAvailability If a service can\u0026rsquo;t be reached at all then it is of no value to its intended user and in some cases detrimental.","tags":["reliability","availability"],"title":"Exploring the Availability Aspect of Reliability","type":"post"},{"authors":null,"categories":null,"content":"Did the process that you ran on the data yield the correct or expected result? For each input it produced the expected output.\nThis is an important factor to include in monitoring for reliability.\nNo matter how fast or “always available” your service or site is, if it returns incorrect results, it’s not reliable in the eyes of your customers.\nMonitoring for correctness of results is an important part of monitoring for reliability.\nNext, let\u0026rsquo;s talk about an aspect of reliability that those who work large amounts of data or records are likely to be familiar with, coverage. Did it process all the data?\n Exploring the Coverage Aspect of Reliability\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"cc57015663354c64237436e0f804dcdd","permalink":"/post/exploring-the-correctness-aspect-of-reliability/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/exploring-the-correctness-aspect-of-reliability/","section":"post","summary":"Did the process that you ran on the data yield the correct or expected result? For each input it produced the expected output.\nThis is an important factor to include in monitoring for reliability.\nNo matter how fast or “always available” your service or site is, if it returns incorrect results, it’s not reliable in the eyes of your customers.\nMonitoring for correctness of results is an important part of monitoring for reliability.","tags":["reliability","correctness"],"title":"Exploring the Correctness Aspect of Reliability","type":"post"},{"authors":null,"categories":null,"content":"Coverage refers to how much of the data that you expected to process was actually processed.\nReliability means getting the whole job done, every time. How can we monitor for that in a way that indicates what it\u0026rsquo;s like as a user?\nAnother aspect of reliablity that is closely tied to data is our next one, durability. It is really crucial that data written to the service can be read back out again later when desired.\n Exploring the Durability Aspect of Reliability\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"808eb0d45fd25012dcf8478584605d17","permalink":"/post/exploring-the-coverage-aspect-of-reliability/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/exploring-the-coverage-aspect-of-reliability/","section":"post","summary":"Coverage refers to how much of the data that you expected to process was actually processed.\nReliability means getting the whole job done, every time. How can we monitor for that in a way that indicates what it\u0026rsquo;s like as a user?\nAnother aspect of reliablity that is closely tied to data is our next one, durability. It is really crucial that data written to the service can be read back out again later when desired.","tags":["reliability","coverage"],"title":"Exploring the Coverage Aspect of Reliability","type":"post"},{"authors":null,"categories":null,"content":"Durability generally relates to longevity and resilience. It’s the ability to remain functional over time.\nDurability is especially important in situations such as storage systems where it is really crucial that a bit written to the service can be read back out again later when desired.\n Exploring the Fidelity Aspect of Reliability\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"5588013f2e7ab4088151df4381eb3436","permalink":"/post/exploring-the-durability-aspect-of-reliability/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/exploring-the-durability-aspect-of-reliability/","section":"post","summary":"Durability generally relates to longevity and resilience. It’s the ability to remain functional over time.\nDurability is especially important in situations such as storage systems where it is really crucial that a bit written to the service can be read back out again later when desired.\n Exploring the Fidelity Aspect of Reliability","tags":["reliability","durability"],"title":"Exploring the Durability Aspect of Reliability","type":"post"},{"authors":null,"categories":null,"content":"Fidelity pertains to the ability of your service to continue to provide a reduced or degraded experience when something goes wrong.\nFor example, if different parts of the home page on your website are provided by different microservices, and one of those microservices goes down, ideally you can still serve the home page with only that section missing or replaced with some static content.\nFidelity, then, is the measure of how often the page served that degraded or partial experience in comparison to serving the full page as intended with full fidelity.\nIt\u0026rsquo;s common to architect systems in these graceful degradated states.\nHowever, this means we need to pay attention to just how often and for what duration those degradations are taking place. Users will appreciate the resilient design but lose support if something is always broken from their perspective.\nThe next aspect of reliability applies to users who need real time information, updates, and access to take action on something.\n Exploring the Freshness Aspect of Reliability\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"8ee8d83625b73471ec3c0894375675ee","permalink":"/post/exploring-the-fidelity-aspect-of-reliability/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/exploring-the-fidelity-aspect-of-reliability/","section":"post","summary":"Fidelity pertains to the ability of your service to continue to provide a reduced or degraded experience when something goes wrong.\nFor example, if different parts of the home page on your website are provided by different microservices, and one of those microservices goes down, ideally you can still serve the home page with only that section missing or replaced with some static content.\nFidelity, then, is the measure of how often the page served that degraded or partial experience in comparison to serving the full page as intended with full fidelity.","tags":["reliability","fidelity"],"title":"Exploring the Fidelity Aspect of Reliability","type":"post"},{"authors":null,"categories":null,"content":"Freshness refers to how up-to-date the information is in situations where timeliness matters to the customer.\nFreshness of what is available to users is a big component of reliability to many. If my travel booking tool is out of sync with the airline inventory, it\u0026rsquo;s frustrating to delay the process because something wasn\u0026rsquo;t \u0026ldquo;actually\u0026rdquo; available. Their information and therfore value to me is unreliable.\nExamples would include sports scores or election results, in which the data is constantly and quickly changing. Freshness is not a factor for sites with static content that can remain the same over time, but if your site serves time-sensitive information, customers will expect that content to be kept current if they’re to consider the site reliable.\nThe next one is particularly important when running pipelines or batch processing systems.\n Exploring the Throughput Aspect of Reliability\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"d4adaf6082e8500f74d82fc5308297dd","permalink":"/post/exploring-the-freshness-aspect-of-reliability/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/exploring-the-freshness-aspect-of-reliability/","section":"post","summary":"Freshness refers to how up-to-date the information is in situations where timeliness matters to the customer.\nFreshness of what is available to users is a big component of reliability to many. If my travel booking tool is out of sync with the airline inventory, it\u0026rsquo;s frustrating to delay the process because something wasn\u0026rsquo;t \u0026ldquo;actually\u0026rdquo; available. Their information and therfore value to me is unreliable.\nExamples would include sports scores or election results, in which the data is constantly and quickly changing.","tags":["reliability","freshness"],"title":"Exploring the Freshness Aspect of Reliability","type":"post"},{"authors":null,"categories":null,"content":"Latency refers to the amount of delay between a request and a response.\nSlow is the New Down Today’s users have high expectations.\nThey demand fast performance and have little patience with a site or service that leaves them waiting. Reliability means the site is not only available, but available in a reasonable amount of time.\nEspecially for external customer interactions – for example, when a customer visits your company’s website – it is critical that the service respond as quickly as possible. A slow website can drive your customers to your competitors.\nAzure is always monitoring latency from points all around the world to ensure speeds are within pre-defined limits.\nLet\u0026rsquo;s say your service is available and fast, what else might users care about?\nIf your service returns the wrong thing, it’s not reliable in the eyes of your customers.\nIt\u0026rsquo;s supposed to work as advertised. If it returns the wrong results, calculates a bad value, or generally doesn\u0026rsquo;t do what it\u0026rsquo;s supposed to do \u0026ldquo;correctly\u0026rdquo;, it\u0026rsquo;s not relable to the user. Who wants to use a service that can\u0026rsquo;t calculate for leap year, let alone manage your bank account?\n Take a look at the latency of Azure datacenters from your current location\n \n Another one can be found here:\n \n Exploring the Correctness Aspect of Reliability\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"801860f90e2ab1dec0176092b11ad91d","permalink":"/post/exploring-the-latency-aspect-of-reliability/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/exploring-the-latency-aspect-of-reliability/","section":"post","summary":"Latency refers to the amount of delay between a request and a response.\nSlow is the New Down Today’s users have high expectations.\nThey demand fast performance and have little patience with a site or service that leaves them waiting. Reliability means the site is not only available, but available in a reasonable amount of time.\nEspecially for external customer interactions – for example, when a customer visits your company’s website – it is critical that the service respond as quickly as possible.","tags":["reliability","latency"],"title":"Exploring the Latency Aspect of Reliability","type":"post"},{"authors":null,"categories":null,"content":"Throughput is a measure of the rate at which something is processed, or the number of transactions that a website, application, or service successfully handles over a specified period of time.\nThere are many factors that can affect throughput. Monitoring throughput can help you pinpoint potential problems that impact your users’ experience.\nIt\u0026rsquo;s been said already but worth repeating often; The aspects of reliability must be measured and examined from the customer\u0026rsquo;s perspective. Objective data from as close as you can get to what they are truly experiencing provides the truest depecition of what its like to use your service.\n Measuring Reliability From the Customer\u0026rsquo;s Perspective\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"2309eb4e32ba6593a84f100550737b5b","permalink":"/post/exploring-the-throughput-aspect-of-reliability/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/exploring-the-throughput-aspect-of-reliability/","section":"post","summary":"Throughput is a measure of the rate at which something is processed, or the number of transactions that a website, application, or service successfully handles over a specified period of time.\nThere are many factors that can affect throughput. Monitoring throughput can help you pinpoint potential problems that impact your users’ experience.\nIt\u0026rsquo;s been said already but worth repeating often; The aspects of reliability must be measured and examined from the customer\u0026rsquo;s perspective.","tags":["reliability","throughput"],"title":"Exploring the Throughput Aspect of Reliability","type":"post"},{"authors":null,"categories":null,"content":"In other words \u0026hellip; unplanned Work\nMost of what we as engineers and technologists do is planned work.\nWe spend a lot of time and effort understanding the work in front of us.\nWe calculate story points. We plan sprints. We have a pretty good idea on what we are supposed to be working on.\nSo, when an incident occurs, it is disruptive. It is\u0026hellip; unplanned work.\nOften, we view this as a terrible thing, but in reality, incidents are actually “investments” in supplying the value we are trying to deliver to end users.\nWe just need to change how we look at incidents.\nNext we\u0026rsquo;ll discuss the full lifecycle of an incident. From detection to analysis and everything in between. When we start to view incidents through a new lense and begin analyzing for opportunities to learn, you\u0026rsquo;ll begin to see your systems for what they are and why having a response plan in place makes sense.\nNext steps  Understanding the Full Lifecycle of An Incident\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"a98aed5dcbaf1ec6c8f26b740085df80","permalink":"/post/exploring-why-incidents-are-unexpected/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/exploring-why-incidents-are-unexpected/","section":"post","summary":"In other words \u0026hellip; unplanned Work\nMost of what we as engineers and technologists do is planned work.\nWe spend a lot of time and effort understanding the work in front of us.\nWe calculate story points. We plan sprints. We have a pretty good idea on what we are supposed to be working on.\nSo, when an incident occurs, it is disruptive. It is\u0026hellip; unplanned work.\nOften, we view this as a terrible thing, but in reality, incidents are actually “investments” in supplying the value we are trying to deliver to end users.","tags":["reliability","incidents","response"],"title":"Exploring Why Incidents are Unexpected","type":"post"},{"authors":null,"categories":null,"content":"The post-incident review process begins with the data. That includes the data in the conversations related to the incident as well as the data collected by monitoring systems. You can use Azure tools to collect, find, correlate, and share the data in the course of conducting your learning review.\nIn this unit, you’ll learn how to go about the construction of a shared, accurate chronology that reflects the non-linear nature of an incident. Non-linear refers to the fact that incidents are almost never just a matter of “this happened, and then that happened, then we noticed, then we did something, and then we were done.”\nAs an incident occurs, develops, and plays out, different people get involved. They notice and try different things, and some work, some don’t. If multiple people are working at the same time, all these things can be happening simultaneously. Thus, it’s a bit more complicated than just a straight-line description.\nTo create a logical timeline, then, you need a starting point and the place to start is with the relevant data.\nGather the data Before you can conduct a learning review, you first need to gather data. Specifically, you need to collect as much of the conversation surrounding the event as you can so you can use all of the crucial data contained in it. The conversation among team members that happened during the outage or incident will be one of your richest sources of information.\nYou also should gather from your monitoring system and other places from which the people involved in the incident drew context. What information were they getting from your systems when the incident was going on?\nAnd finally, if possible, it would be helpful for you to get a better picture of what changed just prior to and during the incident because changes are often contributing factors when an incident occurs.\nThe data-gathering step has three parts:\n  Collect the conversation. It’s important to have a place for people to communicate and share what worked and what failed, what they’re hesitant to try, what they’ve tried in the past. This conversation between the people as they work through and solve the issue is your best source of learning.\n  Determine the context. This is done primarily through monitoring. It involves building a point in time to collect the data you have and correlate it to the event now that you’re looking back at it.\n  Find the changes. You do this through activity and audits logs.\n  Data collection tools There are tools that you can use to make these steps easier. Using Azure for these systems can help make you better able to gather all this data quickly.\nAzure DevOps\nYou have data already captured in the Azure DevOps Boards that includes tracking of incidents with information about who was on call, who was assigned to the incident, and so forth. You can also use the Azure DevOps Wiki as a centralized way to pull in some of the pieces of information about both the incident itself and the conversation that happened during the incident.\nGraph Explorer\nYou can also use queries in Graph Explorer to programmatically find, export and bring in the conversation that was collected inside the Teams channel that was devoted to this specific incident.\nDashboards, Application Insights and JSON\nYou can create a dashboard in the Azure portal to correlate what the operators were seeing to the incident itself. Application Insights give you graphs you can add to the dashboard to visualize the information. This allows you to see your tracked metrics side-by-side to better identify where and when the incident happened. You can pull down the dashboard and its components using JSON, and, for example, focus on a specific window of time and drill down to provide more context for your investigation.\nLog Analytics and Audit Log\nYou can use information here to discover changes and find the delta between where you started and where you ended.\n Reviewing Azure Monitoring Tools\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"e7b7ef7a2729bf970de85502f89b648b","permalink":"/post/gathering-data-for-the-post-incident-review/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/gathering-data-for-the-post-incident-review/","section":"post","summary":"The post-incident review process begins with the data. That includes the data in the conversations related to the incident as well as the data collected by monitoring systems. You can use Azure tools to collect, find, correlate, and share the data in the course of conducting your learning review.\nIn this unit, you’ll learn how to go about the construction of a shared, accurate chronology that reflects the non-linear nature of an incident.","tags":["incidents","data","analysis"],"title":"Gathering Data For the Post-incident Review","type":"post"},{"authors":null,"categories":null,"content":"The remediation phase is the blurriest of them all. A big reason is that sometimes there\u0026rsquo;s no difference between what takes place during the response and an action intended to improve the situation (i.e. remediation step).\nMuch of incident response is just trial and error, quite honestly. We quickly think through what to do, we do it, we hope for quick feedback, we examine if things improved, and we iterate.\nBecause of this, measuring the remediation phase is a bit trickier.\nWhat we are looking for is to determine the distinction between when we have identified the fix (or series of fixes) that will result in recovery of services and the end of the incident. What is the time between? Good or bad, it\u0026rsquo;s data to potentially discuss and discover revelations.\nIt\u0026rsquo;s not until the analysis phase that engineers can definitively determine the exact point along the incident timeline that everyone agrees the problem and solution were both understood.\nThis underlines the importance of not only capturing the timeline of events, including conversations and actions taken but also analyzing it in retrospect with a diverse audience encouraged to ask questions. Questions that help radiate a broader and more informed knowledge base across an organization.\nOne reason for measuring this way is to set aside the time between definitively knowing what will restore service and when services were actually back.\nLet\u0026rsquo;s say the payment process thing from before was pretty easy to determine. It probably took less than 5 minutes to know it was something with the backend talking to an API and that after someone followed a specific series of steps everything would be fine.\nHowever, the process to do this is not only complicated and requires administrative access, it\u0026rsquo;s not well documented, and what is documented is extremely dated.\nIf this type of problem occurs again, we could shorten the total time of the incident, and therefore cost to the business, simply by making a few small adjustments.\nThese types of opportunities begin to surface when we can ask questions like, \u0026ldquo;how long does the backup script take?\u0026quot;. \u0026ldquo;Was the documentation helpful?\u0026rdquo;\nJust because you can figure out what the problem is in an acceptable amount of time, does not mean your system will recover as quickly as the business needs it to.\nOnce service is restored and things return to normal it\u0026rsquo;s important to set aside time to reflect on what took place, discuss it openly, broadcast what has been learned, and prepare for the future.\nThis takes us to our next phase of the incident lifecycle - analysis.\nNext  Identifying the Analysis Phase of an Incident\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"a113e803c17d7f654878219bdb56af8f","permalink":"/post/identifying-the-incident-remediation-phase/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/identifying-the-incident-remediation-phase/","section":"post","summary":"The remediation phase is the blurriest of them all. A big reason is that sometimes there\u0026rsquo;s no difference between what takes place during the response and an action intended to improve the situation (i.e. remediation step).\nMuch of incident response is just trial and error, quite honestly. We quickly think through what to do, we do it, we hope for quick feedback, we examine if things improved, and we iterate.","tags":["phase","incident"],"title":"Identifyig the Incident Remediation Phase","type":"post"},{"authors":null,"categories":null,"content":"The Communication Coordinator is meant to be the person working in conjunction with the incident commander to share more information beyond those who are in the firefight actively working to recover from the incident itself. That could be customers. It could be the sales and marketing teams. Maybe your customer support. There are many people within an organization who need to be made aware of what’s taking place and the status around how things are progressing. It\u0026rsquo;s always good to put someone in charge of managing that communication and making sure that other stakeholders are aware of what is happening and what’s being done.\nNext steps  Identifying The Scribe\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"98ebba6e17864c2e42de6dbbe5ea45dd","permalink":"/post/identifying-the-communication-coordinator/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/identifying-the-communication-coordinator/","section":"post","summary":"The Communication Coordinator is meant to be the person working in conjunction with the incident commander to share more information beyond those who are in the firefight actively working to recover from the incident itself. That could be customers. It could be the sales and marketing teams. Maybe your customer support. There are many people within an organization who need to be made aware of what’s taking place and the status around how things are progressing.","tags":["role","on-call","communication","chatops","coordination"],"title":"Identifying the Communication Coordinator","type":"post"},{"authors":null,"categories":null,"content":"The post-incident review is where the idea of incidents begin to shift from things that are feared and avoided to things that can provide valuable information to a team and business.\nRarely will you find a business today that doesn\u0026rsquo;t heavily rely on digital services to earn and keep customers. There are going to be problems along the way. Not only that, but customers expect improvements, technology changes, competitors get smarter. There are fewer and fewer industries that can maintain the status quo and continue to exist, let alone prosper.\nThe analysis phase allows for an honest and open retrospective discussion about what took place. We as an organization want to understand the realities of the scenario from an objective perspective.\nExactly how you conduct the exercises will vary but the focus of the conversation is on what and how things happened rather than who and why.\nBy identifying the incident timeline as well as the specific highlights, people can identify the beginning and end of each phase, including the conversations that took place.\nThis helps isolate specific areas of improvements such as moving away from using email distribution lists as the default channel and method of delivering actionable alerts. In discussions about the detection phase, it seemed clear to everyone that the problem could have been solved sooner had they known about it sooner. Sometimes even small changes can have a huge impact on improving the overall time to recover.\nRegardless of how businesses choose to perform their post-incident review, they should take place no more than 36-48 hours after the incident has concluded.\nWe are looking to collect as much objective data as well as testimonial from a diverse set of perspectives. It\u0026rsquo;s difficult to remember what took place in much detail after a couple of days.\nIf possible, the exercise should be facilitated by someone that was not involved in the incident. Someone who can remove themselves from the timeline of events and perspectives of what took place. Objective data is often easier to obtain when someone else asks the questions and encourages an honest conversation.\nThe point of a post-incident review isn\u0026rsquo;t to end up with a document or artifact that the meeting took place. It\u0026rsquo;s not an exploration on what definitively caused the issue in the first place.\nOur systems are always changing. That\u0026rsquo;s just where we are today. Businesses that are providing some kind of service to their customers where technology is involved are required to make constant changes. It doesn\u0026rsquo;t matter if it\u0026rsquo;s in the cloud, data center, or closet, servers need improvements and replacements. Operating systems need patches and upgrades. Applications need to be updated and restarted. Databases and logs are changing and growing. Networks are coming and going. There\u0026rsquo;s a lot going on and it\u0026rsquo;s often difficult to put our thumb on exactly what\u0026rsquo;s causing what.\nThe good news is, it\u0026rsquo;s ok.\nPart of the advantage of examining incidents in phases means that regardless to the problems we experience in the future, we know that we can detect a problem, get the right people involved, and recover services faster than before. We are prepared for the infinite world of possibilities we like to call the \u0026ldquo;unknown unknowns\u0026rdquo;.\nThis takes us to our final phase of the incident lifecycle, readiness.\nNext Steps  Identifying the Readiness Phase of an Incident\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"6c3deeb10f123737de09e154e081d3ea","permalink":"/post/identifying-the-incident-analysis-phase/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/identifying-the-incident-analysis-phase/","section":"post","summary":"The post-incident review is where the idea of incidents begin to shift from things that are feared and avoided to things that can provide valuable information to a team and business.\nRarely will you find a business today that doesn\u0026rsquo;t heavily rely on digital services to earn and keep customers. There are going to be problems along the way. Not only that, but customers expect improvements, technology changes, competitors get smarter.","tags":["incidents","on-call","analysis","phase"],"title":"Identifying the Incident Analysis Phase","type":"post"},{"authors":null,"categories":null,"content":"Another key role to identify, in many cases, is the incident commander. An incident commander can be extremely helpful when you have got a large-scale outage that effects a lot of different components or requires coordination across many teams and different systems. They are great for making sure that engineers stay focused and they are working on their own remediation efforts\u0026hellip; Ensuring people are not stepping on each other or undoing each other\u0026rsquo;s work. It is good to have a central person who can track what is going on and who is doing what.\nNext steps  Identifying the Communication Coordinator\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"fce996bbc2e50bb9fae30c21ee65853f","permalink":"/post/identifying-the-incident-commander/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/identifying-the-incident-commander/","section":"post","summary":"Another key role to identify, in many cases, is the incident commander. An incident commander can be extremely helpful when you have got a large-scale outage that effects a lot of different components or requires coordination across many teams and different systems. They are great for making sure that engineers stay focused and they are working on their own remediation efforts\u0026hellip; Ensuring people are not stepping on each other or undoing each other\u0026rsquo;s work.","tags":["role","on-call","coordination","communication"],"title":"Identifying the Incident Commander","type":"post"},{"authors":null,"categories":null,"content":"The methods used to determine when we have a problem have changed over the years.\nAlerting a person to a spike in CPU usage isn\u0026rsquo;t as valuable these days. Especially those in the process of adopting the cloud. Instead, we want to know when our customer is experiencing a problem while using our system.\nThe problems will vary but the methods used to determine when a human needs to get involved have evolved.\nBy monitoring systems in a way that matches the customer\u0026rsquo;s perspective we can see when they experience a problem rather than we think we experienced a problem.\nIf the customer is experiencing an issue, that\u0026rsquo;s far more important to the business than any spike in CPU usage.\nIn today\u0026rsquo;s connected world, no matter how complex or simple a system appears to be there is much more that goes in to what a user experiences.\nIt\u0026rsquo;s entirely possible that all systems appear healthy and no alarms are going off when in reality users aren\u0026rsquo;t able to complete a shopping purchase due to a third party payment processor. No amount of monitoring for memory or network performance would have tipped off engineers or leadership to this business impacting problem.\nEvery system is different and while there may be legitimate reasons to set up alerts for problems at the component level. However, by and large if we are planning to get engineers involved (especially outside of office hours) then we need to make sure the problem is real, it\u0026rsquo;s impacting the business, and it requires human intervention immediately.\nIf an alert isn\u0026rsquo;t actionable - meaning it requires a person or group of people to respond and investigate right away then it\u0026rsquo;s not an incident.\nIf we can measure some minor details about when amd how we detect problems in the first place then we can look for opportunities to improve.\nIn conversations about what took place with the payment processor incident it is reasonable to ask \u0026ldquo;how could we have detected this sooner?\u0026quot;.\n\u0026ldquo;How could we have detected this \u0026hellip; at all?\u0026rdquo; may be a better question.\nNext steps  Identifying the Response Phase of an Incident\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"ab9195c4e46c8e0e2cc614a024f0454c","permalink":"/post/identifying-the-incident-detection-phase/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/identifying-the-incident-detection-phase/","section":"post","summary":"The methods used to determine when we have a problem have changed over the years.\nAlerting a person to a spike in CPU usage isn\u0026rsquo;t as valuable these days. Especially those in the process of adopting the cloud. Instead, we want to know when our customer is experiencing a problem while using our system.\nThe problems will vary but the methods used to determine when a human needs to get involved have evolved.","tags":["phase","incident"],"title":"Identifying the Incident Detection Phase","type":"post"},{"authors":null,"categories":null,"content":"During and after a post-incident review many ideas will surface around how to improve not only various aspects of each phase of the lifecycle but also how the team can improve in other areas. Communication for example.\nDuring the review, engineers might have pointed out that there were long gaps in the conversation timeline where nobody said anything. It\u0026rsquo;s helpful to be verbose in what engineers are doing, thinking, even feeling. If someone isn\u0026rsquo;t completely comfortable following the steps from documentation, we should address that. Who else on the team carries fears about performing actions on the system during an incident? We want our team to be confident and ready.\nSo, what did we learn that helps improve that readiness?\nAction items aren\u0026rsquo;t really the point of a post-incident reivew but inevitably, creative ideas will emerge. Some engineering efforts will make sense to schedule and implement as a result of the conversations. Adding telemetry to help keep a better eye on the credit card processing system, for example.\nProduct and engineering teams should work together to prioritize and schedule work for those enhancements. Tradeoffs will be made since the uptime of a new feature is just as important as the feature itself. Ultimately, what\u0026rsquo;s best for the business is what leadership will have to wrestle with.\nThe bigger win that helps with our readiness efforts is that we have measurements by which we can set goals around.\n Understanding the Foundations of Incident Response\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"777a2e68f8a83d198d6e397b7ff1bd99","permalink":"/post/identifying-the-incident-readiness-phase/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/identifying-the-incident-readiness-phase/","section":"post","summary":"During and after a post-incident review many ideas will surface around how to improve not only various aspects of each phase of the lifecycle but also how the team can improve in other areas. Communication for example.\nDuring the review, engineers might have pointed out that there were long gaps in the conversation timeline where nobody said anything. It\u0026rsquo;s helpful to be verbose in what engineers are doing, thinking, even feeling.","tags":["phase","incident"],"title":"Identifying the Incident Readiness Phase","type":"post"},{"authors":null,"categories":null,"content":"The first role we need to talk about is the “Primary Responder” – the Primary “On-call” engineer.\nThis person is expected to acknowledge their awareness of an incident once the alert has been received.\nNext steps  Identifying the Secondary Responder\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"5c60570f8df3445498da00a54db40497","permalink":"/post/identifying-the-primary-responder/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/identifying-the-primary-responder/","section":"post","summary":"The first role we need to talk about is the “Primary Responder” – the Primary “On-call” engineer.\nThis person is expected to acknowledge their awareness of an incident once the alert has been received.\nNext steps  Identifying the Secondary Responder","tags":["role","response"],"title":"Identifying the Primary Responder","type":"post"},{"authors":null,"categories":null,"content":"Once our detection efforts have been configured to send actionable alerts to the people who build the systems, we need to make sure they are sending those alerts to the right people.\nRight People How do you know who the right people are? In most cases it is situational. A few things that can be done to help establish some formatlity and standard around responding to incidents is through the use of roles, rosters, and rotations. We\u0026rsquo;ll go more in depth on what each of those are soon.\nTooling The right person for the job needs the right tools for the job. If someone is responding to an issue they need to get busy immediately. Making sure the right monitoring, communications, access, and documentation is provided is also important. People should be familiar with the tooling and know how and where to find additional resources to help diagnose, theorize, and triage.\nDiagnose Everyone experiences problems. Sometimes routinely throughout the day in fact. When something doesn\u0026rsquo;t go as expected or breaks entirely our impulse is to fix it. In order to do so we must first have a look at what\u0026rsquo;s currently observable. What is the status? Who and what is impacted? What hints or clues are there? What information do we have to work with?\nWhat do we know right now?\nTheorize Once information has been obtained, we begin to theorize next best steps. What action can we take to minimize or stop the impact? What are the repurcussions of that action? Will something else go wrong? If we take one action, what result do we expect? In very brief moments we are creatively thinking through as many possible scenarios to restore service as we can. And then stack ranking them based on our own calculations on the probability of success.\nTriage At some point we all need help. That could be access to an admin account, theories from subject matter experts, someone to amplify updates to a broader audience. Rarely are incidents viewed as a success if only a single person was involved.\nRegardless of the size of your response team, by isolating it as a phase in the incident lifecycle, we can examine this section of the timeline for improvements on how we coordinate our response. If it took an excessive amount of time for the engineering team to correct the payment processor problem simply because it took too long to find the right person, with the right tool, and with the appropriate level of access then there are some clear opportunities for improvement right there.\n Identifying the Remediation Phase of an Incident\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"dfc348c8f9e3616308a1421d980243ee","permalink":"/post/identifying-the-incident-response-phase/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/identifying-the-incident-response-phase/","section":"post","summary":"Once our detection efforts have been configured to send actionable alerts to the people who build the systems, we need to make sure they are sending those alerts to the right people.\nRight People How do you know who the right people are? In most cases it is situational. A few things that can be done to help establish some formatlity and standard around responding to incidents is through the use of roles, rosters, and rotations.","tags":["phase","incident","response"],"title":"Identifying the Response Phase of an Incident","type":"post"},{"authors":null,"categories":null,"content":"The scribe’s role is to document the conversation in as much detail as possible. Teams commonly use phone bridges, conference calls, or video chat to get everyone together and try to understand what is going on, which can certainly help create space for the conversation. However, it is difficult for us to go through and understand in detail what the engineers were saying and doing unless it is transcribed. As a result, a scribe is that person who can help us document as much as possible to review later. What were people saying, doing, feeling, and even experiencing? It is all data to be analyzed – but only if we capture it.\nNext steps  Identifying the Subject Matter Experts\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"d5fa7188dfba634922f2ef15fe5e49e6","permalink":"/post/identifying-the-scribe/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/identifying-the-scribe/","section":"post","summary":"The scribe’s role is to document the conversation in as much detail as possible. Teams commonly use phone bridges, conference calls, or video chat to get everyone together and try to understand what is going on, which can certainly help create space for the conversation. However, it is difficult for us to go through and understand in detail what the engineers were saying and doing unless it is transcribed. As a result, a scribe is that person who can help us document as much as possible to review later.","tags":["role","communication","analysis"],"title":"Identifying The Scribe","type":"post"},{"authors":null,"categories":null,"content":"Then we have the secondary responder – who is there as back up -Another engineer who can step in if the primary responder is unavailable or unreachable. Or if they just need another pair of eyes.\nNext steps  Identifying The Incident Commander\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"17f02fa4586ba15e4cd4a9a7cbbcfb3f","permalink":"/post/identifying-the-secondary-responder/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/identifying-the-secondary-responder/","section":"post","summary":"Then we have the secondary responder – who is there as back up -Another engineer who can step in if the primary responder is unavailable or unreachable. Or if they just need another pair of eyes.\nNext steps  Identifying The Incident Commander","tags":["role","response"],"title":"Identifying the Secondary Responder","type":"post"},{"authors":null,"categories":null,"content":"It’s quite common within on-call rosters to identify subject matter experts, so that early responders know who to escalate too quickly. These people should not be on call all the time, of course, but we do want to be able to identify who is our database expert. Who is our front-end expert? Who are the people that we can reach out to if our primary and secondary responders are not able to diagnose and resolve the issue themselves?\nNext steps  Establishing On-call Rosters\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"03f85de3af58e24cbd6e60c89e4d8676","permalink":"/post/identifying-the-subject-matter-experts/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/identifying-the-subject-matter-experts/","section":"post","summary":"It’s quite common within on-call rosters to identify subject matter experts, so that early responders know who to escalate too quickly. These people should not be on call all the time, of course, but we do want to be able to identify who is our database expert. Who is our front-end expert? Who are the people that we can reach out to if our primary and secondary responders are not able to diagnose and resolve the issue themselves?","tags":["role","collaboration"],"title":"Identifying the Subject Matter Experts","type":"post"},{"authors":null,"categories":null,"content":"Although thinking of incidents in terms of phases allows for us to shorten each in their own unique ways, responding to and remediating an incident often begin to blur. Especially when actions to mitigate or improve the situation, have the opposite result.\nNow that we’ve covered the foundations of building a good incident response plan, let\u0026rsquo;s talk about remediation efforts and how Supplying Context \u0026amp; Guidance to on-call engineers rather than step by step procedures can dramatically help reduce the impact of an incident.\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"bcb465ef073112aa3fe8278738afa010","permalink":"/post/improving-the-remediation-of-incidents/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/improving-the-remediation-of-incidents/","section":"post","summary":"Although thinking of incidents in terms of phases allows for us to shorten each in their own unique ways, responding to and remediating an incident often begin to blur. Especially when actions to mitigate or improve the situation, have the opposite result.\nNow that we’ve covered the foundations of building a good incident response plan, let\u0026rsquo;s talk about remediation efforts and how Supplying Context \u0026amp; Guidance to on-call engineers rather than step by step procedures can dramatically help reduce the impact of an incident.","tags":["phase","incident","remediation"],"title":"Improving the Remediation of Incidents","type":"post"},{"authors":null,"categories":null,"content":"Chatops is the use of tools from within a group chat tool.\nOne great example of Chatops is when engineers can update important information regarding an incident to a broader audience, possibly even to affected users.\nAllowing people to quickly update stakeholders by typing a few quick commands from within the same environment as the related conversations has a number of clear benefits.\nLet\u0026rsquo;s take a look at Building a Serverless Status Page Solution with Azure Functions, blob storage, and Microsoft Teams to illustrate.\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"db8acd14b1eb5395627e2deb62626675","permalink":"/post/managing-tasks-from-group-chat-chatops/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/managing-tasks-from-group-chat-chatops/","section":"post","summary":"Chatops is the use of tools from within a group chat tool.\nOne great example of Chatops is when engineers can update important information regarding an incident to a broader audience, possibly even to affected users.\nAllowing people to quickly update stakeholders by typing a few quick commands from within the same environment as the related conversations has a number of clear benefits.\nLet\u0026rsquo;s take a look at Building a Serverless Status Page Solution with Azure Functions, blob storage, and Microsoft Teams to illustrate.","tags":["chatops","devops","communication","collaboration"],"title":"Managing Tasks from Group Chat - ChatOps","type":"post"},{"authors":null,"categories":null,"content":"Are you familiar with the acronym TTR?\nIt is known as the “Time to Recover” or often referred to as the “time to remediate” or “time to restore.”\nIn other words, the total time that it takes for engineers to bring services back online… with regards to the value provided to end users and customers. It is the total duration of time for the incident?\nThe time to recover will vary from incident to incident and the circumstances around what contributed to the problem will rarely repeat. Because of this, measuring the TTR in aggregate can be a misleading metric.\nThe Mean Time to Recover does not reflect a valuable performance metric on either the uptime of your systems or how well (or poorly) teams can respond to and remediate service disruptions.\nWhile not a perfect metric, it is one in which organizations can begin to measure how teams are performing when it comes to responding to incidents individually. We will need to examine the entire incident timeline to gain a broader understanding of what took place and where improvements can be made.\nMost anyone who works with technology will agree that the complete prevention of problems is not very realistic.\nInstead, we must do better at knowing when something is wrong and being able to respond to it in an effective way.\nLet\u0026rsquo;s now look at how Creating Custom Incident Reports and Charts in Azure Boards will allow us to keep a pulse on how our incident management response is going.\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"bd994e2c51ef95a5100cf1b7e8d0c387","permalink":"/post/measuring-incident-response/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/measuring-incident-response/","section":"post","summary":"Are you familiar with the acronym TTR?\nIt is known as the “Time to Recover” or often referred to as the “time to remediate” or “time to restore.”\nIn other words, the total time that it takes for engineers to bring services back online… with regards to the value provided to end users and customers. It is the total duration of time for the incident?\nThe time to recover will vary from incident to incident and the circumstances around what contributed to the problem will rarely repeat.","tags":["incidents"],"title":"Measuring Incident Response","type":"post"},{"authors":null,"categories":null,"content":"These eight components cover a big part of reliability. Not all of the factors will apply in every situation.\nWhen considering these factors, the most important point to remember is (again) that reliability has to be measured from the customer’s perspective, not the component perspective. Knowing our CPU\u0026rsquo;s running at over 90% utilization has no correlation to what our users are experiencing, let alone doing.\n Understanding Measurements of Reliability\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"b2417b562d7996ab092997bde1575778","permalink":"/post/measuring-reliability-from-the-customers-perspective/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/measuring-reliability-from-the-customers-perspective/","section":"post","summary":"These eight components cover a big part of reliability. Not all of the factors will apply in every situation.\nWhen considering these factors, the most important point to remember is (again) that reliability has to be measured from the customer’s perspective, not the component perspective. Knowing our CPU\u0026rsquo;s running at over 90% utilization has no correlation to what our users are experiencing, let alone doing.\n Understanding Measurements of Reliability","tags":["incidents","reliability"],"title":"Measuring Reliability From the Customer's Perspective","type":"post"},{"authors":null,"categories":null,"content":"Sustainably achieving an appropriate level of reliability first requires that you know what’s going on with your systems and services. You find that out by monitoring. Before you can effectively monitor for reliability, however, you must have a reasonable level of operational awareness. This means you need to understand how systems in production are functioning in order to work toward reliability of those systems.\nToday’s production environments and the paths by which we deploy systems and applications are complex. Thus, it is not uncommon to have to do a bit of discovery to obtain an operational awareness baseline.\nSome questions to ask include:\n  What exactly is running in production?\n  Given a specific application, what are its component parts?\n  Which of those parts communicate with which other parts?\n  There are also some more complex factors that you’ll want to examine to give you a better understanding of your systems and services and how to make them more reliable.\n  How did the app or service perform in the past? It’s true that “past performance is no guarantee of future results.” However, knowledge about past performance can be useful in calibrating expectations, and awareness of past outages can provide you with a sense of potential failure modes that you should incorporate into your thought processes around reliability.\n  What is the socio-technical context? In other words, who owns or cares about the service or app? How did it get deployed? Information about the stakeholders, as well as knowing, for example, whether it was deployed by hand or via an automated process can have many ramifications when we begin to make updates to improve reliability.\n  The answers to all these questions will help you to build a baseline around “normal” behavior.\n Monitoring For Reliability\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"cfe1fb14218dbcbd410a70ff3e3cbc3f","permalink":"/post/monitoring-for-operational-awareness/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/monitoring-for-operational-awareness/","section":"post","summary":"Sustainably achieving an appropriate level of reliability first requires that you know what’s going on with your systems and services. You find that out by monitoring. Before you can effectively monitor for reliability, however, you must have a reasonable level of operational awareness. This means you need to understand how systems in production are functioning in order to work toward reliability of those systems.\nToday’s production environments and the paths by which we deploy systems and applications are complex.","tags":["monitoring","reliability"],"title":"Monitoring For Operational Awareness","type":"post"},{"authors":null,"categories":null,"content":"The world today is extremely connected.\nDigital services have become so embedded in our daily lives that when they become unavailable, it often has an adverse effect on our own livelihoods. Like electricity and running water, we don\u0026rsquo;t realize our reliance on it until it\u0026rsquo;s suddenly unavailable.\nMonitoring Monitoring is a way of collecting information about what is going on in your systems so you can improve things as well as make objectively informed decisions. Azure Monitor is an example of tooling that allows this type of collection and analysis.\nMany aspects of a system can be monitored. From the component level (CPU, Memory) to business impact and everything in between. We might gauge performance, examine security, or (in our case) evaluate and improve reliability.\nReliability When monitoring for reliability, we are trying to proactively manage not only the availability of a service, but many aspects of reliability.\nDepending on the service and the expectations of the end user, some aspects of reliability may be more valuable than others.\n Exercise: Discuss and document perceived expectations users have regarding the reliability of your service?   To determine what to monitor, continue with Examinging All Aspects of Reliability\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"1a90e519632ba0afa5132ee3ef1d0501","permalink":"/post/monitoring-for-reliability/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/monitoring-for-reliability/","section":"post","summary":"The world today is extremely connected.\nDigital services have become so embedded in our daily lives that when they become unavailable, it often has an adverse effect on our own livelihoods. Like electricity and running water, we don\u0026rsquo;t realize our reliance on it until it\u0026rsquo;s suddenly unavailable.\nMonitoring Monitoring is a way of collecting information about what is going on in your systems so you can improve things as well as make objectively informed decisions.","tags":["monitoring","reliability"],"title":"Monitoring For Reliability","type":"post"},{"authors":null,"categories":null,"content":"Azure Monitor includes a number of features and tools you can use to monitor and measure reliability, using data from many different sources.\nThis article’s focus is to help you keep your “eyes on the prize” – the goal of improving reliability in your organization. You should now understand what reliability is and why it’s important. You know that operational awareness is an essential precursor to monitoring for reliability, and that you need to establish a baseline of “normal” behavior as a first step.\nNow you’ll look at the practical question of how to go about doing that. As with all complex tasks, the right tools can help you get the job done more quickly and easily. In this unit, you’ll get a quick introduction to Azure Monitor, Application Insights, and Log Analytics.\nAzure Monitor Azure Monitor is a comprehensive platform for monitoring Azure resources to gain insights into your applications, infrastructure, and network. In this module, the focus will be on the Azure Monitor tools that you can use to monitor and measure reliability.\nA look at Azure Monitor starts with the data that comes into the system. Azure Monitor takes in data from a number of different sources. These include:\n  Data from applications\n  Data from the various operating systems running in Azure\n  Information fed from Azure resources, subscriptions, and tenants\n  Custom data of any sort and from any source\n  There are many things we can do with that information: analyze it, visualize it, integrate it, and respond to it when things go wrong.\nData types The data that comes into Azure Monitor can be divided into two types:\n  Metrics: small numerical pieces of information from counters, gauges, and so forth that are collected on a regular basis\n  Log data: information gathered from many different logs such as Windows event logs, Linux syslog, agents running on virtual machines, custom logs, telemetry from Application Insights, and more.\n  In this module, we will be focusing primarily on log data.\nYou can create, import, and export dashboards, using JSON files, to help you better visualize the data and provide you with operational awareness about all aspects of your Azure subscription, including reliability information. You can also set access controls on the dashboards and share them with others.\n Examining Common Traps\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"b1c38d5dcd617934962d1097096c22ab","permalink":"/post/reviewing-azures-monitoring-tools/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/reviewing-azures-monitoring-tools/","section":"post","summary":"Azure Monitor includes a number of features and tools you can use to monitor and measure reliability, using data from many different sources.\nThis article’s focus is to help you keep your “eyes on the prize” – the goal of improving reliability in your organization. You should now understand what reliability is and why it’s important. You know that operational awareness is an essential precursor to monitoring for reliability, and that you need to establish a baseline of “normal” behavior as a first step.","tags":["monitoring","incident","tools","azure"],"title":"Reviewing Azure's Monitoring Tools","type":"post"},{"authors":null,"categories":null,"content":"We want to develop a sense making approach to the present situation in order to act upon it better in the future\u0026hellip; in the moment.\nThere are different ways to conduct a useful post-incident review, but there are some common practices that can make the process easier and more effective. If you run a facilitated review meeting, keep your review and planning meetings separate, ask better questions, and identify how things went right, you can learn more and use what you learn to improve the reliability of your systems, services, and processes.\nNow you’re aware of some of the common pitfalls that can sabotage your post-incident review, and you know what not to do. The next logical question is “what should you do instead?”\nIn this unit, you’ll learn about four helpful practices that can make your response team better and improve the post-incident analysis process.\nPractice 1: Run a facilitated post-incident review You already know that a post-incident review is not a document or report, so it follows that having one person write up a “postmortem” of what happened doesn’t make for a helpful post-incident review. No matter how knowledgeable or how deeply involved in the incident that person might be, not much will be learned if everything comes from a single viewpoint.\nGetting those who were involved in the incident together at the same time is the first step. But there is something else you can do to increase the useful learning that comes out of the review even more.\nA facilitator is someone or something that makes an action or process easier. To make the post-incident review process both easier and more effective, you should plan to have a post-incident review meeting with a neutral facilitator – someone who was not involved in the response to this incident.\nEveryone will learn more if the facilitator cannot be seen to have any preconceived ideas or a personal agenda in telling the story of the incident.\nThe exact format for the meeting will depend on your team, scheduling, and the nature of the incident, but here are some more basic guidelines:\n  Meetings, not marathons: the meetings don’t have to be long. Generally, sixty to ninety minutes is the maximum length of time most people can fully concentrate and participate effectively, so limit the meeting to not more than an hour and a half.\n  Pre-meeting prep: to make better use of the meeting time, it can be helpful for the facilitator to prepare by conducting one-to-one interviews with some of the members of the response team to get an overview of the incident and ideas about which topics to talk about in the meeting. Individual interviews are also appropriate with certain personalities, when presenting in front of the room is a cause for discomfort.\n  Not required for every incident: this is a learning process, and you’re “learning to learn,” so start small. You don’t have to do this for every incident. You can pick and choose. You might want to start with smaller incidents or start with a review meeting only once per month.\n  The post-incident review meeting is an opportunity to find out what went wrong, what was done right, and how failures can be handled better in the future. The ultimate goal is to improve reliability.\nPractice 2: Ask better questions You already know that language matters, and in the post-incident review, this applies especially to the questions you ask. Objective questions will usually elicit more useful answers.\nIn particular, it’s better to ask people “how” or “what” instead of “why.”\nWhen people are asked to explain “why” they did something or “why” something happened, it tends to put them on the defensive. Beginning a question with “why” often comes across as a judgement, criticism, or accusation. It forces people to justify their actions, and people don’t always know why they did something, or why something happened as a result of their actions.\nThis doesn’t mean you can’t explore the reasons the incident occurred or the reasoning a person used to decide what to do in response to it. It just means you should pay attention to how you word those questions:\nDon’t ask “why did you do that?”\nInstead, ask “what factored into your decision to make that change?”\nDon’t ask “why wasn’t this caught in canary?”\nInstead, ask “how effective is canary at catching this sort of issue, usually?”\nRemember that the post-incident review is about learning. Each participant in the incident is likely to have had a slightly different view on events. You’ll learn more if you ask questions that expose these multiple views and interpretations.\nDon’t get tunnel vision. Sometimes focusing too narrowly on the incident in question will cause you to miss valuable information. You will often learn as much by asking about how work “normally” happens as you will by asking about the specific incident.\nTo learn more about how to ask better questions, check out this resource:\n The Etsy Debriefing Facilitation Guide\nPractice 3: Ask how things went right When you think about learning from failure, you may forget that even within a major outage or other incident, in addition to the things that go wrong there are also things that go right. Far from our view of incidents as one-offs or products of extreme conditions, in most complex systems things go wrong for many of the same reasons they go right.\nIt’s human nature to focus your questions on the negative side of the equation. However, also asking about how things went right will provide you with insights that you would not have gotten otherwise.\nDon’t just ask how the outage happened.\nAsk how your team recovered the systems.\nYou want to know what insights, tools, skills, and people helped in the recovery effort. These are things you want to be able to reproduce, so this information will be valuable in planning what to do going forward.\nIn this context, you want to ask how people came to know what they knew and on what basis they made the decisions they made. Was there a critical moment when someone shared a piece of information that helped unlock the puzzle of what was happening? How did they know to do that? Where did the information come from?\nLook for themes and patterns. Finally, as part of recognizing what went right, ask What do you know now that you didn’t know previously? If learning resulted from the incident and from the incident response and review processes, that’s another thing that went right.\nPractice 4: Keep review and planning meetings separate After you’ve resolved the immediate incident, you’re naturally going to want to talk about repair items and future mitigation – and you should – but these topics should not be a part of your post-incident review meeting. Your post-incident review meeting has a purpose and allowing the discussion of repair items into that meeting will distract from that purpose.\nThe best plan is to discuss repair items and planning issues in a separate meeting a day or two after your post-incident review. You might want to do this with a smaller group. Both of the meetings will be more productive as a result of this separation.\nIt will help you keep the focus of the post-incident review where it belongs, on what actually happened. In addition, allowing a day or two of “soak time” will result in better “synthesis,” giving your subconscious time to work on the issues. This will help you to identify the most “energy-efficient” repair items – those that require minimum energy for maximum impact.\n Gathering Data for the Post-incident Review\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"5ef2dbbd65366f61cd7e7f16f423445a","permalink":"/post/running-a-post-incident-review/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/running-a-post-incident-review/","section":"post","summary":"We want to develop a sense making approach to the present situation in order to act upon it better in the future\u0026hellip; in the moment.\nThere are different ways to conduct a useful post-incident review, but there are some common practices that can make the process easier and more effective. If you run a facilitated review meeting, keep your review and planning meetings separate, ask better questions, and identify how things went right, you can learn more and use what you learn to improve the reliability of your systems, services, and processes.","tags":["incident","analysis"],"title":"Running a Post-incident Review","type":"post"},{"authors":null,"categories":null,"content":"Alerts play an important role in your reliability monitoring strategy, but in order to be helpful, they must be properly constructed for situations that warrant immediate human attention, and they should be devised with simplicity, scope, and context in mind.\nPreferences on how alerts are delivered can be designed using Action Groups in Azure.\nYou have learned how to monitor and interact with indicators of the reliability of your systems and create reliability goals, but there is also an important way by which reliability interacts with you. That’s through Azure Monitor’s log alerts feature.\nIt’s easy to create log alerts using Azure Monitor where the signal is a log query in Log Analytics or Application Insights. However, there is a pitfall that you’ll want to avoid, to prevent derailing all the effort you have put into bringing SLIs and SLOs into your organization.\nTo understand this potential pitfall, review the definition of SRE:\n“Site Reliability Engineering is an engineering discipline devoted to helping organizations sustainably achieve the appropriate level of reliability in their systems, services, and products.”\nAlerts are designed to notify you when there is a problem with your systems. However, when alerts are improperly configured, this can undermine your goal of sustainability. Log alert rules are stateless; they work only on the logic that you build into the query and they send an alert whenever the alert condition is “true.” Thus, it’s important to put some thoughts into constructing your alerts.\n Defining Alerts\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"9cb23c9ea9581381f7ad2e34bd4e420d","permalink":"/post/sending-actionable-alerts/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/sending-actionable-alerts/","section":"post","summary":"Alerts play an important role in your reliability monitoring strategy, but in order to be helpful, they must be properly constructed for situations that warrant immediate human attention, and they should be devised with simplicity, scope, and context in mind.\nPreferences on how alerts are delivered can be designed using Action Groups in Azure.\nYou have learned how to monitor and interact with indicators of the reliability of your systems and create reliability goals, but there is also an important way by which reliability interacts with you.","tags":["on-call","alerts","foundations"],"title":"Sending Actionable Alerts","type":"post"},{"authors":null,"categories":null,"content":"During an active incident, it\u0026rsquo;s often difficult to recall information such as system names, IP addresses, admin logins, location of logs, and more. Remembering where to find dasbhoards, metrics, and reports that could be helpful is often difficult.\nTo build a proactive response plan to managing incidents it\u0026rsquo;s important to provide some useful context and guidance for the first responders to begin investigating, triaging, and escalating.\nWhere should the first responder start? Is there a dashboard that provides a good representation of the health of the system from the customer\u0026rsquo;s perspective? Do responders know how to access it? Does it require a login? Can it be viewed from a mobile device, remotely? These are all considerations for making it easier for on-call responders to begin restoring service as quickly as possible.\nTypically, first responders will need the assistance of others with additional expertise and experience. Or maybe just a second pair of eyes. Understanding how on-call engineers should escalate incidents, helps cut out any delay should exacerbated engineers need to reach out for assistance.\nWhen discussing incident response tacticcs, ask yourselves what metrics, tools, links or general resources might be helpful in those early moments?\nOnce we\u0026rsquo;ve answered that question we can begin building resources in a non-prescriptive way. Objective data and tools that can be provided to help assist in the efforts to fix the problems.\nOften \u0026ldquo;Runbooks\u0026rdquo; (or playbooks) serve this purpose, automating some of the remediation steps that are known to help recover from the problem.\nAutomation can be helpful in many ways during an incident. However, tools that provide context and guidance to allow engineers to make their own decisions towards next actions are meant to be more guides than anything.\nNext, let\u0026rsquo;s look at Building Troubleshooting Guides in Azure Monitor\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"2c2e583a520cd80d1fb910eef7daa6c7","permalink":"/post/supplying-context-and-guidance/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/supplying-context-and-guidance/","section":"post","summary":"During an active incident, it\u0026rsquo;s often difficult to recall information such as system names, IP addresses, admin logins, location of logs, and more. Remembering where to find dasbhoards, metrics, and reports that could be helpful is often difficult.\nTo build a proactive response plan to managing incidents it\u0026rsquo;s important to provide some useful context and guidance for the first responders to begin investigating, triaging, and escalating.\nWhere should the first responder start?","tags":["remediation","collaboration"],"title":"Supplying Context and Guidance","type":"post"},{"authors":null,"categories":null,"content":"Did our monitoring systems tell us, or did a customer inform us?\nCapturing whether problems were detected through telemetry or another method, means we can easily identify gaps in our monitoring tools and practices.\nIt also helps early responders to know where the problem is and what is affected. By communicating to the engineer where the problem was first detected, we provide valuable context in their early triaging efforts.\nIncidents are often chaotic and stressful. It\u0026rsquo;s important to stay informed, especially awareness of a problem. Let\u0026rsquo;s talk now about Tracking Who Knows About a Problem\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"1c36e46ad913b144227f92e1daf97c7b","permalink":"/post/tracking-how-we-knew-about-a-problem/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/tracking-how-we-knew-about-a-problem/","section":"post","summary":"Did our monitoring systems tell us, or did a customer inform us?\nCapturing whether problems were detected through telemetry or another method, means we can easily identify gaps in our monitoring tools and practices.\nIt also helps early responders to know where the problem is and what is affected. By communicating to the engineer where the problem was first detected, we provide valuable context in their early triaging efforts.\nIncidents are often chaotic and stressful.","tags":["incidents","tracking"],"title":"Tracking How We Knew About a Problem","type":"post"},{"authors":null,"categories":null,"content":"How Bad is It?\nWe may not have any notion of severity or impact and there is no place for us to find out how bad the problem really is, and who is affected. These are tough questions to answer if nothing is tracked.\nThere are many people who care about how things are going during an incident. We call them stakeholders. From CEO\u0026rsquo;s to sales teams, many more people outside of the engineering team want, need, and deserve to know \u0026ldquo;How bad is it?\u0026quot;.\nRegardless to if a severity level is assigned, it\u0026rsquo;s important to capture what impact the customer is experiencing. What additional failures might we expect? Being transparent around what is known to be impacted from what is known about the problem allows and empowers others to take action to mitigate or minimize downstream affects.\nThere are great tools out there for tracking the details of incidents. Pagerduty, VictorOps and others have services specifically designed for this.\nUnder the surface, they are similar to ticketing systems or project tracking tools such as Jira and Azure Boards.\nNext, I\u0026rsquo;ll show you how Creating an incident tracking tool with Azure Boards can be done in just a few steps for free.\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"69b2f5ec88b16d9be60900949873b978","permalink":"/post/tracking-impact-of-a-problem/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/tracking-impact-of-a-problem/","section":"post","summary":"How Bad is It?\nWe may not have any notion of severity or impact and there is no place for us to find out how bad the problem really is, and who is affected. These are tough questions to answer if nothing is tracked.\nThere are many people who care about how things are going during an incident. We call them stakeholders. From CEO\u0026rsquo;s to sales teams, many more people outside of the engineering team want, need, and deserve to know \u0026ldquo;How bad is it?","tags":["incidents","tracking"],"title":"Tracking Impact of a Problem","type":"post"},{"authors":null,"categories":null,"content":"To recover from incidents effectively, it\u0026rsquo;s important to communicate and collaborate effectively. In order to share relevant details of what is known about an incident, who is addressing it, and more, it\u0026rsquo;s important to have a method of tracking incident details.\nBasic details such as:\n When did know about the problem? How did we find out about the problem? Who is awareness of the problem? What is being done? How Bad is It?  Centralizing information about an incident allows others to stay on top of what is happening and maybe even offer additional support. It helps teams track the time of each phase, and it\u0026rsquo;s much easier to get a sense of how severe a problem is. What urgency, support, or resources are required? Teams can work much more effectively if they are looking at the same information.\nLet\u0026rsquo;s begin by Tracking When We Knew About a Problem\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"fbd35bac9ff9eb64c711b6a6984f022f","permalink":"/post/tracking-incident-details/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/tracking-incident-details/","section":"post","summary":"To recover from incidents effectively, it\u0026rsquo;s important to communicate and collaborate effectively. In order to share relevant details of what is known about an incident, who is addressing it, and more, it\u0026rsquo;s important to have a method of tracking incident details.\nBasic details such as:\n When did know about the problem? How did we find out about the problem? Who is awareness of the problem? What is being done? How Bad is It?","tags":["incidents","tracking"],"title":"Tracking Incident Details","type":"post"},{"authors":null,"categories":null,"content":"What (if anything) is being done?\nIs everyone assuming someone else is looking into it?\nThese types of questions emerge as additional people join to assist.\nWhen alerts are sent to distribution lists or general chat rooms it\u0026rsquo;s easy for them to be lost or at best delayed of action.\nAs first responders begin to assess the problem, conversations take place in persistent group chat tools such as Microsoft Teams or Slack.\nIt\u0026rsquo;s in these conversations where the most up to date and accurate information will exist about what is taking place during an active incident.\nResponders will share information as it becomes available including who and what is being done. By scrolling back through the conversation, others can quickly get an update not only on what is being done in the moment, but what has already taken place.\nThis saves engineers from duplicating effort, including pausing their effort to recover from the problem in order to update a ticket or send an email.\nWe\u0026rsquo;ll talk more about a technique of managing tasks from withing chat known as chatops.\nSeverity and impact of incidents will vary but it\u0026rsquo;s important teams are Tracking Impact of a Problem in order to spot trends as well as share details to stakeholders as services are restored.\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"43ccdfc50a9f62267088a9a1ef5b132e","permalink":"/post/tracking-what-is-being-done-about-a-problem/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/tracking-what-is-being-done-about-a-problem/","section":"post","summary":"What (if anything) is being done?\nIs everyone assuming someone else is looking into it?\nThese types of questions emerge as additional people join to assist.\nWhen alerts are sent to distribution lists or general chat rooms it\u0026rsquo;s easy for them to be lost or at best delayed of action.\nAs first responders begin to assess the problem, conversations take place in persistent group chat tools such as Microsoft Teams or Slack.","tags":["incidents","tracking"],"title":"Tracking What Is Being Done About a Problem","type":"post"},{"authors":null,"categories":null,"content":"Is this a new incident?\nIf we are trying to reduce the time it takes to recover from incidents, we will need to start capturing when we are aware of issues.\nBy examining incidents in phases, we can look for improvements in specific areas such as Detection. If we start capturing when we knew about a problem, patterns will emerge over time on what could be done to know sooner.\nKowing the Time to Detection (TTD) allows us to establish the beginning of our awareness. Next, we want to start Tracking How We Knew About a Problem\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"10e446277fce13f2e995279fca78a891","permalink":"/post/tracking-when-we-knew-about-a-problem/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/tracking-when-we-knew-about-a-problem/","section":"post","summary":"Is this a new incident?\nIf we are trying to reduce the time it takes to recover from incidents, we will need to start capturing when we are aware of issues.\nBy examining incidents in phases, we can look for improvements in specific areas such as Detection. If we start capturing when we knew about a problem, patterns will emerge over time on what could be done to know sooner.","tags":["incidents","tracking"],"title":"Tracking When We Knew About A Problem","type":"post"},{"authors":null,"categories":null,"content":"Am I the First to Know?\nWhen the primary responder acknowledges an incident, they are announcing their awareness of the alert.\nThey may not have additional information yet, but like an alarm clock, they have to press the stop button and take action. This not only stops any continued alarms but indicates to others that YES, someone is aware and looking in to the problem.\nOk. Who else is aware? Do the right people know there is a problem?\nAnd let\u0026rsquo;s say others are aware\u0026hellip; ?\n Tracking What Is Being Done About a Problem is the next thing we\u0026rsquo;ll talk about.\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"c6c1b44a7e27ea347220ad2afb573e74","permalink":"/post/tracking-who-knows-about-a-problem/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/tracking-who-knows-about-a-problem/","section":"post","summary":"Am I the First to Know?\nWhen the primary responder acknowledges an incident, they are announcing their awareness of the alert.\nThey may not have additional information yet, but like an alarm clock, they have to press the stop button and take action. This not only stops any continued alarms but indicates to others that YES, someone is aware and looking in to the problem.\nOk. Who else is aware? Do the right people know there is a problem?","tags":["incidents","tracking"],"title":"Tracking Who Knows About a Problem","type":"post"},{"authors":null,"categories":null,"content":"To create effective actionable alerts, you must understand their components and characteristics. Actionable alerts have:\n  Simplicity\n  Scope\n  Context\n  Simplicity is self-explanatory: make your alerts easy for you and others to understand, even if you’re reading them after being awakened at 2:00 a.m. Scope and context should be included in the content of the alert.\nLet’s look at some elements that an actionable alert should always include:\n  The source: information about where the alert is coming from. Many organizations have multiple monitoring systems in use at any one time and a large number of interconnected systems. It can save someone a tremendous amount of time if the alert says \u0026ldquo;This alert for payroll system thx-1138 is coming from Azure monitor subscription prod.\u0026rdquo;\n  The problem: information about what expectation has been violated. For example, \u0026ldquo;This server has been returning an error 30% of the time when it should have been returning errors less than 1% of the time.\u0026rdquo;\n  Impact and scope: information about the effect or impact the situation has had or potentially will have and the scope of that impact (ideally, stated from the customer’s point of view).\n  Recommended action: if possible, the alert should include what the person responding should do next, even if that is a pointer to a troubleshooting guide or some other documentation to find help in diagnosing and remediating this problem.\n  Including such helpful context will make operations practices around monitoring more sustainable and make responding to alerts easier.\n Defining Incidents\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"2ce0499f31cfed34c28bf2b9b53f33e6","permalink":"/post/understanding-actionable-alerts/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/understanding-actionable-alerts/","section":"post","summary":"To create effective actionable alerts, you must understand their components and characteristics. Actionable alerts have:\n  Simplicity\n  Scope\n  Context\n  Simplicity is self-explanatory: make your alerts easy for you and others to understand, even if you’re reading them after being awakened at 2:00 a.m. Scope and context should be included in the content of the alert.\nLet’s look at some elements that an actionable alert should always include:","tags":["on-call","alerts","foundations"],"title":"Understanding Actionable Alerts","type":"post"},{"authors":null,"categories":null,"content":"You must “learn to learn” from failure not in case your systems fail, but because it’s a certainty that your systems will fail.\nIn the IT world, the majority of systems we work with today – especially in a cloud environment – are complex. They’re composed of many interconnecting parts that have to work together, and the behavior of the overall system comes from the interaction of those parts, as much as from the individual parts themselves.\nReliability is the thread that runs throughout this learning path, but complex systems are never one hundred percent reliable. Such systems behave in interesting and counterintuitive ways. The complexity of these systems means there are inevitably minor flaws within them, and it is difficult or impossible to predict how minor flaws can join together to produce a significant incident.\nFor a more in-depth discussion of this topic, a good resource is the paper titled How Complex Systems Fail by Dr. Richard I. Cook with the Cognitive Technologies Laboratory at the University of Chicago. His “short treatise on the nature of failure” explains the causative factors that are common to the failures of complex systems of all types, in all fields.\n Link: How Complex Systems Fail\nSome of his key points are particularly relevant to the incident analysis and post-incident review process:\n  Complex systems contain changing mixtures of failure latent within them. It is impossible for your systems to run without multiple flaws being present. The failures change constantly because of changing technology, work organization, and efforts to eradicate failure. Your system is never functioning perfectly.\n  Complex systems run in degraded mode. Complex systems are always running as “broken” systems. They keep “working” in that state because they contain many redundancies, and people can keep them functioning despite the presence of many flaws. System operations are dynamic, with components continually failing and being replaced.\n  Catastrophe is always just around the corner. The complexity of these systems means major system failures are – in the long term – unavoidable. Complex systems always possess the potential for catastrophic failure, and it can happen at any time. It is impossible to eliminate this potential because it’s part of the inherent nature of the system.\n   Distinguishing Prevention and Preparation\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"081b9b8fdbb23e5d9c2df5a95c9d749b","permalink":"/post/understanding-how-complex-systems-fail/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/understanding-how-complex-systems-fail/","section":"post","summary":"You must “learn to learn” from failure not in case your systems fail, but because it’s a certainty that your systems will fail.\nIn the IT world, the majority of systems we work with today – especially in a cloud environment – are complex. They’re composed of many interconnecting parts that have to work together, and the behavior of the overall system comes from the interaction of those parts, as much as from the individual parts themselves.","tags":["complex","systems"],"title":"Understanding How Complex Systems Fail","type":"post"},{"authors":null,"categories":null,"content":" Site Reliability Engineering uses SLIs and SROs to measure the aspects of reliability that you learned about in Unit 2: availability, latency, throughput, coverage, correctness, fidelity, freshness, and durability, and whether you are meeting expectations in each applicable area.\nWhat to measure The first question to ask in relation to the aspect you want to measure is what to measure.\nExample #1: Measure availability\nHow would you determine the availability of a web server?\nYou can do this by measuring the number of HTTP calls the server received and the number to which it successfully responded. This ratio of successful calls to total calls gives you an understanding of the server’s reliability. Multiplying the ratio by 100 gives you a percentage.\n$$ \\ [\\frac {success}{total}] = ratio $$\nexample\n$$ \\ [\\frac {800}{1000}] = .8 $$\nFor example, if the ratio is 0.8 and you multiply by 100, you can conclude that the web server has been available only 80% of the time.\nor\n$$ \\ .8 * 100 = 80 percent $$\nExample #2: Measure latency\nTo get an idea of the latency of the web service, you can measure the number of operations that were completed in fewer than 10 milliseconds against the number of total operations.\nIf, for example, dividing the first number by the second gives you a ratio of 0.8, and you multiply this by 100, your service has an 80% success rate by this measurement.\nWhere to measure To have a clear picture of your reliability based on SLIs, you need to know not only what was measured, but also where the measurement was taken.\nFor example, in measuring the availability of the web server above, you need to specify whether the number of calls was measured at the load balancer or at the server. Likewise, when measuring the latency of the web service, you should note that the number of operations was measured at the client.\nWhy is this so crucial? If the purpose is to create a feedback loop in your organization, in which you are able to have conversations about reliability using objective data, it is important for the people having these conversations to be using the same data.\nTo use a previous example, if you are trying to determine whether a web service is meeting expectations and one person looks at data collected at the server itself while another looks at data collected at the load balancer in front of that web server, you may be looking at radically different sets of numbers. The information collected from the web server itself only reflects the traffic that actually reached that server. If there was an issue with the load balancer or the network and half the packets never reached the web server, the two people will have a very different picture of the situation, especially when it comes to total number of requests.\nThis leads to the logical question: where is the best place to measure SLIs?\nUnfortunately, there is not a universally “correct” answer. It’s a decision you must make with an understanding that there are tradeoffs either way.\nThe key thing to keep in mind is based on our “prime directive” of reliability measurement: Reliability should be measured from the customer’s perspective. Thus, most of the time, you should measure at the point that most accurately reflects the customer’s experience.\n Building Service Level Indicators (SLIs) with Log Analytics\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"da08fc7eaab27888daf4675226a14388","permalink":"/post/understanding-service-level-indicators/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/understanding-service-level-indicators/","section":"post","summary":"Site Reliability Engineering uses SLIs and SROs to measure the aspects of reliability that you learned about in Unit 2: availability, latency, throughput, coverage, correctness, fidelity, freshness, and durability, and whether you are meeting expectations in each applicable area.\nWhat to measure The first question to ask in relation to the aspect you want to measure is what to measure.\nExample #1: Measure availability\nHow would you determine the availability of a web server?","tags":["measurements","reliability","sli","service"],"title":"Understanding Service Level Indicators","type":"post"},{"authors":null,"categories":null,"content":"Now you know how to measure reliability using SLIs, but the ratios and percentages that you’ve calculated only get you halfway toward fulfilling the goal of site reliability engineering. You can now say the web server in our example is 50% reliable, but is that the appropriate level of reliability as discussed in our definition of SRE?\nIt\u0026rsquo;s also useful to know the period of time to which that reliability level applied. For how long was that 50% success rate maintained?\nTo answer these questions, you need to look at Service Level Objectives. SLOs are statements of the objective you have from a reliability standpoint, based on customer expectations. The basic recipe for creating an SLO consists of these ingredients:\n  The “thing” you’re going to measure – number of requests, storage checks, operations; what you’re measuring.\n  The desired proportion – for example, “successful 50% of the time,” “can read 99.9% of the time,” “return in 10ms 90% of the time.”\n  The time – what timeframe are you measuring: the last 10 minutes, during the last quarter, in the previous 30 days, etc.?\nPutting these components together and including the important “where” information, a sample SLO might look like this:\n“90% of HTTP requests as reported by the load balancer succeeded in the last 30 day window.”\n   Building Service Level Objectives with Log Analytics\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"67c6a24e4e821a585f503f751f3eb91b","permalink":"/post/understanding-service-level-objectives/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/understanding-service-level-objectives/","section":"post","summary":"Now you know how to measure reliability using SLIs, but the ratios and percentages that you’ve calculated only get you halfway toward fulfilling the goal of site reliability engineering. You can now say the web server in our example is 50% reliable, but is that the appropriate level of reliability as discussed in our definition of SRE?\nIt\u0026rsquo;s also useful to know the period of time to which that reliability level applied.","tags":["measurements","reliability","slo","service"],"title":"Understanding Service Level Objectives","type":"post"},{"authors":null,"categories":null,"content":"The foundations of building reliable systems including a good incident response plan, have to start with determining “Who is expected to respond to problems?” and “How do let them know?”.\nThe best place to start, is to design what is to establish roles, rosters, and rotations.\nRoles : Well defined responsibilities and expectations of individuals on an on-call team (or roster). The Primary Responder, for example.\nRosters : A group of individuals, each with their own assigned role and understood responsibilities and expectations. The mobile \u0026ldquo;on-call\u0026rdquo; team, consisting of multiple members each with thier own assigned role.\nRotations : A scheduled shift for individuals where they are \u0026ldquo;on-call\u0026rdquo; for a defined period of time. A 24 x n rotation where someone is the** Primary Responder**, for example.\nIt\u0026rsquo;s worth looking at each of these groups a little closer, so let\u0026rsquo;s do that now.\nNext steps  Establishing On-call Roles\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"caa6f8e6b4eece6b16ff372febd0d9b6","permalink":"/post/understanding-the-foundations-of-incident-response/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/understanding-the-foundations-of-incident-response/","section":"post","summary":"The foundations of building reliable systems including a good incident response plan, have to start with determining “Who is expected to respond to problems?” and “How do let them know?”.\nThe best place to start, is to design what is to establish roles, rosters, and rotations.\nRoles : Well defined responsibilities and expectations of individuals on an on-call team (or roster). The Primary Responder, for example.\nRosters : A group of individuals, each with their own assigned role and understood responsibilities and expectations.","tags":["incidents","response","foundations"],"title":"Understanding the Foundations of Incident Response","type":"post"},{"authors":null,"categories":null,"content":"If we start to think of incidents as a normal part of a system, then we can also build some formality around the patterns and practices we inevitably see when people instinctually do what they do when something goes wrong.\nFrom the beginning of a problem to analyzing what and how things happened, we can measure them independantly of each other. By doing so, we can look for improvements in each phase.\nFor example, monitoring systems may be working as expected but because an alert was sent to a email distribution group, once people were aware of the alert, most assumed someone else was investigating the problem. The problem persisted for hours.\nAn incident can be divided into 5 phases. Detection, response, remediation, analysis, and readiness.\nPhase 1 - Detection: A problem has been detected through various tooling and practices\nPhase 2 - Response: A coordinated effort to get the right people and tooling in place to diagnose, theorize, and triage.\nPhase 3 - Remediation: Efforts made to change the system to either restore service or confirm theories.\nPhase 4 - Analysis: Post-incident retrospective exercise to understand the the full the lifecycle of the incident including the human response.\nPhase 5 - Readiness: Implementing knew knowledge and changes to improve and shorten the time and effects of future incidents.\nLet\u0026rsquo;s touch on the detection phase just a little more in depth. This is often the best place to start improving your incident response practices. Solid monitoring is the foundation of building reliable systems.\nNext steps  Identifying the Detection Phase of an Incident\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"9472bf5f380f1304e816c4a7d5bf65a9","permalink":"/post/understanding-the-full-lifecycle-of-an-incident/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/understanding-the-full-lifecycle-of-an-incident/","section":"post","summary":"If we start to think of incidents as a normal part of a system, then we can also build some formality around the patterns and practices we inevitably see when people instinctually do what they do when something goes wrong.\nFrom the beginning of a problem to analyzing what and how things happened, we can measure them independantly of each other. By doing so, we can look for improvements in each phase.","tags":["incident","lifecycle","foundations"],"title":"Understanding the Full Lifecycle of an Incident","type":"post"},{"authors":null,"categories":null,"content":"Now that we have a better idea on the different things that we could be measuring, let\u0026rsquo;s talk about what to do with the data.\nOperational practices to create reliability feedback loops known as service level indicators and objectives help transform the reliability discussion.\n Service Level Indicators are the measurements that you use to determine whether you have reached those goals; in other words, the indicators that your service is behaving reliably. Service Level Objectives are simply the goals that you’ve set, based on the appropriate level of reliability that you want to reach.  You can use SLIs and SLOs to set appropriate goals based on the different aspects of reliability and to determine whether you are meeting those goals, using simple calculations and a basic recipe.\nNow you’ll find out how to actually measure the success of your reliability efforts using Service Level Indicators (SLIs) and Service Level Objectives (SLOs).\n Understanding Service Level Indicators (SLI)\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"0ae00cb7513bdfcaddc1491303c48682","permalink":"/post/understanding-measurements-of-reliability/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/understanding-measurements-of-reliability/","section":"post","summary":"Now that we have a better idea on the different things that we could be measuring, let\u0026rsquo;s talk about what to do with the data.\nOperational practices to create reliability feedback loops known as service level indicators and objectives help transform the reliability discussion.\n Service Level Indicators are the measurements that you use to determine whether you have reached those goals; in other words, the indicators that your service is behaving reliably.","tags":["measurements","reliability"],"title":"Understanding the Measurements of Reliability","type":"post"},{"authors":null,"categories":null,"content":"Incident response doesn’t stop when the incident is over. They say those who don’t study history are doomed to repeat it. Likewise, those who don’t study, analyze, and learn from the incidents they resolved are doomed to keep repeating the process, as well.\nYour most important means of learning from incidents is the post-incident review.\nWhen an incident occurs, your first reaction probably isn’t, “Hurray – a learning opportunity!” Your immediate priority is figuring out what went wrong and fixing it as quickly as possible, to reduce the impact on your customers and end users – as it should be.\nHowever, once the incident has been resolved, it’s important to follow up and benefit from the discovery of whatever mistakes or circumstances led to the failure. Doing so will help you prevent the same thing from happening again and will also help you understand what tactics do and don’t work best when responding to any kind of incident in the future.\nThe post-incident review is part of the analysis phase of the incident response lifecycle. Not all post-incident reviews are created equal. There are different ways to approach the process, and too much focus on certain aspects of the problem or framing questions in the wrong way can reduce the value of the review.\nWe want to now explore the best path towards generating actionable introspection about the systems we co-create.\n Understanding How Complex Systems Fail\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"864c1787aba1952e8cf6bbe6ff970a2d","permalink":"/post/understanding-why-we-learn-from-incidents/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/understanding-why-we-learn-from-incidents/","section":"post","summary":"Incident response doesn’t stop when the incident is over. They say those who don’t study history are doomed to repeat it. Likewise, those who don’t study, analyze, and learn from the incidents they resolved are doomed to keep repeating the process, as well.\nYour most important means of learning from incidents is the post-incident review.\nWhen an incident occurs, your first reaction probably isn’t, “Hurray – a learning opportunity!” Your immediate priority is figuring out what went wrong and fixing it as quickly as possible, to reduce the impact on your customers and end users – as it should be.","tags":["analysis","communication","collaboration","learning","incident"],"title":"Understanding Why We Learn From Incidents","type":"post"},{"authors":null,"categories":null,"content":"It\u0026rsquo;s important that internal teams are aware of what\u0026rsquo;s happening when an incident occurs. If we don’t provide consistent updates, stakeholders will start coming around and asking.\nIt\u0026rsquo;s also important to acknowledge awareness, status, and expectations to stakeholders beyond the internal groups of an organization. Customers, users, fans, and anyone relying on the availability of your service will want to know what\u0026rsquo;s going on when an incident begins to impact them.\nThey have every right to this information, but we\u0026rsquo;ve got to find a better way to make them aware of an issue and what is being done about it.\nAcknowledgement Just letting others know that someone is aware of the situation and \u0026ldquo;on it\u0026rdquo; can go a long way in easing the anxiety of others who may be impacted by an incident. Hearing from the provider themselves that an issue has been detected and is being investigated is almost always enough to satisfy customers and users when they first discover something isn\u0026rsquo;t working as expected.\nWhen customers are aware of an issue and it\u0026rsquo;s not clear if the provider knows about it or is doing anything it can sour the relationship.\nSimply acknowledging an incident provides context to everyone else that action is taking place.\nOnce we have acknowledge awareness of an incident we need to begin providing additional information as it becomes available. In early moments and minutes of an incident it may not be clear what the problem (or problems) is.\nEven if there isn\u0026rsquo;t any information to share yet, that in and of itself is the message that should be communicated. It\u0026rsquo;s important to be clear in what is known and not known.\nClear We should be clear in presenting what we know, what is being done and what kind of expectations should they have in terms of when they\u0026rsquo;re going to hear back from us?\nExpectations  ChatOps\n","date":1583366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583366400,"objectID":"c7dfab5f3499b11ece506df1cab85644","permalink":"/post/updating-stakeholders/","publishdate":"2020-03-05T00:00:00Z","relpermalink":"/post/updating-stakeholders/","section":"post","summary":"It\u0026rsquo;s important that internal teams are aware of what\u0026rsquo;s happening when an incident occurs. If we don’t provide consistent updates, stakeholders will start coming around and asking.\nIt\u0026rsquo;s also important to acknowledge awareness, status, and expectations to stakeholders beyond the internal groups of an organization. Customers, users, fans, and anyone relying on the availability of your service will want to know what\u0026rsquo;s going on when an incident begins to impact them.","tags":["chatops","communication","collaboration"],"title":"Updating Stakeholders","type":"post"}]
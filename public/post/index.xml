<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | On Call Life</title>
    <link>/post/</link>
      <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 02 Apr 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Posts</title>
      <link>/post/</link>
    </image>
    
    <item>
      <title>Building Troubleshooting Guides in Azure Monitor</title>
      <link>/post/building-troubleshooting-guides-in-azure-monitor/</link>
      <pubDate>Thu, 02 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/building-troubleshooting-guides-in-azure-monitor/</guid>
      <description>&lt;p&gt;Also referred to as 
&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/azure-monitor/app/usage-workbooks/?WT.mc_id=oncalllife-blog-jahand&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Workbooks in Azure Monitor&lt;/a&gt;, interactive reports can be quickly built to provide critical information to first responders as they begin to investigate and take early remediation efforts.&lt;/p&gt;
&lt;p&gt;Guides can include data from sources such as logs, metrics, and more through visualizations like charts, grids, graphs, and text.&lt;/p&gt;
&lt;p&gt;Everything displayed in the troubleshooting guide can be customized on the fly through parameters making it interactive.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/microsoft/Application-Insights-Workbooks/raw/master/Documentation/Images/WorkbookExample.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The name &lt;em&gt;Troubleshooting Guide&lt;/em&gt; is specific to the workbooks made available through Azure&amp;rsquo;s Application Insights. To view a gallery of templates or to create a new guide, select &lt;strong&gt;Troubleshooting Guide&lt;/strong&gt; from the Investigate section of the left navigation pane.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;troubleshooting-guide.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Troubleshooting guides are ultimately just JSON files, which means they can be handled just like code. You can put them in a repository right next to your code, infrastructure, deployment pipelines, and more.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;troubleshooting-guide-json.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;To view an example of a Troubleshooting Guide represented in JSON, 
&lt;a href=&#34;https://github.com/microsoft/ignite-learning-paths-training-ops/blob/master/ops20/demos/02/TroubleshootingGuideGalleryTemplate.json&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;take a look at this sample&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Troubleshooting Guides can often provide the jumpstart first responders need to begin diagnosing. By freeing up their cognitive tasks for something other than remembering where resources are, engineers can make a positive difference faster.&lt;/p&gt;
&lt;p&gt;Supplying context and guidance rather than rigid step-by-step processes allows engineers to be creative in novel situations. If an engineer can follow each step of a check list to recover from the problem then it is time to automate that &amp;ldquo;known&amp;rdquo; fix and free up incident response for scenarios that couldn&amp;rsquo;t have been predicted or fixed through a linear process.&lt;/p&gt;
&lt;p&gt;Keeping people informed is a big driving force behind efforts during incident response. Troubleshooting guides are no exception. When engineers (also stakeholders in the incident) are informed with objective data, they can take appropriate action based on information they have available to them.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/updating-stakeholders/&#34;&gt;Updating Stakeholders&lt;/a&gt; can be done in many ways. Let&amp;rsquo;s take a look at a few examples.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Creating Custom Incident Reports and Charts in Azure Boards</title>
      <link>/post/creating-custom-incident-reports-and-charts-in-azure-boards/</link>
      <pubDate>Tue, 24 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/creating-custom-incident-reports-and-charts-in-azure-boards/</guid>
      <description>&lt;p&gt;For the most part, incidents are unique.&lt;/p&gt;
&lt;p&gt;Therefore the lessons learned will vary from problem to problem. However, it&amp;rsquo;s helpful to spot trends in response efforts to both identify what is working and what needs improvement.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s also helpful for engineering teams to have a sense of how frequent problems are arising and how quickly they are addressed and resolved.&lt;/p&gt;
&lt;p&gt;When tracking incidents using Azure Boards, it&amp;rsquo;s quite simple to build reports and charts provide a high level snapshot of incident management efforts.&lt;/p&gt;
&lt;h2 id=&#34;create-a-query&#34;&gt;Create a Query&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; Select the &lt;strong&gt;Queries&lt;/strong&gt; option in the left navigation
&lt;img src=&#34;queries.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt; Customize the query&lt;/p&gt;
&lt;p&gt;Select the  clauses, fields, and values of interest to easily build queries and report back information such as &lt;em&gt;&amp;ldquo;Show me &lt;strong&gt;ANY&lt;/strong&gt; incident&amp;rdquo;&lt;/em&gt; in this example.
&lt;img src=&#34;new-query.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;You&amp;rsquo;ll see the results at the bottom of the screen. In our case we only have two incidents. One is &lt;strong&gt;resolved&lt;/strong&gt; and one is &lt;strong&gt;acknowledged&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3.&lt;/strong&gt; Save the query&lt;/p&gt;
&lt;p&gt;Press the &lt;strong&gt;Save query&lt;/strong&gt; menu option and give it a name such as &lt;strong&gt;&amp;ldquo;All Incidents&amp;rdquo;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;save-query.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;create-a-graph&#34;&gt;Create a Graph&lt;/h2&gt;
&lt;p&gt;To create visualizations of the queries, begin by changing to the &lt;strong&gt;Charts&lt;/strong&gt; option.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; Click on &lt;strong&gt;Charts&lt;/strong&gt;
&lt;img src=&#34;charts.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt; Click on &lt;strong&gt;New Charts&lt;/strong&gt;
&lt;img src=&#34;new-chart.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3.&lt;/strong&gt; Configure the chart&lt;/p&gt;
&lt;p&gt;Choose the type of chart you want to use (Pie in this example). Give the chart a name, such as &amp;ldquo;All Incidents - Chart&amp;rdquo;. Choose how you want the data grouped, like &amp;ldquo;State&amp;rdquo; (&lt;strong&gt;New Incident, Resolved, Acknowledged, etc.&lt;/strong&gt;)&lt;/p&gt;
&lt;p&gt;Choose a color to represent the data and press Ok.
&lt;img src=&#34;configure-chart.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now you have a high level visual to communicate the state of incidents.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;chart-view.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now that we&amp;rsquo;ve discussed ways to help track the response of an incident, let&amp;rsquo;s take a moment to discuss ways of 
&lt;a href=&#34;/post/improving-the-remediation-of-incidents/&#34;&gt;Improving the Remediation of Incidents&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Building Service Level Indicators With Log Analytics</title>
      <link>/post/building-service-level-indicators-with-log-analytics/</link>
      <pubDate>Mon, 16 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/building-service-level-indicators-with-log-analytics/</guid>
      <description>&lt;p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/Ds_7EKMS3N8&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/understanding-service-level-objectives/&#34;&gt;Understanding Service Level Objectives (SLOs)&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Building Service Level Objectives With Log Analytics</title>
      <link>/post/building-service-level-objectives-with-log-analytics/</link>
      <pubDate>Mon, 16 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/building-service-level-objectives-with-log-analytics/</guid>
      <description>&lt;p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/ztZ7RSdwatc&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/sending-actionable-alerts/&#34;&gt;Sending Actionable Alerts&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Creating an On-call Roster Using Azure Table Storage</title>
      <link>/post/creating-an-oncall-roster-using-azure-table-storage/</link>
      <pubDate>Fri, 13 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/creating-an-oncall-roster-using-azure-table-storage/</guid>
      <description>&lt;p&gt;On-call rosters allow teams to identify who is responsible for acknowledging and addressing incidents as they occur.&lt;/p&gt;
&lt;p&gt;They are made up of the names and contact information of everyone expected to take part in the response and remediation of service disruptions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;On-call Roster&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Email&lt;/th&gt;
&lt;th&gt;Service&lt;/th&gt;
&lt;th&gt;On-call&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Jason Hand&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;mailto:jason@xyz.com&#34;&gt;jason@xyz.com&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;API&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Chris Smith&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;mailto:chris@xyz.com&#34;&gt;chris@xyz.com&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;API&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Lauren Jones&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;mailto:lauren@xyz.com&#34;&gt;lauren@xyz.com&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Mobile&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ryan Boggs&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;mailto:ryan@xyz.com&#34;&gt;ryan@xyz.com&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Database&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Depending on the make up of your teams and services, on-call rosters can remain quite simple or become extremely complex.&lt;/p&gt;
&lt;p&gt;One way of creating an on-call roster is with a basic storage table in Azure.&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    First, login or create a &lt;a href=&#34;https://azure.microsoft.com/?wt.mc_id=oncalllife-blog-jahand&#34;&gt;free Azure account&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; Create a new resource
From the home screen in Azure, select the option to &lt;strong&gt;create a resource&lt;/strong&gt;.
&lt;img src=&#34;create-a-resource.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt; Search for and select &amp;ldquo;&lt;strong&gt;Storage Account&lt;/strong&gt;&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;create-storage-account.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3.&lt;/strong&gt; Select a &lt;strong&gt;subscription&lt;/strong&gt;, &lt;strong&gt;resource group&lt;/strong&gt;, etc.&lt;/p&gt;
&lt;p&gt;Choose where you want the storage account created, what name you want to give it, as well as the type of disk and access tiers and then choose &lt;strong&gt;Review &amp;amp; Create&lt;/strong&gt;, followed by &lt;strong&gt;Create&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4.&lt;/strong&gt; Go to the (&lt;strong&gt;Storage Account&lt;/strong&gt;) resource&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5.&lt;/strong&gt; Click on the &lt;strong&gt;Tables&lt;/strong&gt; card
&lt;img src=&#34;click-on-tables-card.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;6.&lt;/strong&gt; Click the &lt;strong&gt;+ Table&lt;/strong&gt; option and give it a name of &lt;code&gt;oncall&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;create-on-call-table.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;7.&lt;/strong&gt; Click on &lt;strong&gt;Storage Exploror&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;8.&lt;/strong&gt; Click on &lt;strong&gt;Tables&lt;/strong&gt;, then &lt;strong&gt;oncall&lt;/strong&gt;, then &lt;strong&gt;+ Add&lt;/strong&gt;
&lt;img src=&#34;open-oncall-table.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;9.&lt;/strong&gt; Add &lt;strong&gt;entity&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;PartitionKey&lt;/strong&gt; field will remain the same. Add new properties for &lt;strong&gt;name&lt;/strong&gt;, &lt;strong&gt;email&lt;/strong&gt;, &lt;strong&gt;service&lt;/strong&gt;, and &lt;strong&gt;oncall&lt;/strong&gt;. All properties should be set to a &lt;em&gt;string&lt;/em&gt; type, except for &lt;strong&gt;oncall&lt;/strong&gt;. It is a boolean. Once all fields have been added, press &lt;strong&gt;Insert&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;add-table-entity.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;10.&lt;/strong&gt; Repeat the process for new entities.
Make sure to use the same PartitionKey but a unique rowkey. Also be sure to use true or false for the oncall field.&lt;/p&gt;
&lt;p&gt;After a couple of entries, it should look as follows.
&lt;img src=&#34;on-call-table-entries.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s it. You&amp;rsquo;ve taken your first steps towards building a basic on-call roster. This will help us identify who to initially alert when an incident occurs.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    This example roster tracks only the &amp;ldquo;&lt;strong&gt;Primary Responder&lt;/strong&gt;&amp;rdquo; (i.e. &lt;code&gt;oncall=true or false&lt;/code&gt;). It doesn&amp;rsquo;t include any alternative contact information. Nor does it identify what rotations someone is associated with.
&lt;br /&gt;&lt;br /&gt;
Try expanding your roster to contain more of the roles previously discussed.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Next, we will take a look at 
&lt;a href=&#34;/post/establishing-oncall-rotations/&#34;&gt;Establishing On-call Rotations&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Customizing Azure Boards for Incident Tracking</title>
      <link>/post/customizing-azure-boards-for-incident-tracking/</link>
      <pubDate>Fri, 13 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/customizing-azure-boards-for-incident-tracking/</guid>
      <description>&lt;p&gt;Now that we have 
&lt;a href=&#34;/post/creating-an-incident-tracking-tool-with-azure-boards/&#34;&gt;a tool to track the incident details&lt;/a&gt;, we need to ensure we are tracking all of the important aspects. Such as &lt;em&gt;When did we know about the problem&lt;/em&gt; and more.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now take a look at how we can customize Azure Boards to track additional incident details.&lt;/p&gt;
&lt;h2 id=&#34;when-did-we-know&#34;&gt;When did we know?&lt;/h2&gt;
&lt;p&gt;When a new record (or incident) is created in Azure Boards we will automatically have the date and time as well as a change log throughout the incident&amp;rsquo;s lifecycle&amp;hellip;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;when-did-we-know.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;However, it would be good to have a separate field where we can also capture when our systems were detecting the issue. It is possible that while no alerts were triggered for humans to respond, the data was there if only we were monitoring and alerting on it.&lt;/p&gt;
&lt;h2 id=&#34;how-did-we-know&#34;&gt;How did we know?&lt;/h2&gt;
&lt;p&gt;The default fields do not provide anything suitable to capture this information. We will need to create a new field to start tracking it.&lt;/p&gt;
&lt;h2 id=&#34;who-is-aware&#34;&gt;Who is aware?&lt;/h2&gt;
&lt;p&gt;Using the &lt;strong&gt;Assigned To&lt;/strong&gt; field we can attempt to communicate some awareness. If the first responder updates the State (to &lt;strong&gt;Acknowledged&lt;/strong&gt;), and sets the &lt;strong&gt;Assigned To&lt;/strong&gt; field to themself, it acknowledges to others that someone has been alerted and is investigating.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;who-is-aware.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;what-is-the-impact&#34;&gt;What is the impact?&lt;/h2&gt;
&lt;p&gt;When possible, it&amp;rsquo;s best to include context such as &amp;ldquo;customer impacting&amp;rdquo; in the alert itself. This allows immediate contextual awareness and a sense of urgency. We can create a new field to track the official severity level.&lt;/p&gt;
&lt;h2 id=&#34;what-is-being-done&#34;&gt;What is being done?&lt;/h2&gt;
&lt;p&gt;Naturally, anyone looking at the details of the incident are going to want to know what is being done. The &lt;strong&gt;description&lt;/strong&gt; and &lt;strong&gt;discussion&lt;/strong&gt; fields provide great places for responders or others assisting in the efforts to update the larger audience on what is being done and what expectations to have regarding future updates.&lt;/p&gt;
&lt;p&gt;However, real-time updates on what is taking place can be found within the chat channel where engineers are collaborating during the response.&lt;/p&gt;
&lt;p&gt;It would be useful to provide a link or information for others to follow if they would like to observe the conversation as it unfolds. We will need to create a field specifically related to the conversation.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now customize Azure Boards to satisfy our basic needs of tracking an incident.&lt;/p&gt;
&lt;h2 id=&#34;creating-new-fields&#34;&gt;Creating New Fields&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; Click the &amp;ldquo;&lt;strong&gt;Customize&lt;/strong&gt;&amp;rdquo; option under the &lt;strong&gt;(&amp;hellip;)&lt;/strong&gt; in the upper right.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;customize-fields.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Before we create new fields, let&amp;rsquo;s first make changes to the &lt;strong&gt;&amp;ldquo;State&amp;rdquo;&lt;/strong&gt; of incidents to better align with industry terminology.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt; Click on &lt;strong&gt;+ New State&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Delete the existing ones and create new states that better align with terminology used. Such as &lt;strong&gt;Acknowledged&lt;/strong&gt; (ACK), &lt;strong&gt;Escalated&lt;/strong&gt;, etc.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;new-incident-states.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3.&lt;/strong&gt; Click on &lt;strong&gt;Layout&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Then add new fields using the button below&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;new-fields.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4.&lt;/strong&gt; Confirm changes&lt;/p&gt;
&lt;p&gt;When you return to the incident, you&amp;rsquo;ll notice a warning message pointing out that the current state of &amp;ldquo;To do&amp;rdquo; does not match the existing options.&lt;/p&gt;
&lt;p&gt;You&amp;rsquo;ll also see the new fields that we have added (Time to detection, source of alert, etc.)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;updated-incident.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This should be a good place to start capturing information that will be helpful and important to analyze in a post-incident review later.&lt;/p&gt;
&lt;p&gt;With the information tracked in Azure Boards along with the conversations taking place in Microsoft Teams we will have a lot of great data to analyze.&lt;/p&gt;
&lt;p&gt;From that analysis we will measure and establish baselines and expectations around incident response.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s important to discuss 
&lt;a href=&#34;/post/measuring-incident-response/&#34;&gt;measuring incident response&lt;/a&gt; now. However, on our path to continuous improvement there are certain traps we want to avoid when examining incidents, especially in aggregate.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Answers To Your Questions</title>
      <link>/post/answers-to-questions/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/answers-to-questions/</guid>
      <description>&lt;h2 id=&#34;what-is-this&#34;&gt;What Is This?&lt;/h2&gt;
&lt;p&gt;The systems we work in eventually have problems.&lt;/p&gt;
&lt;p&gt;They are built, maintained, and supported by technolgists such as yourself. And when an issue inevitably occurs, someone needs to take action to restore services.&lt;/p&gt;
&lt;p&gt;Responding to those problems helps maintain functionality and operational abilities of an organization&amp;rsquo;s IT services, serving both internal and external users.&lt;/p&gt;
&lt;p&gt;Many organizations don&amp;rsquo;t currently have an incident response plan in place. In fact, efforts to recover from service disruptions rarely follow any kind of repeatable and measured framework at all. Engineers react rather than respond.&lt;/p&gt;
&lt;p&gt;With the increased reliance on digital services and their underlying technology it&amp;rsquo;s more important than ever to establish an explicit response plan. There are small steps that you could take immediately so that when the next problem occurs, everyone knows what to do. The incident itself can be viewed not just as an outage but an opportunity to learn.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;On-call Life&lt;/strong&gt; is dedicated to providing foundational concepts and information related to being on-call including monitoring, incident response, and the post-incident review process.&lt;/p&gt;
&lt;p&gt;This is a live site with new information added regularly. Much of the content is syndicated from presentations created for and delivered during Microsoft&amp;rsquo;s Ignite the Tour.&lt;/p&gt;
&lt;p&gt;Throughout these articles, demonstrations and resources specific to Azure will be used, but the foundations of monitoring, incident response, and retrospectives are agnostic to tooling. Demonstrations on Azure is done to illustrate rather than to suggest &lt;em&gt;&amp;ldquo;best practice&amp;rdquo;&lt;/em&gt; implementations.&lt;/p&gt;
&lt;h2 id=&#34;how-do-i-use-this&#34;&gt;How Do I Use This?&lt;/h2&gt;
&lt;p&gt;Begin, by examining why the responsibilities of on-call have become so critical to nearly every business, group, and government.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/monitoring-for-reliability/&#34;&gt;Monitoring For Reliability&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;who-is-behind-this&#34;&gt;Who Is Behind This?&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://twitter.com/jasonhand&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jason Hand - @jasonhand&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Breaking Down the Components of a Post-incident Review</title>
      <link>/post/breaking-down-the-components-of-a-post-incident-review/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/breaking-down-the-components-of-a-post-incident-review/</guid>
      <description>&lt;p&gt;You now know what a post-incident review is, its role in the incident response
process, and when it should be conducted. In this unit, you’ll dive a little
deeper into the details of what makes a post-incident review most effective.&lt;/p&gt;
&lt;p&gt;Because incidents differ, the exact makeup of post-incident reviews can be
different, too. But there are some common characteristics and components of a
good review that can provide you with a solid foundation for carrying out the
process.&lt;/p&gt;
&lt;h2 id=&#34;what-its-not&#34;&gt;What it’s not&lt;/h2&gt;
&lt;p&gt;Before you can understand the characteristics that make for a good post-incident
review, you should consider what it’s &lt;em&gt;not.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;It’s not a document or report.&lt;/strong&gt; It’s easy to think of a “review” as a
written summary, and indeed, a summary report often follows a post-incident
review. However, these are two different and distinct parts of the Analysis
phase of the incident response lifecycle.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;It’s not a determination of causality.&lt;/strong&gt; Your review will look at the
factors that contributed to the failure, but the purpose isn’t to pinpoint a
culprit. It’s to think about and share information about all aspects of the
incident in order to learn and improve.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;It’s not a list of action items.&lt;/strong&gt; You may end up with such a list as a
result of what you learn in the review, but this isn’t the focus. If you
don’t come away with a stepwise punch list but you do know more about your
systems than before, the review was successful.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The incident review is, more than anything, a &lt;em&gt;conversation.&lt;/em&gt; It’s a defined
space within which your team can review what they knew at the time and what they
know now, and explore and better understand how the parts of the system –
including the human parts – do or don’t work together in response to problems.&lt;/p&gt;
&lt;h2 id=&#34;characteristics-and-components&#34;&gt;Characteristics and components&lt;/h2&gt;
&lt;p&gt;A good incident review is, first and foremost, &lt;em&gt;blameless.&lt;/em&gt; Although you need to
examine how the human parts of the system interacted with it, you don’t do this
in order to label anyone “at fault.” The focus should be on the failures of the
technology and the process, not of the people.&lt;/p&gt;
&lt;p&gt;Frame your questions to reflect this:&lt;/p&gt;
&lt;p&gt;“What was the deficit in your monitoring that failed to give the person at the
keyboard the necessary context to make the right decision?”&lt;/p&gt;
&lt;p&gt;“Why was there a &lt;strong&gt;delete the entire database&lt;/strong&gt; option in the tool that didn’t ask
for confirmation?”&lt;/p&gt;
&lt;p&gt;When things go wrong, it can be tempting to point fingers. However, you must
remember this key point: &lt;em&gt;You can’t fire your way to reliability.&lt;/em&gt; Shaming and
blaming and an investigation that’s aimed at finding and firing the person who
is “responsible” won’t lead to more reliable systems. Instead, it will lead to
an inexperienced operations team and personnel who are afraid to act.&lt;/p&gt;
&lt;p&gt;Approach the review as a search for knowledge and context, not a hunt for who
did what and a reaction to that.&lt;/p&gt;
&lt;p&gt;Although the review is about the failures of the technology, it’s not a
technical process as much as it is a people process. Talk – and more important,
listen – to the people who were involved in the incident. Keep an open mind.
Different people have different perspectives and not everyone will agree, and
that mix of perspectives is invaluable to the learning process.&lt;/p&gt;
&lt;p&gt;A post-incident review is an honest inquiry. As such, it embraces these key
components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Discussion&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Discourse&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dissent&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Discovery&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These “4 Ds” create a framework on which you can build a post-incident learning
review that can result in more reliable systems and more productive teams that
work together.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/running-a-post-incident-review/&#34;&gt;Running a Post-incident Review&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Building a Serverless Status Page Solution</title>
      <link>/post/building-a-serverless-status-page-solution/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/building-a-serverless-status-page-solution/</guid>
      <description>&lt;p&gt;
&lt;a href=&#34;https://jhandcdn.blob.core.windows.net/blob/UpdateFromTeams.gif&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://jhandcdn.blob.core.windows.net/blob/UpdateFromTeams.gif&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;tech-used&#34;&gt;Tech Used&lt;/h3&gt;
&lt;p&gt;The brains behind this solution is an Azure Function (running Node.js) that is triggered via outgoing webhook (from Microsoft Teams). The function modifies an index.html file stored in a &amp;ldquo;web server&amp;rdquo; served from a serverless SMB file share in Azure Storage.&lt;/p&gt;
&lt;p&gt;Users can &lt;code&gt;open&lt;/code&gt;, &lt;code&gt;update&lt;/code&gt;, and &lt;code&gt;close&lt;/code&gt; &amp;ldquo;status updates&amp;rdquo; by invoking them from within a chat channel.&lt;/p&gt;
&lt;p&gt;The text that follows the command will be stored and displayed on the site below the colored (Red or Green) header.&lt;/p&gt;
&lt;p&gt;In addition to the website files, an Azure Table will be used for storing the history of each status update.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; The table is not provisioned with the deployment script. It needs to be manually created in Step 2 below.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;SignalR manages refreshing the client so that changes made to the HTML are immediately visible without any end user interaction.&lt;/p&gt;
&lt;p&gt;Application Insights is used to provide observability on the operation, behavior, and usage of the solution and is &amp;ldquo;best practice&amp;rdquo; for building highly available and reliable system&amp;hellip; which we expect from any Status Page solution.&lt;/p&gt;
&lt;h3 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h3&gt;
&lt;p&gt;You will need an account with the following services:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://cda.ms/16X&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Microsoft Azure&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://cda.ms/17f&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Microsoft Teams&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;deployment-instructions&#34;&gt;Deployment Instructions&lt;/h3&gt;
&lt;p&gt;The blue button below will deploy all resources needed for this solution in to the Resource Group and Azure region of your choice. The name you choose also determines the URL used to view the Status Page as well as the incoming URL used to trigger updates.&lt;/p&gt;
&lt;h4 id=&#34;steps-to-deploy&#34;&gt;Steps To Deploy&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; Create the Azure Function app, Storage account, and SignalR Service with this button: 
&lt;a href=&#34;https://azuredeploy.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://azuredeploy.net/deploybutton.png&#34; alt=&#34;Deploy to Azure&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This will begin deploying everything needed for the solution and will provide a link to the public facing URL of the Status Page as well as a link to your new resource group where you will continue with step 2.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt; In the Azure portal, open the Storage account and add a table named &lt;code&gt;statuses&lt;/code&gt;. You do not need to set any properties or add records.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jhandcdn.blob.core.windows.net/blob/CreateStatusesTable.gif&#34; alt=&#34;Create table&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;3.&lt;/strong&gt; Navigate to the function app, and open the &lt;code&gt;teams-webhook&lt;/code&gt; function. Click &amp;ldquo;Get Function URL&amp;rdquo; and copy the URL.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4.&lt;/strong&gt; Open the URL of the Function in a new tab. This is &amp;ldquo;Status Page&amp;rdquo; that will change automatically when updated. It is NOT the same as the &amp;ldquo;Get Function URL&amp;rdquo; used in the next step.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jhandcdn.blob.core.windows.net/blob/ReindeerGuidanceSystemURL.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;5.&lt;/strong&gt; Open Microsoft Teams and navigate to the &amp;ldquo;Apps&amp;rdquo; page of the team in which you want to create the bot. Click &amp;ldquo;Create outgoing webhook&amp;rdquo;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use &lt;code&gt;StatusPage&lt;/code&gt; as the bot name (this is hardcoded, for now).&lt;/li&gt;
&lt;li&gt;Paste in the function URL, and enter a description and press the create button.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;You will be prompted with a secret code for validating webhook calls from Teams. We currently do not use this. Close the dialog box.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;
&lt;a href=&#34;https://jhandcdn.blob.core.windows.net/blob/Webhook.gif&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://jhandcdn.blob.core.windows.net/blob/WebhookStatic.png&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;6.&lt;/strong&gt; In Microsoft Teams, update the status page by typing &lt;code&gt;@StatusPage&lt;/code&gt; to summon the bot followed by &lt;code&gt;open We are experiencing a problem. Standby for more information&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Available commands are:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bot&#34;&gt;        @StatusPage open [message]
        @StatusPage update [message]
        @StatusPage close [message]
        @StatusPage help`
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;p&gt;This solution is based on the on-stage demonstrations built for Microsoft Ignite The Tour.&lt;/p&gt;
&lt;p&gt;To learn more about the full demonstration, view the repo for &amp;ldquo;
&lt;a href=&#34;https://myignite.techcommunity.microsoft.com/sessions/82997/?WT.mc_id=oncalllife-blog-jahand&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OPS20 - Responding To Incidents&lt;/a&gt;&amp;quot;. Huge Thanks to 
&lt;a href=&#34;https://github.com/anthonychu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Anthony Chu&lt;/a&gt; in bringing this to life.&lt;/p&gt;
&lt;p&gt;Now, let&amp;rsquo;s move on to

&lt;a href=&#34;/post/understanding-why-we-learn-from-incidents/&#34;&gt;Understanding Why We Learn From Incidents&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Creating an Incident Tracking Tool with Azure Boards</title>
      <link>/post/creating-an-incident-tracking-tool-with-azure-boards/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/creating-an-incident-tracking-tool-with-azure-boards/</guid>
      <description>&lt;p&gt;Tracking incidents is as easy as setting up a datastore, like the table storage used for the on-call roster. However, why reinvent the wheel? Why not use something already available, customizable, extensible, and free?&lt;/p&gt;
&lt;p&gt;Azure Boards is my tool of choice in this tutorial, but honestly this could probably be done with any popular project management tool with an API.&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    First, login or create a &lt;a href=&#34;https://azure.microsoft.com/?wt.mc_id=oncalllife-blog-jahand&#34;&gt;free Azure &lt;strong&gt;Devops&lt;/strong&gt; account&lt;/a&gt;.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; Select an existing or create a new project in &lt;strong&gt;Azure Devops&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt; Click on &lt;strong&gt;Work Items&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the left navigation menu, choose &lt;strong&gt;Work Items&lt;/strong&gt; under the Boards group. This is what we will use to track our incidents.&lt;/p&gt;
&lt;p&gt;But before we do, we need to create a custom work item type and then start a new project based on different default options.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;work-items-boards.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3.&lt;/strong&gt; Navigate to the &lt;strong&gt;Project Configuration&lt;/strong&gt; settings&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Project Settings&lt;/strong&gt; icon is in the lower left. Then select &lt;strong&gt;Project Configuration&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;From here we will create a custom work item type for incidents.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;project-settings.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4.&lt;/strong&gt; Click the &amp;ldquo;&lt;strong&gt;go to the process customization page&lt;/strong&gt;&amp;rdquo; link in the upper right&lt;/p&gt;
&lt;p&gt;This is where we can view existing &lt;strong&gt;Work Item Types&lt;/strong&gt; as well as create new ones. But first, we have to create a new inherited process in order to enable customizations.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;custom-work-item-types.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5.&lt;/strong&gt; Click on the &amp;ldquo;&lt;strong&gt;create an inherited process&lt;/strong&gt;&amp;rdquo; link near the top&lt;/p&gt;
&lt;p&gt;Give it a name, such as &lt;code&gt;on-call&lt;/code&gt; and a description.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;create-inherited-process.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once you click &amp;ldquo;&lt;strong&gt;Create process&lt;/strong&gt;&amp;rdquo; you&amp;rsquo;ll be returned to the previous screen where you can now select the &lt;strong&gt;+ New work item type&lt;/strong&gt; option.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;create-new-work-item-type.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Give it a name, like &lt;code&gt;Incident&lt;/code&gt;, a description, icon, and icon color. Then press create.&lt;/p&gt;
&lt;p&gt;From the next screen click on &lt;strong&gt;Process&lt;/strong&gt; found in the &lt;strong&gt;Boards&lt;/strong&gt; section of the left navigation pane. You&amp;rsquo;ll see the new inherited process &amp;ldquo;on-call&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;6.&lt;/strong&gt; Click the settings elipses &lt;strong&gt;(&amp;hellip;)&lt;/strong&gt; followed by &lt;strong&gt;+ New team project&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;7.&lt;/strong&gt; Click create &lt;strong&gt;new project&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;new-team-project-option.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Give the project a name, description, and a few other selections and press &lt;strong&gt;create&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;create-new-project.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;We will use this new project for all incident tracking leaving the original project for something else.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s check to see if our new work item type is available. Return to Azure Boards.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;8.&lt;/strong&gt; Click the &lt;strong&gt;+ New Work Item&lt;/strong&gt; option&lt;/p&gt;
&lt;p&gt;You&amp;rsquo;ll now see an additional work item type of &amp;ldquo;&lt;strong&gt;incident&lt;/strong&gt;&amp;quot;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;see-new-work-item-type.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Select the Incident option to view the default fields available to track details.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;create-new-work-item.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can see that we have a number of fields available to us by default such as &lt;strong&gt;Assigned To&lt;/strong&gt;, &lt;strong&gt;State&lt;/strong&gt;, &lt;strong&gt;Discussion&lt;/strong&gt;, and more.&lt;/p&gt;
&lt;p&gt;We now have a basic way of tracking incidents. Any time an incident is detected, we can add a new work item type of &amp;ldquo;incident&amp;rdquo; along with the important details so that there is a central place for everyone to stay informed.&lt;/p&gt;
&lt;p&gt;However, the default settings only provide us a few important things that we want to track. We mentioned before that we&amp;rsquo;d like to track when, how, who, and what we know about the incident.&lt;/p&gt;
&lt;p&gt;We can easily add and modify fields within Azure Boards to allow for tracking additional details we know are important to our incident response efforts.&lt;/p&gt;
&lt;p&gt;Now, let&amp;rsquo;s talk about ways of 
&lt;a href=&#34;/post/customizing-azure-boards-for-incident-tracking/&#34;&gt;Customizing Azure Boards for Incident Tracking&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Defining Alerts</title>
      <link>/post/defining-alerts/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/defining-alerts/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&amp;quot;An alert is something which requires a human to perform an action.&amp;rdquo; - Pagerduty 
&lt;a href=&#34;https://response.pagerduty.com/oncall/alerting_principles/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Alerting Principles&amp;rdquo;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To understand why alerting can create a problem, you need to think about the purpose of alerts and how they differ from other monitoring components.&lt;/p&gt;
&lt;p&gt;Actionable alerts are &lt;em&gt;not&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Logs.&lt;/strong&gt; Alerts are not records of events; that’s the role of logs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Notifications.&lt;/strong&gt; Alerts are not intended to announce non-critical
occurrences such as the completion of a software build.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Heartbeats.&lt;/strong&gt; Alerts shouldn’t be used to document failure of a heartbeat
signal periodically sent between two systems at regular intervals.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Actionable alerts are used for situations in which you need a human to
investigate and intervene to remediate the problem. Alerts should be
communications that something exceptional or unexpected has happened that
requires someone’s attention.&lt;/p&gt;
&lt;p&gt;A lot of thought should be put in to how alerts are delivered and when it is necessary. 
&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/azure-monitor/platform/alerts-overview/?wt.mc_id=oncalllife-blog-jahand&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;More on how to configure alerts in Azure can be found here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If the event is something that the system can handle through automated
processes, such as scaling resources within a preset limit, an alert is not
necessary. A simple line in a log should suffice.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/understanding-actionable-alerts&#34;&gt;Understanding Actionable Alerts&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Defining Incidents</title>
      <link>/post/defining-incidents/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/defining-incidents/</guid>
      <description>&lt;p&gt;If you search online for &amp;ldquo;Incident Response&amp;rdquo; a majority of what you&amp;rsquo;ll find is information related to security threats and breaches. What doesn&amp;rsquo;t show up in the results is stuff about how to properly respond to threats related to something else entirely.&lt;/p&gt;
&lt;p&gt;How should a business respond to technical challenges and failures as they come up? The ones that affect reliability concerns such as availability, latency, correctness, and others. What happens when service level expectations are breached and it&amp;rsquo;s time for a human to get involved?&lt;/p&gt;
&lt;p&gt;Services such as VictorOps, PagerDuty, and others provide &amp;ldquo;on-call&amp;rdquo; solutions as well as documentation and best practices regarding this type of incident management. Service Now has opinions as well but the language is aimed more for those who follow ITSM guidance regarding service management. Ticketing with a tiered support structure doesn&amp;rsquo;t provide the fasted path to uptime for many companies however.&lt;/p&gt;
&lt;p&gt;In the devops and web operations space, the idea of anyone but the engineers building the system responding to customer impacting problems is completely unacceptable. Irresponsible even. Time is of the essence and those who helped build the applications and underlying infrastructure are the best suited to maintain it&amp;rsquo;s health and upgrades in a production environment.&lt;/p&gt;
&lt;p&gt;Exactly when an engineer should be expected to take action is why we need to define what we mean by an incident.&lt;/p&gt;
&lt;p&gt;We can all agree that an incident is a “&lt;strong&gt;service disruption&lt;/strong&gt;” - something that is affecting our user&amp;rsquo;s ability to use the services they have come to rely on.&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s a given. However, there are other things about incidents that are often overlooked or never considered. For example incidents are commonly subjective, feared, and unexpected&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;from-microsofts-icm-documentation&#34;&gt;From Microsoft&amp;rsquo;s IcM Documentation&lt;/h2&gt;
&lt;h3 id=&#34;what-is-an-incident&#34;&gt;What is an incident?&lt;/h3&gt;
&lt;p&gt;An incident is any unplanned interruption or degradation of a product or service that is causing customer impact. For example, a bad HTTP request, slow connection, security vulnerability, or customer-reported error message could constitute an incident. Every service across Microsoft has a different definition of what an incident is and when one should be triggered.&lt;/p&gt;
&lt;p&gt;Sometimes, an incident can be severe enough to affect many different services and customers. For example, a datacenter power failure may impact dozens of Microsoft products. Severe incidents with broad impact are called outages.&lt;/p&gt;
&lt;p&gt;Typically any customer impacting incident must be mitigated as soon as possible to minimize the customer impact. Organizations track Time-To-Mitigate (TTM) metrics for their services. For example: In Azure, TTM must be &amp;lt;= 30mins for any customer impacting incident.&lt;/p&gt;
&lt;h3 id=&#34;what-is-incident-management&#34;&gt;What is incident management?&lt;/h3&gt;
&lt;p&gt;Incident Management is the process of detecting a live-site problem with a service, creating an incident, determining the cause, restoring the service to full operation, and capturing learnings to prevent it from happening again.&lt;/p&gt;
&lt;h3 id=&#34;what-is-icm&#34;&gt;What is IcM?&lt;/h3&gt;
&lt;p&gt;IcM is the one incident management system for all Microsoft services. IcM provides tools for managing live site and on call rotations. IcM runs around the clock to keep services working across the world. Incident Types supported are &amp;ldquo;Livesite Incident&amp;rdquo;, &amp;ldquo;Deployment Incident&amp;rdquo; and &amp;ldquo;Customer Reported Incidents&amp;rdquo;. No additional types will be supported.&lt;/p&gt;
&lt;h3 id=&#34;what-icm-is-not&#34;&gt;What IcM is not?&lt;/h3&gt;
&lt;p&gt;IcM is not intended to be a ticketing solution. IcM was designed for incidents that must be mitigated within minutes to minimize the customer impact. Here are some differences:&lt;/p&gt;
&lt;p&gt;In a ticketing solution, a ticket can take &amp;lsquo;n&amp;rsquo; days to resolve. In IcM, incidents need to be resolved as soon as possible to minimize customer impact.&lt;/p&gt;
&lt;p&gt;In a ticketing solution, teams might want to pause or have a count down timer for tickets. In IcM, this will not be supported because the incident has to be resolved in minutes.
In a ticketing solution, teams might want to customize the workflows. In IcM, incident workflow is fixed: Create -&amp;gt; Acknowledge -&amp;gt; Mitigate -&amp;gt; Resolve.&lt;/p&gt;
&lt;p&gt;In a ticketing solution, teams might want different &amp;lsquo;Types&amp;rsquo; and &amp;lsquo;Sub-Types&amp;rsquo;. In IcM, only types that will be supported are &amp;ldquo;Livesite Incident&amp;rdquo;, &amp;ldquo;Deployment Incident&amp;rdquo; and &amp;ldquo;Customer Reported Incidents&amp;rdquo;. This is to ensure that Service teams are focused on addressing the customer impacting incidents as soon as possible.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/exploring-subjectivity-of-incidents/&#34;&gt;Exploring Subjectivity of Incidents&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Defining Site Reliability Engineering</title>
      <link>/post/defining-site-reliability-engineering/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/defining-site-reliability-engineering/</guid>
      <description>&lt;p&gt;Site Reliability Engineering is an engineering discipline devoted to helping
organizations sustainably achieve the appropriate level of reliability in their
systems, services, and products.&lt;/p&gt;
&lt;p&gt;The key concepts to take away from this definition are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Reliability.&lt;/strong&gt; You learned in the introductory module that there are
multiple aspects to reliability and later in this module, you’ll examine
each in more detail. You also learned about the importance of reliability –
why it matters.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sustainability.&lt;/strong&gt; In this context, “sustainable” refers to the role of
people in creating a sustainable operations practice. Reliable systems,
services, products are built by people. It is crucial to the concept of SRE
to implement an operations practice that is sustainable over time, so that
people are able to bring their best to the job.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Appropriateness.&lt;/strong&gt; It’s important to understand that 100% reliability
isn’t often possible, especially in today’s complex systems with
dependencies on other systems. 100% reliability means zero down time, which
also means no opportunity to make any changes or improvements that could
create some down time. Instead of striving for a goal of absolute
reliability, determine the appropriate level of reliability for a particular
application or service.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The role of a site reliability engineer bridges the span between operations and
development but also goes beyond DevOps, and the SRE philosophy is the basis of
a new and more efficient and effective approach to building and operating
reliable systems and applications.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Defining the Post-incident Review</title>
      <link>/post/defining-the-post-incident-review/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/defining-the-post-incident-review/</guid>
      <description>&lt;p&gt;To conduct a good post-incident review, you must first ensure that everyone is
on the same page. Toward that end, remember that language matters; there are
terms you should use and some you shouldn’t. Key points in conducting the review
are who to include (everyone) and when to do it (within twenty-four to
thirty-six hours). This will help you accomplish the purpose of the review: to
learn and improve.&lt;/p&gt;
&lt;p&gt;We learn from incidents by conducting a post-incident review, which happens
during the analysis phase. You should do a post-incident review after every
significant incident.&lt;/p&gt;
&lt;p&gt;(Placeholder: graphic of incident response lifecycle in Slide #28)&lt;/p&gt;
&lt;p&gt;Although the formal review takes place after the response and remediation
phases, you begin to set the stage for your analysis as soon as you receive an
actionable alert that an incident has occurred, inform team members, and begin a
conversation around the incident.&lt;/p&gt;
&lt;h2 id=&#34;language-matters&#34;&gt;Language matters&lt;/h2&gt;
&lt;p&gt;In conducting that conversation, and as you go through this unit, there is an
important point that you need to keep in mind: &lt;em&gt;language matters.&lt;/em&gt; There are
specific terms that you should use and others that you should deliberately avoid
using. That’s because the words you use affect how you think about what happened
and can dramatically impact what and how much you learn from the incident.&lt;/p&gt;
&lt;p&gt;This has been shown by research in safety-critical industries such as aviation,
medicine, search and rescue, firefighting, and more. This field of research is
known as resilience engineering.&lt;/p&gt;
&lt;p&gt;Resilience engineering principles are applicable to the technology sector, as
well. In this module, you’ll learn some of the useful concepts from resilience
engineering, including four of the most common traps people fall into when
attempting to learn from failure.&lt;/p&gt;
&lt;h2 id=&#34;defining-the-post-incident-review&#34;&gt;Defining the post-incident review&lt;/h2&gt;
&lt;p&gt;Not everyone uses exactly the same language to refer to the results of the
analysis phase. You might call it:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Post-incident review&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Post-incident learning review&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Postmortem&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Retrospective&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition, not everyone goes about it in exactly the same way. For example,
many people start by getting everyone who had any connection to the incident
into a room, while other people choose to create the review via individual
interviews and then report back to the group.&lt;/p&gt;
&lt;p&gt;The latter method often works better when group settings in your organization
are more difficult because of group dynamics or personalities or may be based on
how distributed the group member are, or the type of incident. You should do
what works best for your team and the circumstances.&lt;/p&gt;
&lt;p&gt;Whatever you call it and however you organize it, there are a couple of key
points:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Inclusiveness:&lt;/strong&gt; You should try to include in the post-incident review
&lt;em&gt;everyone who was involved&lt;/em&gt; in the incident response.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Timing&lt;/strong&gt;: you should perform the post-incident review &lt;em&gt;within twenty-four
to thirty-six hours&lt;/em&gt; after the event if at all possible. Neuroscience has
confirmed that human memory is notoriously unreliable; people forget things.
The more time that passes after an event, the less detailed and specific
memories tend to be.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;purpose-of-the-post-incident-review&#34;&gt;Purpose of the post-incident review&lt;/h2&gt;
&lt;p&gt;The goal of the post-incident review is so your team can learn and improve. You
will want to learn about the systems and about the things that you had put in
place that worked or didn’t work, so you can make improvements.&lt;/p&gt;
&lt;p&gt;At the same time, you should remember that action items that you generate –
reports, tasks, feedback – are useful but are peripheral to the point of the
process, which is to learn and improve. Action items are a secondary effect.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/breaking-down-the-components-of-a-post-incident-review/&#34;&gt;Breaking Down the Components of a Post-incident Review&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Distinguishing Prevention and Preparation</title>
      <link>/post/distinguishing-prevention-and-preparation/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/distinguishing-prevention-and-preparation/</guid>
      <description>&lt;p&gt;You’ve probably heard all your life the adage, attributed to Benjamin Franklin,
that “an ounce of prevention is worth a pound of cure.” The accepted meaning is
that it’s better to keep a problem from happening than to fix it after it’s
happened.&lt;/p&gt;
&lt;p&gt;In your efforts to achieve a high level of reliability for your systems and
services, you should do everything possible to prevent incidents from occurring.
However, due to the complexity of those systems, as explained above, prevention
isn’t always possible.&lt;/p&gt;
&lt;p&gt;Thus, you must take a two-pronged approach to failure: prevention on the one
hand, and when that isn’t possible, preparation to respond – quickly and
effectively. These two are interlinked.&lt;/p&gt;
&lt;p&gt;Here’s an example of why it’s essential to do both: Sometimes an organization
will deploy an automated system. It works well. In fact, it works almost too
well – because people may take for granted that it will always work. They don’t
make proper preparation for the day that it doesn’t work. When that day comes –
as it inevitably will, sooner or later – and the system fails, it fails
spectacularly, and it’s much harder for operators to understand what went wrong.&lt;/p&gt;
&lt;p&gt;The systems you work on are made up of more than the technology. In fact, you
don’t work “on” or “with” a system; you work &lt;em&gt;in&lt;/em&gt; the system. You are part of
the system. Complex systems include both technical components (hardware,
software) and human components (people – and their personalities, training, and
knowledge).&lt;/p&gt;
&lt;p&gt;How the humans respond when things go wrong is as important as preventing things
from going wrong in the first place. We learn from failure to respond faster and
better.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/defining-the-post-incident-review/&#34;&gt;Defining the Post-incident Review&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Establishing Communication Channels</title>
      <link>/post/establishing-communications-channel/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/establishing-communications-channel/</guid>
      <description>&lt;p&gt;To address some of the challenges around how we communicate we also want to find a way to create a unique channel or space for engineers to discuss the details of the incident - a “conversation bridge” in our persistent group chat tool -which for Tailwind Traders is Microsoft Teams.&lt;/p&gt;
&lt;p&gt;We want a channel that is unique to the incident only. We do not want conversations about other engineering efforts.We don’t want conversations about what people are doing for lunch. We ONLY want conversations related to the incident. Because then we can take that text (or data) and analyze later in a Post-incident review.&lt;/p&gt;
&lt;h2 id=&#34;next-steps&#34;&gt;Next steps&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;/post/understanding-why-we-learn-from-incidents/&#34;&gt;Understanding Why We Learn From Incidents&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Establishing On-call Roles</title>
      <link>/post/establishing-oncall-roles/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/establishing-oncall-roles/</guid>
      <description>&lt;p&gt;Creating a repeatable response plan means establishing who does what when something goes wrong. We don&amp;rsquo;t want there to be any question around who is supposed to be doing what.&lt;/p&gt;
&lt;p&gt;Because of this, it is important to establish roles and the associated expectations. This isn&amp;rsquo;t a separation of duties exercise. In fact, we want to encourage less of that. It is however, a way of establishing better coordination and communication. It prevents people from stepping on each others toes while enabling cross-collaboration amongst not only on-call rosters, but an entire organization.&lt;/p&gt;
&lt;p&gt;The first role we need to talk about is the &lt;strong&gt;Primary Responder&lt;/strong&gt; – the Primary “On-call” engineer.&lt;/p&gt;
&lt;p&gt;This person is expected to acknowledge their awareness of an incident once the alert has been received.&lt;/p&gt;
&lt;p&gt;Then we have the &lt;strong&gt;secondary responder&lt;/strong&gt; – who is there as back up -Another engineer who can step in if the primary responder is unavailable or unreachable. Or if they just need another pair of eyes.&lt;/p&gt;
&lt;p&gt;Another key role to identify, in many cases, is the &lt;strong&gt;incident commander&lt;/strong&gt;. An incident commander can be extremely helpful when you have got a large-scale outage that effects a lot of different components or requires coordination across many teams and different systems. They are great for making sure that engineers stay focused and they are working on their own remediation efforts&amp;hellip; Ensuring people are not stepping on each other or undoing each other&amp;rsquo;s work.  It is good to have a central person who can track what is going on and who is doing what.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Communication Coordinator&lt;/strong&gt; is meant to be the person working in conjunction with the incident commander to share more information beyond those who are in the firefight actively working to recover from the incident itself. That could be customers. It could be the sales and marketing teams. Maybe your customer support. There are many people within an organization who need to be made aware of what’s taking place and the status around how things are progressing. It&amp;rsquo;s always good to put someone in charge of managing that communication and making sure that other stakeholders are aware of what is happening and what’s being done.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;scribe&lt;/strong&gt;’s role is to document the conversation in as much detail as possible. Teams commonly use phone bridges, conference calls, or video chat to get everyone together and try to understand what is going on, which can certainly help create space for the conversation. However, it is difficult for us to go through and understand in detail what the engineers were saying and doing unless it is transcribed. As a result, a scribe is that person who can help us document as much as possible to review later. What were people saying, doing, feeling, and even experiencing?  It is all data to be analyzed – but only if we capture it.&lt;/p&gt;
&lt;p&gt;It’s quite common within on-call rosters to identify &lt;strong&gt;subject matter experts&lt;/strong&gt;, so that early responders know who to escalate too quickly. These people should not be on call all the time, of course, but we do want to be able to identify who is our database expert. Who is our front-end expert? Who are the people that we can reach out to if our primary and secondary responders are not able to diagnose and resolve the issue themselves?&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s take a closer look at each of these roles to better understand their place within our incident response efforts.&lt;/p&gt;
&lt;h2 id=&#34;next-steps&#34;&gt;Next steps&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;/post/identifying-the-primary-responder/&#34;&gt;Identifying the Primary Responder&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Establishing On-call Rosters</title>
      <link>/post/establishing-oncall-rosters/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/establishing-oncall-rosters/</guid>
      <description>&lt;p&gt;Rosters establish a framework around who is on-call at any given point. A roster, or team, is made up of multiple engineers. Rosters can also contain multiple rotations. I&amp;rsquo;m testing out how to edit a page.&lt;/p&gt;
&lt;h2 id=&#34;next-steps&#34;&gt;Next steps&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;/post/creating-an-oncall-roster-using-azure-table-storage/&#34;&gt;Creating an on-call roster using Azure Table Storage&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Establishing On-call Rotations</title>
      <link>/post/establishing-oncall-rotations/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/establishing-oncall-rotations/</guid>
      <description>&lt;p&gt;People shouldn&amp;rsquo;t have to be &lt;em&gt;on-call&lt;/em&gt; for long periods of time. It&amp;rsquo;s detrimental to their health and therefore the systems they create and look after.&lt;/p&gt;
&lt;p&gt;In order to make sure people aren&amp;rsquo;t expected to respond to problems 24-hours a day, indefinitely, we use rotations. Also referred to as scheduled shifts. That way people can take turns.&lt;/p&gt;
&lt;p&gt;Being on-call requires a heavy cognitive task which then negatively impacts many aspects of a person&amp;rsquo;s life. Shifts allow engineers to take “on-call” responsibility for their own specific recurring rotation and share the load in a way that keeps the human element a priority.&lt;/p&gt;
&lt;p&gt;Site Reliability Engineering is an engineering discipline devoted to helping an organization &lt;strong&gt;sustainably&lt;/strong&gt; achieve the appropriate level of reliability in their systems, services, and products. Long periods of on-call responsibilities is &lt;strong&gt;not&lt;/strong&gt; sustainable.&lt;/p&gt;
&lt;p&gt;When creating shifts there are a number of common approaches.&lt;/p&gt;
&lt;h2 id=&#34;24-x-7&#34;&gt;24 x 7&lt;/h2&gt;
&lt;p&gt;The majority of rotations used by teams are known as 27 x 7 shifts where engineers will be “on-call” for several days in a row. However, most “Elite/High performers” would agree that rotations longer than 3 or 4 days are detrimental to the overall health of engineering staff and therefore the entire system.&lt;/p&gt;
&lt;h2 id=&#34;follow-the-sun&#34;&gt;Follow the Sun&lt;/h2&gt;
&lt;p&gt;Follow the sun shifts are nice for distributed teams. These allow for engineers to schedule their “on-call” shifts only during their normal working office hours. As they end their day and go home, engineers in a different time zone can take over.&lt;/p&gt;
&lt;p&gt;And of course, there are many ways to customize shifts, especially for weekends when engineers need more flexibility. Engineers should be able to hand off the role to someone when personal conflicts arise.&lt;/p&gt;
&lt;p&gt;Once roles, rosters, and rotations have been determined and put in place, we can now focus our attion on methods of 
&lt;a href=&#34;/post/tracking-incident-details/&#34;&gt;Tracking Incident Details&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Examining All Aspects of Reliability</title>
      <link>/post/examining-all-aspects-of-reliability/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/examining-all-aspects-of-reliability/</guid>
      <description>&lt;p&gt;When deciding what to monitor for reliability it becomes clear that there are many approaches because there are many aspects to reliability. It&amp;rsquo;s important to examine the reliability of your service from the user&amp;rsquo;s perspective to determine what is important and how to prioritize.&lt;/p&gt;
&lt;h2 id=&#34;what-does-reliability-mean-to-the-user&#34;&gt;What Does Reliability Mean to the User?&lt;/h2&gt;
&lt;p&gt;To determine the reliability of a system, service, application, or process many look to a combination of the following eight aspects of reliability.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Availability&lt;/li&gt;
&lt;li&gt;Latency&lt;/li&gt;
&lt;li&gt;Throughput&lt;/li&gt;
&lt;li&gt;Coverage&lt;/li&gt;
&lt;li&gt;Correctness&lt;/li&gt;
&lt;li&gt;Fidelity&lt;/li&gt;
&lt;li&gt;Freshness&lt;/li&gt;
&lt;li&gt;Durability&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Once it is determined which aspects to measure and how, we can begin understanding the behavior of our systems under &amp;ldquo;normal&amp;rdquo; conditions.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;aspects-of-reliability.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;establish-a-baseline&#34;&gt;Establish a Baseline&lt;/h2&gt;
&lt;p&gt;Establishing a baseline of reliable performance helps set expectations and recognize deviations from what you might call &amp;ldquo;normal&amp;rdquo;. Once you have a baseline, you can do a comparison.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/exploring-the-availability-aspect-of-reliability/&#34;&gt;Exploring the Availability Aspect of Reliability&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE: Look in to &amp;ldquo;The Golden Signals&amp;rdquo; - Jason Hand&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Examining Common Traps</title>
      <link>/post/examining-common-traps/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/examining-common-traps/</guid>
      <description>&lt;p&gt;As we delve into our post-incident review, we need to be on guard against some
human tendencies that can lead us to inaccurate or incomplete conclusions and
distract us from accomplishing the core purpose of the review: learning about
our systems so as to improve their reliability.&lt;/p&gt;
&lt;p&gt;Now you have a roadmap help you get started on the post-incident review process,
but it would also be useful to know about some of the obstacles you might
encounter on this journey.&lt;/p&gt;
&lt;p&gt;Learning from your own mistakes is something everyone should all do, but you
don’t always have to experience something first-hand to learn from it. Learning
from the mistakes of others, without criticizing or judging them, allows you to
benefit from those lessons without personally suffering the consequences.&lt;/p&gt;
&lt;p&gt;In this unit, you’ll find out about some common traps that others have fallen
into during the post-incident review process and how to avoid them.&lt;/p&gt;
&lt;h2 id=&#34;trap-1-attribution-to-human-error&#34;&gt;Trap 1: Attribution to “human error”&lt;/h2&gt;
&lt;h2 id=&#34;trap-2-counterfactual-reasoning&#34;&gt;Trap 2: Counterfactual reasoning&lt;/h2&gt;
&lt;h2 id=&#34;trap-3-normative-language&#34;&gt;Trap 3: Normative language&lt;/h2&gt;
&lt;h2 id=&#34;trap-4-mechanistic-reasoning&#34;&gt;Trap 4: Mechanistic reasoning&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;/post/examining-attribution-of-human-error/&#34;&gt;Examining Attribution of Human Error&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Examining Counter-factual Reasoning</title>
      <link>/post/examining-counter-factual-reasoning/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/examining-counter-factual-reasoning/</guid>
      <description>&lt;p&gt;In the field of psychology, counterfactual thinking is a concept that’s
associated with the human tendency to invent possible alternatives to past
events – how things might have turned out differently.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Counterfactual&lt;/em&gt; means “contrary to facts,” and &lt;em&gt;counterfactual reasoning&lt;/em&gt;
refers to telling a story about events that did not happen, in order to explain
the events that did. You can identify counterfactual statements by key phrases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Could have&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Should have&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Would have&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Failed to&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Did not&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If only&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some examples of counterfactual statements related to post-incident reviews:&lt;/p&gt;
&lt;p&gt;“The monitoring system failed to detect the problem.”&lt;/p&gt;
&lt;p&gt;“The engineer did not check the validity of the configuration before enacting
it.”&lt;/p&gt;
&lt;p&gt;“This could have been picked up in the canary environment.”&lt;/p&gt;
&lt;p&gt;The problem with this type of reasoning is that you’re talking about things that
didn’t happen instead of taking the time to understand how what did happen,
happened. You don’t learn anything from this speculation.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/examining-mechanistic-reasoning/&#34;&gt;Examining Mechanistic Reasoning&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Examining Mechanistic Reasoning</title>
      <link>/post/examining-mechanistic-reasoning/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/examining-mechanistic-reasoning/</guid>
      <description>&lt;p&gt;&lt;em&gt;Mechanistic reasoning&lt;/em&gt; refers to the concept that a particular outcome can be
inferred from intervention. It’s sometimes called the &lt;em&gt;meddling kids syndrome&lt;/em&gt;
based on the premise that “Our system would have worked fine … if it hadn’t been
for those meddling kids.”&lt;/p&gt;
&lt;p&gt;When you use mechanistic reasoning in your post-incident review, you build your
conclusions on the fallacy that the systems you work with and within are
basically working correctly, and if only those “meddling kids” hadn’t done
whatever they did, the failure would not have occurred.&lt;/p&gt;
&lt;p&gt;However, that’s not how systems work.&lt;/p&gt;
&lt;p&gt;To illustrate this point, imagine the following scenario: You work on a
production service. Now you’re told that you are not allowed to touch or do
anything to that service. Everything outside your team continues as before –
customers continue to use the service, external dependencies continue to change,
the Internet functions normally.&lt;/p&gt;
&lt;p&gt;But you can’t make any changes to the code or configuration. No deployments, no
control plane operations, nothing.&lt;/p&gt;
&lt;p&gt;Do you think your service would still be running as expected after a day? After
a week? After a month? After a year? How long could you realistically expect
your service to keep running without human intervention?&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Human adaptive capacity is necessary to keep our systems up and running&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The only reason your systems are up and running in the first place is because of
the actions of humans in the control loop. It’s only through human action
and ability to adapt to changing circumstances that the system continues to
work.&lt;/p&gt;
&lt;p&gt;Therefore, it’s erroneous to conclude the system was “basically working…
if it hadn’t been for those meddling kids.” In fact, the reliability of your
service is not independent of the humans who work on it. Instead, it’s a direct
result of the work that the humans do on it every day.&lt;/p&gt;
&lt;p&gt;The problem with mechanistic reasoning is that it leads you down a path where
you believe that finding the faulty human is equivalent to finding the problem.
However, that same faulty human has been improvising and adapting to keep the
system running for weeks and months.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/examining-normative-language/&#34;&gt;Examining Normative Language&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Examining Normative Language</title>
      <link>/post/examining-normative-language/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/examining-normative-language/</guid>
      <description>&lt;p&gt;&lt;em&gt;Normativity&lt;/em&gt; relates to the human tendency to designate some actions and
outcomes as desirable or good and other actions or outcomes as undesirable or
bad. A norm is a standard of correctness agreed upon by a society.&lt;/p&gt;
&lt;p&gt;When you use normative language in a post-incident review, you judge the
decisions and actions of those responding to the incident with the benefit of
&lt;em&gt;hindsight&lt;/em&gt;. This language implies that there was an obviously correct course of
action that the operator should have followed.&lt;/p&gt;
&lt;p&gt;Normative language can usually be identified by adverbs such as “inadequately,”
“carelessly,” “hastily,” and so forth.&lt;/p&gt;
&lt;p&gt;Normative thinking leads you to judge decisions based on their outcomes. This
isn’t logical because the outcome is the &lt;em&gt;only piece of information that was not
available&lt;/em&gt; to those who made the decisions and judgments.&lt;/p&gt;
&lt;p&gt;The problem with normative thinking is that you neglect to understand how the
actions of the operators made sense to them at the time.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/&#34;&gt;Home&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Examining the Attribution of Human Error</title>
      <link>/post/examining-attribution-of-human-error/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/examining-attribution-of-human-error/</guid>
      <description>&lt;p&gt;Humans make mistakes. However, &lt;em&gt;human error&lt;/em&gt; is not a diagnosis; it’s a symptom.
When human error is deemed to be the reason for a failure, you may stop there
instead of further analyzing the incident to determine the &lt;em&gt;root cause –&lt;/em&gt; which
is a deeper, systemic issue.&lt;/p&gt;
&lt;p&gt;System design, organizational context, and personal context all affect when, how
and with what impact people make mistakes. &lt;em&gt;“&lt;em&gt;Human error&lt;/em&gt;”&lt;/em&gt; is a label that
causes you to quit investigating at precisely the moment when you’re about to
discover something interesting about your system.&lt;/p&gt;
&lt;p&gt;The problem with the “human error” conclusion in investigations is that it
causes you to lose sight of the fact that what the humans did made sense to them
at the time. Mistakes, by definition, aren’t deliberate, so they didn’t intend
to make a mistake.&lt;/p&gt;
&lt;p&gt;When we see or hear “human error”, it is a signal that we need to look deeper.
Root cause analysis is needed to identify the sequence of events that resulted
in the human error.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/examining-counter-factual-reasoning/&#34;&gt;Examining Counter-factural Reasoning&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Exploring Fear of Incidents</title>
      <link>/post/exploring-fear-of-incidents/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/exploring-fear-of-incidents/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&amp;quot;There is also a subtler effect, which is that the fear of on-call is often enough by itself to radically change people’s behavior. Entire development teams reject outright the notion of going on call, because of the impact on their personal lives, family, and in-hours effectiveness.&amp;rdquo; - Niall Murphy, Microsoft &amp;ldquo;Seeking SRE&amp;rdquo;(O&amp;rsquo;Reilly)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In some cases, we downplay the significance of an outage &amp;hellip; or worse &amp;hellip; intentionally mis-label or not report a disruption in service for fear of reprimand.&lt;/p&gt;
&lt;p&gt;Historically, we have felt that incidents reflected poorly in several areas of our engineering efforts and the organization.&lt;/p&gt;
&lt;p&gt;It has not been until more recently through many of the conversations around devops and site reliability engineering, that we are starting to rethink incidents and now view them as opportunities to learn and improve our systems.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/exploring-why-incidents-are-unexpected/&#34;&gt;Exploring Why Incidents Are Unexpected&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Exploring Subjectivity of Incidents</title>
      <link>/post/exploring-subjectivity-of-incidents/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/exploring-subjectivity-of-incidents/</guid>
      <description>&lt;p&gt;If you ask engineers across different organizations and industries, you will get many different answers about what an incident is.&lt;/p&gt;
&lt;p&gt;Sometimes it is only when a customer is affected.&lt;/p&gt;
&lt;p&gt;Others will label disruptions as incidents even if a customer never experienced a thing.&lt;/p&gt;
&lt;p&gt;Subjectivity is an unfortunate property of incidents in a lot of cases, even when it comes to identifying severity levels.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/exploring-fear-of-incidents/&#34;&gt;Exploring Fear of Incidents&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Exploring the Availability Aspect of Reliability</title>
      <link>/post/exploring-the-availability-aspect-of-reliability/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/exploring-the-availability-aspect-of-reliability/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Can people reach a website or service?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is typically the first thing people think of when talking about reliability. Can &amp;ldquo;the thing&amp;rdquo; be reached (and used) when expected?&lt;/p&gt;
&lt;p&gt;This is important from the perspective of both external and internal users who depend on a service. Because of this, availability is a good place to start discussing reliability.&lt;/p&gt;
&lt;h2 id=&#34;availability&#34;&gt;Availability&lt;/h2&gt;
&lt;p&gt;If a service can&amp;rsquo;t be reached at all then it is of no value to its intended user and in some cases detrimental. Availability lets us communicate whether or not our service is ready to go.&lt;/p&gt;
&lt;p&gt;In terms of monitoring for availability, we could set alerts for low disk space of service. This could tip us off to an imminent problem that is known to impact the availability of a service.&lt;/p&gt;
&lt;p&gt;We could also examine logs that tell us in aggregate what the customer is experiencing when attempting to communicate with our service. Are they gaining access or receiving responses? Or are they seeing 404 and 501 error codes?&lt;/p&gt;
&lt;p&gt;Answering those questions gives a more accurate picture of the availability of your service. Just because everything appears to be working, does not mean that is what your user is seeing.&lt;/p&gt;
&lt;p&gt;You can&amp;rsquo;t always do something about availability. The number of hops between a website and a person&amp;rsquo;s mobile device is astounding. Failure could happen at any point between them.&lt;/p&gt;
&lt;p&gt;The closer we can measure that experience from their perspective, the better off we&amp;rsquo;ll be in measuring our availability.&lt;/p&gt;
&lt;p&gt;The next aspect of reliability that often gets a lot of attention is latency. Despite your service being available, if it takes too long to communicate back and forth, there&amp;rsquo;s no denying it is a bad experience for the user. In the high speed world, patience for slow services is nearly non-existent.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/exploring-the-latency-aspect-of-reliability/&#34;&gt;Exploring the Latency Aspect of Reliability&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Exploring the Correctness Aspect of Reliability</title>
      <link>/post/exploring-the-correctness-aspect-of-reliability/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/exploring-the-correctness-aspect-of-reliability/</guid>
      <description>&lt;p&gt;Did the process that you ran on the data yield the correct or expected result? For each input it produced the expected output.&lt;/p&gt;
&lt;p&gt;This is an important factor to include in monitoring for reliability.&lt;/p&gt;
&lt;p&gt;No matter how fast or “always available” your service or site is, if it returns incorrect results, it’s not reliable in the eyes of your customers.&lt;/p&gt;
&lt;p&gt;Monitoring for correctness of results is an important part of monitoring for reliability.&lt;/p&gt;
&lt;p&gt;Next, let&amp;rsquo;s talk about an aspect of reliability that those who work large amounts of data or records are likely to be familiar with, coverage. Did it process all the data?&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/exploring-the-coverage-aspect-of-reliability/&#34;&gt;Exploring the Coverage Aspect of Reliability&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Exploring the Coverage Aspect of Reliability</title>
      <link>/post/exploring-the-coverage-aspect-of-reliability/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/exploring-the-coverage-aspect-of-reliability/</guid>
      <description>&lt;p&gt;Coverage refers to how much of the data that you expected to process was
actually processed.&lt;/p&gt;
&lt;p&gt;Reliability means getting the whole job done, every time. How can we monitor for that in a way that indicates what it&amp;rsquo;s like as a user?&lt;/p&gt;
&lt;p&gt;Another aspect of reliablity that is closely tied to data is our next one, durability. It is really crucial that data written to the service can be read back out again later when desired.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/exploring-the-durability-aspect-of-reliability/&#34;&gt;Exploring the Durability Aspect of Reliability&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Exploring the Durability Aspect of Reliability</title>
      <link>/post/exploring-the-durability-aspect-of-reliability/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/exploring-the-durability-aspect-of-reliability/</guid>
      <description>&lt;p&gt;Durability generally relates to longevity and resilience. It’s the ability to remain functional over time.&lt;/p&gt;
&lt;p&gt;Durability is especially important in situations such as storage systems where it is really crucial that a bit written to the service can be read back out again later when desired.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/exploring-the-fidelity-aspect-of-reliability/&#34;&gt;Exploring the Fidelity Aspect of Reliability&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Exploring the Fidelity Aspect of Reliability</title>
      <link>/post/exploring-the-fidelity-aspect-of-reliability/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/exploring-the-fidelity-aspect-of-reliability/</guid>
      <description>&lt;p&gt;Fidelity pertains to the ability of your service to continue to provide a reduced or degraded experience when something goes wrong.&lt;/p&gt;
&lt;p&gt;For example, if different parts of the home page on your website are provided by different microservices, and one of those microservices goes down, ideally you can still serve the home page with only that section missing or replaced with some static content.&lt;/p&gt;
&lt;p&gt;Fidelity, then, is the measure of how often the page served that degraded or partial experience in comparison to serving the full page as intended with full fidelity.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s common to architect systems in these graceful degradated states.&lt;/p&gt;
&lt;p&gt;However, this means we need to pay attention to just how often and for what duration those degradations are taking place. Users will appreciate the resilient design but lose support if something is always broken from their perspective.&lt;/p&gt;
&lt;p&gt;The next aspect of reliability applies to users who need real time information, updates, and access to take action on something.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/exploring-the-freshness-aspect-of-reliability/&#34;&gt;Exploring the Freshness Aspect of Reliability&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Exploring the Freshness Aspect of Reliability</title>
      <link>/post/exploring-the-freshness-aspect-of-reliability/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/exploring-the-freshness-aspect-of-reliability/</guid>
      <description>&lt;p&gt;Freshness refers to how up-to-date the information is in situations where timeliness matters to the customer.&lt;/p&gt;
&lt;p&gt;Freshness of what is available to users is a big component of reliability to many. If my travel booking tool is out of sync with the airline inventory, it&amp;rsquo;s frustrating to delay the process because something wasn&amp;rsquo;t &amp;ldquo;actually&amp;rdquo; available. Their information and therfore value to me is unreliable.&lt;/p&gt;
&lt;p&gt;Examples would include sports scores or election results, in which the data is constantly and quickly changing. Freshness is not a factor for sites with static content that can remain the same over time, but if your site serves time-sensitive information, customers will expect that content to be kept current if they’re to consider the site reliable.&lt;/p&gt;
&lt;p&gt;The next one is particularly important when running pipelines or batch processing systems.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/exploring-the-throughput-aspect-of-reliability/&#34;&gt;Exploring the Throughput Aspect of Reliability&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Exploring the Latency Aspect of Reliability</title>
      <link>/post/exploring-the-latency-aspect-of-reliability/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/exploring-the-latency-aspect-of-reliability/</guid>
      <description>&lt;p&gt;Latency refers to the amount of delay between a request and a response.&lt;/p&gt;
&lt;h2 id=&#34;slow-is-the-new-down&#34;&gt;Slow is the New Down&lt;/h2&gt;
&lt;p&gt;Today’s users have high expectations.&lt;/p&gt;
&lt;p&gt;They demand fast performance and have little patience with a site or service that leaves them waiting. Reliability means the site is not only available, but available in a reasonable amount of time.&lt;/p&gt;
&lt;p&gt;Especially for external customer interactions – for example, when a customer visits your company’s website – it is critical that the service respond as quickly as possible. A slow website can drive your customers to your competitors.&lt;/p&gt;
&lt;p&gt;Azure is always 
&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/networking/azure-network-latency/?wt.mc_id=oncalllife-blog-jahand&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;monitoring latency from points all around the world&lt;/a&gt; to ensure speeds are within pre-defined limits.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://docs.microsoft.com/en-us/azure/networking/media/azure-network-latency/azure-network-latency.png&#34; alt=&#34;azure latency&#34;&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s say your service is available and fast, what else might users care about?&lt;/p&gt;
&lt;p&gt;If your service returns the wrong thing, it’s not reliable in the eyes of your customers.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s supposed to work as advertised. If it returns the wrong results, calculates a bad value, or generally doesn&amp;rsquo;t do what it&amp;rsquo;s supposed to do &amp;ldquo;correctly&amp;rdquo;, it&amp;rsquo;s not relable to the user. Who wants to use a service that can&amp;rsquo;t calculate for leap year, let alone manage your bank account?&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://azurespeedtest.azurewebsites.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Take a look at the latency of Azure datacenters from your current location&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://azurespeedtest.azurewebsites.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;latency-speed.gif&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://azurespeed.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Another one can be found here:&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://azurespeed.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;latency-test.gif&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/exploring-the-correctness-aspect-of-reliability/&#34;&gt;Exploring the Correctness Aspect of Reliability&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Exploring the Throughput Aspect of Reliability</title>
      <link>/post/exploring-the-throughput-aspect-of-reliability/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/exploring-the-throughput-aspect-of-reliability/</guid>
      <description>&lt;p&gt;Throughput is a measure of the rate at which something is processed, or the number of transactions that a website, application, or service successfully handles over a specified period of time.&lt;/p&gt;
&lt;p&gt;There are many factors that can affect throughput. Monitoring throughput can help you pinpoint potential problems that impact your users’ experience.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s been said already but worth repeating often; The aspects of reliability must be measured and examined from the customer&amp;rsquo;s perspective. Objective data from as close as you can get to what they are truly experiencing provides the truest depecition of what its like to use your service.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/measuring-reliability-from-the-customers-perspective/&#34;&gt;Measuring Reliability From the Customer&amp;rsquo;s Perspective&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Exploring Why Incidents are Unexpected</title>
      <link>/post/exploring-why-incidents-are-unexpected/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/exploring-why-incidents-are-unexpected/</guid>
      <description>&lt;p&gt;In other words &amp;hellip; unplanned Work&lt;/p&gt;
&lt;p&gt;Most of what we as engineers and technologists do is planned work.&lt;/p&gt;
&lt;p&gt;We spend a lot of time and effort understanding the work in front of us.&lt;/p&gt;
&lt;p&gt;We calculate story points.  We plan sprints. We have a pretty good idea on what we are supposed to be working on.&lt;/p&gt;
&lt;p&gt;So, when an incident occurs, it is disruptive. It is&amp;hellip; unplanned work.&lt;/p&gt;
&lt;p&gt;Often, we view this as a terrible thing, but in reality, incidents are actually “investments” in supplying the value we are trying to deliver to end users.&lt;/p&gt;
&lt;p&gt;We just need to change how we look at incidents.&lt;/p&gt;
&lt;p&gt;Next we&amp;rsquo;ll discuss the full lifecycle of an incident. From detection to analysis and everything in between. When we start to view incidents through a new lense and begin analyzing for opportunities to learn, you&amp;rsquo;ll begin to see your systems for what they are and why having a response plan in place makes sense.&lt;/p&gt;
&lt;h2 id=&#34;next-steps&#34;&gt;Next steps&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;/post/understanding-the-full-lifecycle-of-an-incident&#34;&gt;Understanding the Full Lifecycle of An Incident&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gathering Data For the Post-incident Review</title>
      <link>/post/gathering-data-for-the-post-incident-review/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/gathering-data-for-the-post-incident-review/</guid>
      <description>&lt;p&gt;The post-incident review process begins with the data. That includes the data in
the conversations related to the incident as well as the data collected by
monitoring systems. You can use Azure tools to collect, find, correlate, and
share the data in the course of conducting your learning review.&lt;/p&gt;
&lt;p&gt;In this unit, you’ll learn how to go about the construction of a shared,
accurate chronology that reflects the non-linear nature of an incident.
&lt;em&gt;Non-linear&lt;/em&gt; refers to the fact that incidents are almost never just a matter of
“this happened, and then that happened, then we noticed, then we did something,
and then we were done.”&lt;/p&gt;
&lt;p&gt;As an incident occurs, develops, and plays out, different people get involved.
They notice and try different things, and some work, some don’t. If multiple
people are working at the same time, all these things can be happening
simultaneously. Thus, it’s a bit more complicated than just a straight-line
description.&lt;/p&gt;
&lt;p&gt;To create a logical timeline, then, you need a starting point and the place to
start is with the relevant data.&lt;/p&gt;
&lt;h2 id=&#34;gather-the-data&#34;&gt;Gather the data&lt;/h2&gt;
&lt;p&gt;Before you can conduct a learning review, you first need to gather data.
Specifically, you need to collect as much of the conversation surrounding the
event as you can so you can use all of the crucial data contained in it. The
conversation among team members that happened during the outage or incident will
be one of your richest sources of information.&lt;/p&gt;
&lt;p&gt;You also should gather from your monitoring system and other places from which
the people involved in the incident drew context. What information were they
getting from your systems when the incident was going on?&lt;/p&gt;
&lt;p&gt;And finally, if possible, it would be helpful for you to get a better picture of
what changed just prior to and during the incident because changes are often
contributing factors when an incident occurs.&lt;/p&gt;
&lt;p&gt;The data-gathering step has three parts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Collect the conversation.&lt;/strong&gt; It’s important to have a place for people to
communicate and share what worked and what failed, what they’re hesitant to
try, what they’ve tried in the past. This conversation between the people as
they work through and solve the issue is your best source of learning.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Determine the context.&lt;/strong&gt; This is done primarily through monitoring. It
involves building a point in time to collect the data you have and correlate
it to the event now that you’re looking back at it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Find the changes.&lt;/strong&gt; You do this through activity and audits logs.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;data-collection-tools&#34;&gt;Data collection tools&lt;/h2&gt;
&lt;p&gt;There are tools that you can use to make these steps easier. Using Azure for
these systems can help make you better able to gather all this data quickly.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Azure DevOps&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You have data already captured in the Azure DevOps Boards that includes tracking
of incidents with information about who was on call, who was assigned to the
incident, and so forth. You can also use the Azure DevOps Wiki as a centralized
way to pull in some of the pieces of information about both the incident itself
and the conversation that happened during the incident.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Graph Explorer&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You can also use queries in Graph Explorer to programmatically find, export and
bring in the conversation that was collected inside the Teams channel that was
devoted to this specific incident.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dashboards, Application Insights and JSON&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You can create a dashboard in the Azure portal to correlate what the operators
were seeing to the incident itself. Application Insights give you graphs you can
add to the dashboard to visualize the information. This allows you to see your
tracked metrics side-by-side to better identify where and when the incident
happened. You can pull down the dashboard and its components using JSON, and,
for example, focus on a specific window of time and drill down to provide more
context for your investigation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Log Analytics and Audit Log&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You can use information here to discover changes and find the delta between
where you started and where you ended.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/reviewing-azures-monitoring-tools/&#34;&gt;Reviewing Azure Monitoring Tools&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Identifyig the Incident Remediation Phase</title>
      <link>/post/identifying-the-incident-remediation-phase/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/identifying-the-incident-remediation-phase/</guid>
      <description>&lt;p&gt;The remediation phase is the blurriest of them all. A big reason is that sometimes there&amp;rsquo;s no difference between what takes place during the response and an action intended to improve the situation (i.e. remediation step).&lt;/p&gt;
&lt;p&gt;Much of incident response is just trial and error, quite honestly. We quickly think through what to do, we do it, we hope for quick feedback, we examine if things improved, and we iterate.&lt;/p&gt;
&lt;p&gt;Because of this, measuring the remediation phase is a bit trickier.&lt;/p&gt;
&lt;p&gt;What we are looking for is to determine the distinction between when we have &lt;em&gt;identified the fix&lt;/em&gt; (or series of fixes) that will result in recovery of services and the end of the incident. What is the time between? Good or bad, it&amp;rsquo;s data to potentially discuss and discover revelations.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s not until the analysis phase that engineers can definitively determine the exact point along the incident timeline that everyone agrees the problem and solution were both understood.&lt;/p&gt;
&lt;p&gt;This underlines the importance of not only capturing the timeline of events, including conversations and actions taken but also analyzing it in retrospect with a diverse audience encouraged to ask questions. Questions that help radiate a broader and more informed knowledge base across an organization.&lt;/p&gt;
&lt;p&gt;One reason for measuring this way is to set aside the time between definitively knowing what will restore service and when services were actually back.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s say the payment process thing from before was pretty easy to determine. It probably took less than 5 minutes to know it was something with the backend talking to an API and that after someone followed a specific series of steps everything would be fine.&lt;/p&gt;
&lt;p&gt;However, the process to do this is not only complicated and requires administrative access, it&amp;rsquo;s not well documented, and what is documented is extremely dated.&lt;/p&gt;
&lt;p&gt;If this type of problem occurs again, we could shorten the total time of the incident, and therefore cost to the business, simply by making a few small adjustments.&lt;/p&gt;
&lt;p&gt;These types of opportunities begin to surface when we can ask questions like, &amp;ldquo;how long does the backup script take?&amp;quot;. &amp;ldquo;Was the documentation helpful?&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Just because you can figure out what the problem is in an acceptable amount of time, does not mean your system will recover as quickly as the business needs it to.&lt;/p&gt;
&lt;p&gt;Once service is restored and things return to normal it&amp;rsquo;s important to set aside time to reflect on what took place, discuss it openly, broadcast what has been learned, and prepare for the future.&lt;/p&gt;
&lt;p&gt;This takes us to our next phase of the incident lifecycle - analysis.&lt;/p&gt;
&lt;h2 id=&#34;next&#34;&gt;Next&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;/post/identifying-the-incident-analysis-phase/&#34;&gt;Identifying the Analysis Phase of an Incident&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Identifying the Communication Coordinator</title>
      <link>/post/identifying-the-communication-coordinator/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/identifying-the-communication-coordinator/</guid>
      <description>&lt;p&gt;The Communication Coordinator is meant to be the person working in conjunction with the incident commander to share more information beyond those who are in the firefight actively working to recover from the incident itself. That could be customers. It could be the sales and marketing teams. Maybe your customer support. There are many people within an organization who need to be made aware of what’s taking place and the status around how things are progressing. It&amp;rsquo;s always good to put someone in charge of managing that communication and making sure that other stakeholders are aware of what is happening and what’s being done.&lt;/p&gt;
&lt;h2 id=&#34;next-steps&#34;&gt;Next steps&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;/post/identifying-the-scribe/&#34;&gt;Identifying The Scribe&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Identifying the Incident Analysis Phase</title>
      <link>/post/identifying-the-incident-analysis-phase/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/identifying-the-incident-analysis-phase/</guid>
      <description>&lt;p&gt;The post-incident review is where the idea of incidents begin to shift from things that are feared and avoided to things that can provide valuable information to a team and business.&lt;/p&gt;
&lt;p&gt;Rarely will you find a business today that doesn&amp;rsquo;t heavily rely on digital services to earn and keep customers. There are going to be problems along the way. Not only that, but customers expect improvements, technology changes, competitors get smarter. There are fewer and fewer industries that can maintain the status quo and continue to exist, let alone prosper.&lt;/p&gt;
&lt;p&gt;The analysis phase allows for an honest and open retrospective discussion about what took place. We as an organization want to understand the realities of the scenario from an objective perspective.&lt;/p&gt;
&lt;p&gt;Exactly how you conduct the exercises will vary but the focus of the conversation is on what and how things happened rather than who and why.&lt;/p&gt;
&lt;p&gt;By identifying the incident timeline as well as the specific highlights, people can identify the beginning and end of each phase, including the conversations that took place.&lt;/p&gt;
&lt;p&gt;This helps isolate specific areas of improvements such as moving away from using email distribution lists as the default channel and method of delivering actionable alerts. In discussions about the detection phase, it seemed clear to everyone that the problem could have been solved sooner had they known about it sooner. Sometimes even small changes can have a huge impact on improving the overall time to recover.&lt;/p&gt;
&lt;p&gt;Regardless of how businesses choose to perform their post-incident review, they should take place no more than 36-48 hours after the incident has concluded.&lt;/p&gt;
&lt;p&gt;We are looking to collect as much objective data as well as testimonial from a diverse set of perspectives. It&amp;rsquo;s difficult to remember what took place in much detail after a couple of days.&lt;/p&gt;
&lt;p&gt;If possible, the exercise should be facilitated by someone that was not involved in the incident. Someone who can remove themselves from the timeline of events and perspectives of what took place. Objective data is often easier to obtain when someone else asks the questions and encourages an honest conversation.&lt;/p&gt;
&lt;p&gt;The point of a post-incident review isn&amp;rsquo;t to end up with a document or artifact that the meeting took place. It&amp;rsquo;s not an exploration on what definitively caused the issue in the first place.&lt;/p&gt;
&lt;p&gt;Our systems are always changing. That&amp;rsquo;s just where we are today. Businesses that are providing some kind of service to their customers where technology is involved are required to make constant changes. It doesn&amp;rsquo;t matter if it&amp;rsquo;s in the cloud, data center, or closet, servers need improvements and replacements. Operating systems need patches and upgrades. Applications need to be updated and restarted. Databases and logs are changing and growing. Networks are coming and going. There&amp;rsquo;s a lot going on and it&amp;rsquo;s often difficult to put our thumb on exactly what&amp;rsquo;s causing what.&lt;/p&gt;
&lt;p&gt;The good news is, it&amp;rsquo;s ok.&lt;/p&gt;
&lt;p&gt;Part of the advantage of examining incidents in phases means that regardless to the problems we experience in the future, we know that we can detect a problem, get the right people involved, and recover services faster than before. We are prepared for the infinite world of possibilities we like to call the &amp;ldquo;unknown unknowns&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;This takes us to our final phase of the incident lifecycle, readiness.&lt;/p&gt;
&lt;h2 id=&#34;next-steps&#34;&gt;Next Steps&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;/post/identifying-the-incident-readiness-phase&#34;&gt;Identifying the Readiness Phase of an Incident&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Identifying the Incident Commander</title>
      <link>/post/identifying-the-incident-commander/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/identifying-the-incident-commander/</guid>
      <description>&lt;p&gt;Another key role to identify, in many cases, is the incident commander. An incident commander can be extremely helpful when you have got a large-scale outage that effects a lot of different components or requires coordination across many teams and different systems. They are great for making sure that engineers stay focused and they are working on their own remediation efforts&amp;hellip; Ensuring people are not stepping on each other or undoing each other&amp;rsquo;s work.  It is good to have a central person who can track what is going on and who is doing what.&lt;/p&gt;
&lt;h2 id=&#34;next-steps&#34;&gt;Next steps&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;/post/identifying-the-communication-coordinator/&#34;&gt;Identifying the Communication Coordinator&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Identifying the Incident Detection Phase</title>
      <link>/post/identifying-the-incident-detection-phase/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/identifying-the-incident-detection-phase/</guid>
      <description>&lt;p&gt;The methods used to determine when we have a problem have changed over the years.&lt;/p&gt;
&lt;p&gt;Alerting a person to a spike in CPU usage isn&amp;rsquo;t as valuable these days. Especially those in the process of adopting the cloud. Instead, we want to know when our customer is experiencing a problem while using our system.&lt;/p&gt;
&lt;p&gt;The problems will vary but the methods used to determine when a human needs to get involved have evolved.&lt;/p&gt;
&lt;p&gt;By monitoring systems in a way that matches the customer&amp;rsquo;s perspective we can see when they experience a problem rather than we &lt;em&gt;think&lt;/em&gt; we experienced a problem.&lt;/p&gt;
&lt;p&gt;If the customer is experiencing an issue, that&amp;rsquo;s far more important to the business than any spike in CPU usage.&lt;/p&gt;
&lt;p&gt;In today&amp;rsquo;s connected world, no matter how complex or simple a system appears to be there is much more that goes in to what a user experiences.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s entirely possible that all systems appear healthy and no alarms are going off when in reality users aren&amp;rsquo;t able to complete a shopping purchase due to a third party payment processor. No amount of monitoring for memory or network performance would have tipped off engineers or leadership to this business impacting problem.&lt;/p&gt;
&lt;p&gt;Every system is different and while there may be legitimate reasons to set up alerts for problems at the component level. However, by and large if we are planning to get engineers involved (especially outside of office hours) then we need to make sure the problem is real, it&amp;rsquo;s impacting the business, and it requires human intervention immediately.&lt;/p&gt;
&lt;p&gt;If an alert isn&amp;rsquo;t actionable - meaning it requires a person or group of people to respond and investigate right away then it&amp;rsquo;s not an incident.&lt;/p&gt;
&lt;p&gt;If we can measure some minor details about when amd how we detect problems in the first place then we can look for opportunities to improve.&lt;/p&gt;
&lt;p&gt;In conversations about what took place with the payment processor incident it is reasonable to ask &amp;ldquo;how could we have detected this sooner?&amp;quot;.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;How could we have detected this &amp;hellip; at all?&amp;rdquo; may be a better question.&lt;/p&gt;
&lt;h2 id=&#34;next-steps&#34;&gt;Next steps&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;/post/identifying-the-incident-response-phase/&#34;&gt;Identifying the Response Phase of an Incident&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Identifying the Incident Readiness Phase</title>
      <link>/post/identifying-the-incident-readiness-phase/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/identifying-the-incident-readiness-phase/</guid>
      <description>&lt;p&gt;During and after a post-incident review many ideas will surface around how to improve not only various aspects of each phase of the lifecycle but also how the team can improve in other areas. Communication for example.&lt;/p&gt;
&lt;p&gt;During the review, engineers might have pointed out that there were long gaps in the conversation timeline where nobody said anything. It&amp;rsquo;s helpful to be verbose in what engineers are doing, thinking, even feeling. If someone isn&amp;rsquo;t completely comfortable following the steps from documentation, we should address that. Who else on the team carries fears about performing actions on the system during an incident? We want our team to be confident and ready.&lt;/p&gt;
&lt;p&gt;So, what did we learn that helps improve that readiness?&lt;/p&gt;
&lt;p&gt;Action items aren&amp;rsquo;t really the point of a post-incident reivew but inevitably, creative ideas will emerge. Some engineering efforts will make sense to schedule and implement as a result of the conversations. Adding telemetry to help keep a better eye on the credit card processing system, for example.&lt;/p&gt;
&lt;p&gt;Product and engineering teams should work together to prioritize and schedule work for those enhancements. Tradeoffs will be made since the uptime of a new feature is just as important as the feature itself. Ultimately, what&amp;rsquo;s best for the business is what leadership will have to wrestle with.&lt;/p&gt;
&lt;p&gt;The bigger win that helps with our readiness efforts is that we have measurements by which we can set goals around.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/understanding-the-foundations-of-incident-response/&#34;&gt;Understanding the Foundations of Incident Response&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Identifying the Primary Responder</title>
      <link>/post/identifying-the-primary-responder/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/identifying-the-primary-responder/</guid>
      <description>&lt;p&gt;The first role we need to talk about is the “Primary Responder” – the Primary “On-call” engineer.&lt;/p&gt;
&lt;p&gt;This person is expected to acknowledge their awareness of an incident once the alert has been received.&lt;/p&gt;
&lt;h2 id=&#34;next-steps&#34;&gt;Next steps&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;/post/identifying-the-secondary-responder/&#34;&gt;Identifying the Secondary Responder&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Identifying the Response Phase of an Incident</title>
      <link>/post/identifying-the-incident-response-phase/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/identifying-the-incident-response-phase/</guid>
      <description>&lt;p&gt;Once our detection efforts have been configured to send actionable alerts to the people who build the systems, we need to make sure they are sending those alerts to the &lt;em&gt;right&lt;/em&gt; people.&lt;/p&gt;
&lt;h2 id=&#34;right-people&#34;&gt;Right People&lt;/h2&gt;
&lt;p&gt;How do you know who the right people are? In most cases it is situational. A few things that can be done to help establish some formatlity and standard around responding to incidents is through the use of roles, rosters, and rotations. We&amp;rsquo;ll go more in depth on what each of those are soon.&lt;/p&gt;
&lt;h2 id=&#34;tooling&#34;&gt;Tooling&lt;/h2&gt;
&lt;p&gt;The right person for the job needs the right tools for the job. If someone is responding to an issue they need to get busy immediately. Making sure the right monitoring, communications, access, and documentation is provided is also important. People should be familiar with the tooling and know how and where to find additional resources to help diagnose, theorize, and triage.&lt;/p&gt;
&lt;h3 id=&#34;diagnose&#34;&gt;Diagnose&lt;/h3&gt;
&lt;p&gt;Everyone experiences problems. Sometimes routinely throughout the day in fact. When something doesn&amp;rsquo;t go as expected or breaks entirely our impulse is to fix it. In order to do so we must first have a look at what&amp;rsquo;s currently observable. What is the status? Who and what is impacted? What hints or clues are there? What information do we have to work with?&lt;/p&gt;
&lt;p&gt;What do we know &lt;em&gt;right now?&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;theorize&#34;&gt;Theorize&lt;/h3&gt;
&lt;p&gt;Once information has been obtained, we begin to theorize next best steps.
What action can we take to minimize or stop the impact? What are the repurcussions of that action? Will something else go wrong? If we take one action, what result do we expect? In very brief moments we are creatively thinking through as many possible scenarios to restore service as we can. And then stack ranking them based on our own calculations on the probability of success.&lt;/p&gt;
&lt;h3 id=&#34;triage&#34;&gt;Triage&lt;/h3&gt;
&lt;p&gt;At some point we all need help. That could be access to an admin account, theories from subject matter experts, someone to amplify updates to a broader audience. Rarely are incidents viewed as a success if only a single person was involved.&lt;/p&gt;
&lt;p&gt;Regardless of the size of your response team, by isolating it as a phase in the incident lifecycle, we can examine this section of the timeline for improvements on how we coordinate our response. If it took an excessive amount of time for the engineering team to correct the payment processor problem simply because it took too long to find the right person, with the right tool, and with the appropriate level of access then there are some clear opportunities for improvement right there.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/identifying-the-incident-remediation-phase/&#34;&gt;Identifying the Remediation Phase of an Incident&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Identifying The Scribe</title>
      <link>/post/identifying-the-scribe/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/identifying-the-scribe/</guid>
      <description>&lt;p&gt;The scribe’s role is to document the conversation in as much detail as possible. Teams commonly use phone bridges, conference calls, or video chat to get everyone together and try to understand what is going on, which can certainly help create space for the conversation. However, it is difficult for us to go through and understand in detail what the engineers were saying and doing unless it is transcribed. As a result, a scribe is that person who can help us document as much as possible to review later. What were people saying, doing, feeling, and even experiencing?  It is all data to be analyzed – but only if we capture it.&lt;/p&gt;
&lt;h2 id=&#34;next-steps&#34;&gt;Next steps&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;/post/identifying-the-subject-matter-experts/&#34;&gt;Identifying the Subject Matter Experts&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Identifying the Secondary Responder</title>
      <link>/post/identifying-the-secondary-responder/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/identifying-the-secondary-responder/</guid>
      <description>&lt;p&gt;Then we have the secondary responder – who is there as back up -Another engineer who can step in if the primary responder is unavailable or unreachable. Or if they just need another pair of eyes.&lt;/p&gt;
&lt;h2 id=&#34;next-steps&#34;&gt;Next steps&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;/post/identifying-the-incident-commander/&#34;&gt;Identifying The Incident Commander&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Identifying the Subject Matter Experts</title>
      <link>/post/identifying-the-subject-matter-experts/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/identifying-the-subject-matter-experts/</guid>
      <description>&lt;p&gt;It’s quite common within on-call rosters to identify subject matter experts, so that early responders know who to escalate too quickly. These people should not be on call all the time, of course, but we do want to be able to identify who is our database expert. Who is our front-end expert? Who are the people that we can reach out to if our primary and secondary responders are not able to diagnose and resolve the issue themselves?&lt;/p&gt;
&lt;h2 id=&#34;next-steps&#34;&gt;Next steps&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;/post/establishing-oncall-rosters&#34;&gt;Establishing On-call Rosters&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Improving the Remediation of Incidents</title>
      <link>/post/improving-the-remediation-of-incidents/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/improving-the-remediation-of-incidents/</guid>
      <description>&lt;p&gt;Although thinking of incidents in terms of phases allows for us to shorten each in their own unique ways, responding to and remediating an incident often begin to blur. Especially when actions to mitigate or improve the situation, have the opposite result.&lt;/p&gt;
&lt;p&gt;Now that we’ve covered the foundations of building a good incident response plan, let&amp;rsquo;s talk about remediation efforts and how 
&lt;a href=&#34;/post/supplying-context-and-guidance/&#34;&gt;Supplying Context &amp;amp; Guidance&lt;/a&gt; to on-call engineers rather than step by step procedures can dramatically help reduce the impact of an incident.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Managing Tasks from Group Chat - ChatOps</title>
      <link>/post/managing-tasks-from-group-chat-chatops/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/managing-tasks-from-group-chat-chatops/</guid>
      <description>&lt;p&gt;Chatops is the use of tools from within a group chat tool.&lt;/p&gt;
&lt;p&gt;One great example of Chatops is when engineers can update important information regarding an incident to a broader audience, possibly even to affected users.&lt;/p&gt;
&lt;p&gt;Allowing people to quickly update stakeholders by typing a few quick commands from within the same environment as the related conversations has a number of clear benefits.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s take a look at 
&lt;a href=&#34;/post/building_a_serverless_status_page_solution/&#34;&gt;Building a Serverless Status Page Solution with Azure Functions, blob storage, and Microsoft Teams&lt;/a&gt; to illustrate.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Measuring Incident Response</title>
      <link>/post/measuring-incident-response/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/measuring-incident-response/</guid>
      <description>&lt;p&gt;Are you familiar with the acronym &lt;strong&gt;TTR&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;It is known as the &lt;strong&gt;“Time to Recover”&lt;/strong&gt; or often referred to as the “time to remediate” or “time to restore.”&lt;/p&gt;
&lt;p&gt;In other words, the total time that it takes for engineers to bring services back online… with regards to the value provided to end users and customers. It is the total duration of time for the incident?&lt;/p&gt;
&lt;p&gt;The time to recover will vary from incident to incident and the circumstances around what contributed to the problem will rarely repeat. Because of this, measuring the TTR in aggregate can be a misleading metric.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Mean Time to Recover&lt;/strong&gt; does not reflect a valuable performance metric on either the uptime of your systems or how well (or poorly) teams can respond to and remediate service disruptions.&lt;/p&gt;
&lt;p&gt;While not a perfect metric, it is one in which organizations can begin to measure how teams are performing when it comes to responding to incidents individually. We will need to examine the entire incident timeline to gain a broader understanding of what took place and where improvements can be made.&lt;/p&gt;
&lt;p&gt;Most anyone who works with technology will agree that the complete prevention of problems is not very realistic.&lt;/p&gt;
&lt;p&gt;Instead, we must do better at knowing when something is wrong and being able to respond to it in an effective way.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now look at how 
&lt;a href=&#34;/post/creating-custom-incident-reports-and-charts-in-azure-boards/&#34;&gt;Creating Custom Incident Reports and Charts in Azure Boards&lt;/a&gt; will allow us to keep a pulse on how our incident management response is going.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Measuring Reliability From the Customer&#39;s Perspective</title>
      <link>/post/measuring-reliability-from-the-customers-perspective/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/measuring-reliability-from-the-customers-perspective/</guid>
      <description>&lt;p&gt;These eight components cover a big part of reliability. Not all of the factors will apply in every situation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;reliability-customer-perspective.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;When considering these factors, the most important point to remember is (again) that reliability has to be measured from the customer’s perspective, not the component perspective. Knowing our CPU&amp;rsquo;s running at over 90% utilization has no correlation to what our users are experiencing, let alone doing.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/understanding-measurements-of-reliability/&#34;&gt;Understanding Measurements of Reliability&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Monitoring For Operational Awareness</title>
      <link>/post/monitoring-for-operational-awareness/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/monitoring-for-operational-awareness/</guid>
      <description>&lt;p&gt;Sustainably achieving an appropriate level of reliability first requires that you know what’s going on with your systems and services. You find that out by monitoring. Before you can effectively monitor for reliability, however, you must have a reasonable level of operational awareness. This means you need to
understand how systems in production are functioning in order to work toward reliability of those systems.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/monitoring-for-operational-awareness/operational-awareness.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Today’s production environments and the paths by which we deploy systems and applications are complex. Thus, it is not uncommon to have to do a bit of discovery to obtain an operational awareness baseline.&lt;/p&gt;
&lt;p&gt;Some questions to ask include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;What exactly is running in production?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Given a specific application, what are its component parts?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Which of those parts communicate with which other parts?&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are also some more complex factors that you’ll want to examine to give you a better understanding of your systems and services and how to make them more reliable.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;How did the app or service perform in the past?&lt;/strong&gt; It’s true that “past
performance is no guarantee of future results.” However, knowledge about
past performance can be useful in calibrating expectations, and awareness of
past outages can provide you with a sense of potential failure modes that
you should incorporate into your thought processes around reliability.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;What is the socio-technical context?&lt;/strong&gt; In other words, who owns or cares
about the service or app? How did it get deployed? Information about the
stakeholders, as well as knowing, for example, whether it was deployed by
hand or via an automated process can have many ramifications when we begin
to make updates to improve reliability.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The answers to all these questions will help you to build a baseline around “normal” behavior.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/monitoring-for-reliability/&#34;&gt;Monitoring For Reliability&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Monitoring For Reliability</title>
      <link>/post/monitoring-for-reliability/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/monitoring-for-reliability/</guid>
      <description>&lt;p&gt;The world today is extremely connected.&lt;/p&gt;
&lt;p&gt;Digital services have become so embedded in our daily lives that when they become unavailable, it often has an adverse effect on our own livelihoods. Like electricity and running water, we don&amp;rsquo;t realize our reliance on it until it&amp;rsquo;s suddenly unavailable.&lt;/p&gt;
&lt;h2 id=&#34;monitoring&#34;&gt;Monitoring&lt;/h2&gt;
&lt;p&gt;Monitoring is a way of collecting information about what is going on in your systems so you can improve things as well as make objectively informed decisions. 
&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/azure-monitor/overview/?wt_mc_id=oncalllife-blog-jahand&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Azure Monitor&lt;/a&gt; is an example of tooling that allows this type of collection and analysis.&lt;/p&gt;
&lt;p&gt;Many aspects of a system can be monitored. From the component level (CPU, Memory) to business impact and everything in between. We might gauge performance, examine security, or (in our case) evaluate and improve reliability.&lt;/p&gt;
&lt;h2 id=&#34;reliability&#34;&gt;Reliability&lt;/h2&gt;
&lt;p&gt;When monitoring for reliability, we are trying to proactively manage not only the availability of a service, but many aspects of reliability.&lt;/p&gt;
&lt;p&gt;Depending on the service and the expectations of the end user, some aspects of reliability may be more valuable than others.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;strong&gt;Exercise:&lt;/strong&gt; &lt;br /&gt; Discuss and document perceived expectations users have regarding the reliability of your service?
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;To determine what to monitor, continue with  
&lt;a href=&#34;/post/examining-all-aspects-of-reliability/&#34;&gt;Examinging All Aspects of Reliability&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Reviewing Azure&#39;s Monitoring Tools</title>
      <link>/post/reviewing-azures-monitoring-tools/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/reviewing-azures-monitoring-tools/</guid>
      <description>&lt;p&gt;Azure Monitor includes a number of features and tools you can use to monitor and
measure reliability, using data from many different sources.&lt;/p&gt;
&lt;p&gt;This article’s focus is to help you keep your “eyes on the prize” – the goal of
improving reliability in your organization. You should now understand what
reliability is and why it’s important. You know that operational awareness is an
essential precursor to monitoring for reliability, and that you need to
establish a baseline of “normal” behavior as a first step.&lt;/p&gt;
&lt;p&gt;Now you’ll look at the practical question of how to go about doing that. As with
all complex tasks, the right tools can help you get the job done more quickly
and easily. In this unit, you’ll get a quick introduction to Azure Monitor,
Application Insights, and Log Analytics.&lt;/p&gt;
&lt;h2 id=&#34;azure-monitor&#34;&gt;Azure Monitor&lt;/h2&gt;
&lt;p&gt;Azure Monitor is a comprehensive platform for monitoring Azure resources to gain
insights into your applications, infrastructure, and network. In this module,
the focus will be on the Azure Monitor tools that you can use to monitor and
measure reliability.&lt;/p&gt;
&lt;p&gt;A look at Azure Monitor starts with the data that comes into the system. Azure
Monitor takes in data from a number of different sources. These include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Data from applications&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Data from the various operating systems running in Azure&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Information fed from Azure resources, subscriptions, and tenants&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Custom data of any sort and from any source&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are many things we can do with that information: analyze it, visualize it,
integrate it, and respond to it when things go wrong.&lt;/p&gt;
&lt;h2 id=&#34;data-types&#34;&gt;Data types&lt;/h2&gt;
&lt;p&gt;The data that comes into Azure Monitor can be divided into two types:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Metrics:&lt;/strong&gt; small numerical pieces of information from counters, gauges,
and so forth that are collected on a regular basis&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Log data:&lt;/strong&gt; information gathered from many different logs such as Windows
event logs, Linux syslog, agents running on virtual machines, custom logs,
telemetry from Application Insights, and more.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this module, we will be focusing primarily on log data.&lt;/p&gt;
&lt;p&gt;You can create, import, and export dashboards, using JSON files, to help you
better visualize the data and provide you with operational awareness about all
aspects of your Azure subscription, including reliability information. You can
also set access controls on the dashboards and share them with others.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/examining-common-traps/&#34;&gt;Examining Common Traps&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Running a Post-incident Review</title>
      <link>/post/running-a-post-incident-review/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/running-a-post-incident-review/</guid>
      <description>&lt;p&gt;We want to develop a sense making approach to the present situation in order to act upon it better in the future&amp;hellip; in the moment.&lt;/p&gt;
&lt;p&gt;There are different ways to conduct a useful post-incident review, but there are
some common practices that can make the process easier and more effective. If
you run a facilitated review meeting, keep your review and planning meetings
separate, ask better questions, and identify how things went right, you can
learn more and use what you learn to improve the reliability of your systems,
services, and processes.&lt;/p&gt;
&lt;p&gt;Now you’re aware of some of the common pitfalls that can sabotage your
post-incident review, and you know what &lt;em&gt;not&lt;/em&gt; to do. The next logical question
is “what should you do instead?”&lt;/p&gt;
&lt;p&gt;In this unit, you’ll learn about four helpful practices that can make your
response team better and improve the post-incident analysis process.&lt;/p&gt;
&lt;h2 id=&#34;practice-1-run-a-facilitated-post-incident-review&#34;&gt;Practice 1: Run a facilitated post-incident review&lt;/h2&gt;
&lt;p&gt;You already know that a post-incident review is not a document or report, so it
follows that having one person write up a “postmortem” of what happened doesn’t
make for a helpful post-incident review. No matter how knowledgeable or how
deeply involved in the incident that person might be, not much will be learned
if everything comes from a single viewpoint.&lt;/p&gt;
&lt;p&gt;Getting those who were involved in the incident together at the same time is the
first step. But there is something else you can do to increase the useful
learning that comes out of the review even more.&lt;/p&gt;
&lt;p&gt;A &lt;em&gt;facilitator&lt;/em&gt; is someone or something that makes an action or process easier.
To make the post-incident review process both easier and more effective, you
should plan to have a post-incident review meeting with a &lt;em&gt;neutral facilitator&lt;/em&gt;
– someone who was not involved in the response to this incident.&lt;/p&gt;
&lt;p&gt;Everyone will learn more if the facilitator cannot be seen to have any
preconceived ideas or a personal agenda in telling the story of the incident.&lt;/p&gt;
&lt;p&gt;The exact format for the meeting will depend on your team, scheduling, and the
nature of the incident, but here are some more basic guidelines:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Meetings, not marathons:&lt;/strong&gt; the meetings don’t have to be long. Generally,
sixty to ninety minutes is the maximum length of time most people can fully
concentrate and participate effectively, so limit the meeting to not more
than an hour and a half.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Pre&lt;/strong&gt;-&lt;strong&gt;meeting prep:&lt;/strong&gt; to make better use of the meeting time, it can be
helpful for the facilitator to prepare by conducting one-to-one interviews
with some of the members of the response team to get an overview of the
incident and ideas about which topics to talk about in the meeting.
Individual interviews are also appropriate with certain personalities, when
presenting in front of the room is a cause for discomfort.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Not required for every incident&lt;/strong&gt;: this is a learning process, and you’re
“learning to learn,” so start small. You don’t have to do this for every
incident. You can pick and choose. You might want to start with smaller
incidents or start with a review meeting only once per month.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The post-incident review meeting is an opportunity to find out what went wrong,
what was done right, and how failures can be handled better in the future. The
ultimate goal is to improve reliability.&lt;/p&gt;
&lt;h2 id=&#34;practice-2-ask-better-questions&#34;&gt;Practice 2: Ask better questions&lt;/h2&gt;
&lt;p&gt;You already know that language matters, and in the post-incident review, this
applies especially to the questions you ask. Objective questions will usually
elicit more useful answers.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In particular, it’s better to ask people “how” or “what” instead of “why.”&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;When people are asked to explain “why” they did something or “why” something
happened, it tends to put them on the defensive. Beginning a question with “why”
often comes across as a judgement, criticism, or accusation. It forces people to
justify their actions, and people don’t always know why they did something, or
why something happened as a result of their actions.&lt;/p&gt;
&lt;p&gt;This doesn’t mean you can’t explore the reasons the incident occurred or the
reasoning a person used to decide what to do in response to it. It just means
you should pay attention to how you word those questions:&lt;/p&gt;
&lt;p&gt;Don’t ask “why did you do that?”&lt;/p&gt;
&lt;p&gt;Instead, ask “&lt;em&gt;what factored into your decision to make that change?&lt;/em&gt;”&lt;/p&gt;
&lt;p&gt;Don’t ask “why wasn’t this caught in canary?”&lt;/p&gt;
&lt;p&gt;Instead, ask “&lt;em&gt;how effective is canary at catching this sort of issue,
usually?&lt;/em&gt;”&lt;/p&gt;
&lt;p&gt;Remember that the post-incident review is about learning. Each participant in
the incident is likely to have had a slightly different view on events. You’ll
learn more if you ask questions that expose these multiple views and
interpretations.&lt;/p&gt;
&lt;p&gt;Don’t get tunnel vision. Sometimes focusing too narrowly on the incident in
question will cause you to miss valuable information. You will often learn as
much by asking about how work “normally” happens as you will by asking about the
specific incident.&lt;/p&gt;
&lt;p&gt;To learn more about how to ask better questions, check out this resource:&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://extfiles.etsy.com/DebriefingFacilitationGuide.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Etsy Debriefing Facilitation
Guide&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;practice-3-ask-how-things-went-right&#34;&gt;Practice 3: Ask how things went right&lt;/h2&gt;
&lt;p&gt;When you think about learning from failure, you may forget that even within a
major outage or other incident, in addition to the things that go wrong there
are also things that go right. Far from our view of incidents as one-offs or
products of extreme conditions, in most complex systems things go wrong for many
of the same reasons they go right.&lt;/p&gt;
&lt;p&gt;It’s human nature to focus your questions on the negative side of the equation.
However, also asking about how things went right will provide you with insights
that you would not have gotten otherwise.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Don’t just ask how the outage happened.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Ask how your team recovered the systems.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;You want to know what insights, tools, skills, and people helped in the recovery
effort. These are things you want to be able to reproduce, so this information
will be valuable in planning what to do going forward.&lt;/p&gt;
&lt;p&gt;In this context, you want to ask how people came to know what they knew and on
what basis they made the decisions they made. Was there a critical moment when
someone shared a piece of information that helped unlock the puzzle of what was
happening? How did they know to do that? Where did the information come from?&lt;/p&gt;
&lt;p&gt;Look for themes and patterns. Finally, as part of recognizing what went right,
ask &lt;em&gt;What do you know now that you didn’t know previously?&lt;/em&gt; If learning resulted
from the incident and from the incident response and review processes, that’s
another thing that went right.&lt;/p&gt;
&lt;h2 id=&#34;practice-4-keep-review-and-planning-meetings-separate&#34;&gt;Practice 4: Keep review and planning meetings separate&lt;/h2&gt;
&lt;p&gt;After you’ve resolved the immediate incident, you’re naturally going to want to
talk about repair items and future mitigation – and you should – but these
topics should &lt;em&gt;not&lt;/em&gt; be a part of your post-incident review meeting. Your
post-incident review meeting has a purpose and allowing the discussion of repair
items into that meeting will distract from that purpose.&lt;/p&gt;
&lt;p&gt;The best plan is to discuss repair items and planning issues in a separate
meeting a day or two after your post-incident review. You might want to do this
with a smaller group. Both of the meetings will be more productive as a result
of this separation.&lt;/p&gt;
&lt;p&gt;It will help you keep the focus of the post-incident review where it belongs, on
what actually happened. In addition, allowing a day or two of “soak time” will
result in better “synthesis,” giving your subconscious time to work on the
issues. This will help you to identify the most “energy-efficient” repair items
– those that require minimum energy for maximum impact.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/gathering-data-for-the-post-incident-review/&#34;&gt;Gathering Data for the Post-incident Review&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sending Actionable Alerts</title>
      <link>/post/sending-actionable-alerts/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/sending-actionable-alerts/</guid>
      <description>&lt;p&gt;Alerts play an important role in your reliability monitoring strategy, but in
order to be helpful, they must be properly constructed for situations that
warrant immediate human attention, and they should be devised with simplicity,
scope, and context in mind.&lt;/p&gt;
&lt;p&gt;Preferences on how alerts are delivered can be designed using 
&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/azure-monitor/platform/action-groups/?wt.mc_id=oncalllife-blog-jahand&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Action Groups&lt;/a&gt; in Azure.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://docs.microsoft.com/en-us/azure/azure-monitor/platform/media/action-groups/action-group-define.png&#34; alt=&#34;Action Group&#34;&gt;&lt;/p&gt;
&lt;p&gt;You have learned how to monitor and interact with indicators of
the reliability of your systems and create reliability goals, but there is also
an important way by which reliability interacts with you. That’s through Azure
Monitor’s log alerts feature.&lt;/p&gt;
&lt;p&gt;It’s easy to create log alerts using Azure Monitor where the signal is a log
query in Log Analytics or Application Insights. However, there is a pitfall that
you’ll want to avoid, to prevent derailing all the effort you have put into
bringing SLIs and SLOs into your organization.&lt;/p&gt;
&lt;p&gt;To understand this potential pitfall, review the definition of SRE:&lt;/p&gt;
&lt;p&gt;“Site Reliability Engineering is an engineering discipline devoted to helping
organizations &lt;strong&gt;sustainably&lt;/strong&gt; achieve the appropriate level of reliability in
their systems, services, and products.”&lt;/p&gt;
&lt;p&gt;Alerts are designed to notify you when there is a problem with your systems.
However, when alerts are improperly configured, this can undermine your goal of
sustainability. Log alert rules are stateless; they work only on the logic that
you build into the query and they send an alert whenever the alert condition is
“true.” Thus, it’s important to put some thoughts into constructing your alerts.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/defining-alerts&#34;&gt;Defining Alerts&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Supplying Context and Guidance</title>
      <link>/post/supplying-context-and-guidance/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/supplying-context-and-guidance/</guid>
      <description>&lt;p&gt;During an active incident, it&amp;rsquo;s often difficult to recall information such as system names, IP addresses, admin logins, location of logs, and more. Remembering where to find dasbhoards, metrics, and reports that could be helpful is often difficult.&lt;/p&gt;
&lt;p&gt;To build a proactive response plan to managing incidents it&amp;rsquo;s important to provide some useful context and guidance for the first responders to begin investigating, triaging, and escalating.&lt;/p&gt;
&lt;p&gt;Where should the first responder start? Is there a dashboard that provides a good representation of the health of the system from the customer&amp;rsquo;s perspective? Do responders know how to access it? Does it require a login? Can it be viewed from a mobile device, remotely? These are all considerations for making it easier for on-call responders to begin restoring service as quickly as possible.&lt;/p&gt;
&lt;p&gt;Typically, first responders will need the assistance of others with additional expertise and experience. Or maybe just a second pair of eyes. Understanding how on-call engineers should escalate incidents, helps cut out any delay should exacerbated engineers need to reach out for assistance.&lt;/p&gt;
&lt;p&gt;When discussing incident response tacticcs, ask yourselves what metrics, tools, links or general resources might be helpful in those early moments?&lt;/p&gt;
&lt;p&gt;Once we&amp;rsquo;ve answered that question we can begin building resources in a non-prescriptive way. Objective data and tools that can be provided to help assist in the efforts to fix the problems.&lt;/p&gt;
&lt;p&gt;Often &amp;ldquo;Runbooks&amp;rdquo; (or playbooks) serve this purpose, automating some of the remediation steps that are known to help recover from the problem.&lt;/p&gt;
&lt;p&gt;Automation can be helpful in many ways during an incident. However, tools that provide context and guidance to allow engineers to make their own decisions towards next actions are meant to be more guides than anything.&lt;/p&gt;
&lt;p&gt;Next, let&amp;rsquo;s look at 
&lt;a href=&#34;/post/building-troubleshooting-guides-in-azure-monitor/&#34;&gt;Building Troubleshooting Guides in Azure Monitor&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tracking How We Knew About a Problem</title>
      <link>/post/tracking-how-we-knew-about-a-problem/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/tracking-how-we-knew-about-a-problem/</guid>
      <description>&lt;p&gt;Did our monitoring systems tell us, or did a customer inform us?&lt;/p&gt;
&lt;p&gt;Capturing whether problems were detected through telemetry or another method, means we can easily identify gaps in our monitoring tools and practices.&lt;/p&gt;
&lt;p&gt;It also helps early responders to know where the problem is and what is affected. By communicating to the engineer where the problem was first detected, we provide valuable context in their early triaging efforts.&lt;/p&gt;
&lt;p&gt;Incidents are often chaotic and stressful. It&amp;rsquo;s important to stay informed, especially awareness of a problem. Let&amp;rsquo;s talk now about 
&lt;a href=&#34;/post/tracking-who-knows-about-a-problem/&#34;&gt;Tracking &lt;strong&gt;Who&lt;/strong&gt; Knows About a Problem&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tracking Impact of a Problem</title>
      <link>/post/tracking-impact-of-a-problem/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/tracking-impact-of-a-problem/</guid>
      <description>&lt;p&gt;&lt;strong&gt;How Bad is It?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We may not have any notion of severity or impact and there is no place for us to find out how bad the problem really is, and who is affected. These are tough questions to answer if nothing is tracked.&lt;/p&gt;
&lt;p&gt;There are many people who care about how things are going during an incident. We call them stakeholders. From CEO&amp;rsquo;s to sales teams, many more people outside of the engineering team want, need, and deserve to know &amp;ldquo;How bad is it?&amp;quot;.&lt;/p&gt;
&lt;p&gt;Regardless to if a severity level is assigned, it&amp;rsquo;s important to capture what impact the customer is experiencing. What additional failures might we expect? Being transparent around what is known to be impacted from what is known about the problem allows and empowers others to take action to mitigate or minimize downstream affects.&lt;/p&gt;
&lt;p&gt;There are great tools out there for tracking the details of incidents. Pagerduty, VictorOps and others have services specifically designed for this.&lt;/p&gt;
&lt;p&gt;Under the surface, they are similar to ticketing systems or project tracking tools such as Jira and Azure Boards.&lt;/p&gt;
&lt;p&gt;Next, I&amp;rsquo;ll show you how 
&lt;a href=&#34;/post/creating-an-incident-tracking-tool-with-azure-boards/&#34;&gt;Creating an incident tracking tool with Azure Boards&lt;/a&gt; can be done in just a few steps for &lt;strong&gt;free&lt;/strong&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tracking Incident Details</title>
      <link>/post/tracking-incident-details/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/tracking-incident-details/</guid>
      <description>&lt;p&gt;To recover from incidents effectively, it&amp;rsquo;s important to communicate and collaborate effectively. In order to share relevant details of what is known about an incident, who is addressing it, and more, it&amp;rsquo;s important to have a method of tracking incident details.&lt;/p&gt;
&lt;p&gt;Basic details such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When did know about the problem?&lt;/li&gt;
&lt;li&gt;How did we find out about the problem?&lt;/li&gt;
&lt;li&gt;Who is awareness of the problem?&lt;/li&gt;
&lt;li&gt;What is being done?&lt;/li&gt;
&lt;li&gt;How Bad is It?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Centralizing information about an incident allows others to stay on top of what is happening and maybe even offer additional support. It helps teams track the time of each phase, and it&amp;rsquo;s much easier to get a sense of how severe a problem is. What urgency, support, or resources are required? Teams can work much more effectively if they are looking at the same information.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s begin by 
&lt;a href=&#34;/post/tracking-when-we-knew-about-a-problem/&#34;&gt;Tracking &lt;strong&gt;When&lt;/strong&gt; We Knew About a Problem&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tracking What Is Being Done About a Problem</title>
      <link>/post/tracking-what-is-being-done-about-a-problem/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/tracking-what-is-being-done-about-a-problem/</guid>
      <description>&lt;p&gt;&lt;strong&gt;What (&lt;em&gt;if anything&lt;/em&gt;) is being done?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Is everyone assuming someone else is looking into it?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;These types of questions emerge as additional people join to assist.&lt;/p&gt;
&lt;p&gt;When alerts are sent to distribution lists or general chat rooms it&amp;rsquo;s easy for them to be lost or at best delayed of action.&lt;/p&gt;
&lt;p&gt;As first responders begin to assess the problem, conversations take place in persistent group chat tools such as Microsoft Teams or Slack.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s in these conversations where the most up to date and accurate information will exist about what is taking place during an active incident.&lt;/p&gt;
&lt;p&gt;Responders will share information as it becomes available including who and what is being done. By scrolling back through the conversation, others can quickly get an update not only on what is being done in the moment, but what has already taken place.&lt;/p&gt;
&lt;p&gt;This saves engineers from duplicating effort, including pausing their effort to recover from the problem in order to update a ticket or send an email.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll talk more about a technique of managing tasks from withing chat known as chatops.&lt;/p&gt;
&lt;p&gt;Severity and impact of incidents will vary but it&amp;rsquo;s important teams are 
&lt;a href=&#34;/post/tracking-impact-of-a-problem/&#34;&gt;Tracking Impact of a Problem&lt;/a&gt; in order to spot trends as well as share details to stakeholders as services are restored.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tracking When We Knew About A Problem</title>
      <link>/post/tracking-when-we-knew-about-a-problem/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/tracking-when-we-knew-about-a-problem/</guid>
      <description>&lt;p&gt;Is this a new incident?&lt;/p&gt;
&lt;p&gt;If we are trying to reduce the time it takes to recover from incidents, we will need to start capturing when we are aware of issues.&lt;/p&gt;
&lt;p&gt;By examining incidents in phases, we can look for improvements in specific areas such as Detection. If we start capturing when we knew about a problem, patterns will emerge over time on what could be done to know sooner.&lt;/p&gt;
&lt;p&gt;Kowing the Time to Detection (TTD) allows us to establish the beginning of our awareness. Next, we want to start 
&lt;a href=&#34;/post/tracking-how-we-knew-about-a-problem/&#34;&gt;Tracking &lt;strong&gt;How&lt;/strong&gt; We Knew About a Problem&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tracking Who Knows About a Problem</title>
      <link>/post/tracking-who-knows-about-a-problem/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/tracking-who-knows-about-a-problem/</guid>
      <description>&lt;p&gt;&lt;em&gt;Am I the First to Know?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;When the primary responder acknowledges an incident, they are announcing their awareness of the alert.&lt;/p&gt;
&lt;p&gt;They may not have additional information yet, but like an alarm clock, they have to press the stop button and take action. This not only stops any continued alarms but indicates to others that YES, someone is aware and looking in to the problem.&lt;/p&gt;
&lt;p&gt;Ok. Who else is aware? Do the right people know there is a problem?&lt;/p&gt;
&lt;p&gt;And let&amp;rsquo;s say others are aware&amp;hellip; ?&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/tracking-what-is-being-done-about-a-problem/&#34;&gt;Tracking &lt;strong&gt;What&lt;/strong&gt; Is Being Done About a Problem&lt;/a&gt; is the next thing we&amp;rsquo;ll talk about.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Actionable Alerts</title>
      <link>/post/understanding-actionable-alerts/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/understanding-actionable-alerts/</guid>
      <description>&lt;p&gt;To create effective actionable alerts, you must understand their components and
characteristics. Actionable alerts have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Simplicity&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Scope&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Context&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Simplicity is self-explanatory: make your alerts easy for you and others to
understand, even if you’re reading them after being awakened at 2:00 a.m. Scope
and context should be included in the content of the alert.&lt;/p&gt;
&lt;p&gt;Let’s look at some elements that an actionable alert should always include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The source:&lt;/strong&gt; information about where the alert is coming from. Many
organizations have multiple monitoring systems in use at any one time and a
large number of interconnected systems. It can save someone a tremendous
amount of time if the alert says &amp;ldquo;This alert for payroll system thx-1138 is
coming from Azure monitor subscription prod.&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt; information about what expectation has been violated. For
example, &amp;ldquo;This server has been returning an error 30% of the time when it
should have been returning errors less than 1% of the time.&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Impact and scope:&lt;/strong&gt; information about the effect or impact the situation
has had or potentially will have and the scope of that impact (ideally,
stated from the customer’s point of view).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Recommended action:&lt;/strong&gt; if possible, the alert should include what the
person responding should do next, even if that is a pointer to a
troubleshooting guide or some other documentation to find help in diagnosing
and remediating this problem.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Including such helpful context will make operations practices around monitoring
more sustainable and make responding to alerts easier.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/defining-incidents&#34;&gt;Defining Incidents&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding How Complex Systems Fail</title>
      <link>/post/understanding-how-complex-systems-fail/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/understanding-how-complex-systems-fail/</guid>
      <description>&lt;p&gt;You must “learn to learn” from failure not in case your systems fail, but
because it’s a certainty that your systems &lt;em&gt;will&lt;/em&gt; fail.&lt;/p&gt;
&lt;p&gt;In the IT world, the majority of systems we work with today – especially in a
cloud environment – are complex. They’re composed of many interconnecting parts
that have to work together, and the behavior of the overall system comes from
the interaction of those parts, as much as from the individual parts themselves.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Reliability&lt;/em&gt; is the thread that runs throughout this learning path, but complex
systems are never one hundred percent reliable. Such systems behave in
interesting and counterintuitive ways. The complexity of these systems means
there are inevitably minor flaws within them, and it is difficult or impossible
to predict how minor flaws can join together to produce a significant incident.&lt;/p&gt;
&lt;p&gt;For a more in-depth discussion of this topic, a good resource is the paper
titled &lt;em&gt;How Complex Systems Fail&lt;/em&gt; by Dr. Richard I. Cook with the Cognitive
Technologies Laboratory at the University of Chicago. His “short treatise on the
nature of failure” explains the causative factors that are common to the
failures of complex systems of all types, in all fields.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://web.mit.edu/2.75/resources/random/How%20Complex%20Systems%20Fail.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link: How Complex Systems
Fail&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Some of his key points are particularly relevant to the incident analysis and
post-incident review process:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Complex systems contain changing mixtures of failure latent within them.&lt;/strong&gt;
It is impossible for your systems to run without multiple flaws being
present. The failures change constantly because of changing technology, work
organization, and efforts to eradicate failure. Your system is never
functioning perfectly.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Complex systems run in degraded mode.&lt;/strong&gt; Complex systems are always running
as “broken” systems. They keep “working” in that state because they contain
many redundancies, and people can keep them functioning despite the presence
of many flaws. System operations are dynamic, with components continually
failing and being replaced.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Catastrophe is always just around the corner.&lt;/strong&gt; The complexity of these
systems means major system failures are – in the long term – unavoidable.
Complex systems always possess the potential for catastrophic failure, and
it can happen at any time. It is impossible to eliminate this potential
because it’s part of the inherent nature of the system.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
&lt;a href=&#34;/post/distinguishing-prevention-and-preparation/&#34;&gt;Distinguishing Prevention and Preparation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Service Level Indicators</title>
      <link>/post/understanding-service-level-indicators/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/understanding-service-level-indicators/</guid>
      <description>&lt;p&gt;
&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/site-reliability-engineering/?wt.mc_id=oncalllife-blog-jahand&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Site Reliability Engineering&lt;/a&gt; uses SLIs and SROs to measure the aspects of reliability that you learned about in Unit 2: availability, latency, throughput, coverage, correctness, fidelity, freshness, and durability, and whether you are
meeting expectations in each applicable area.&lt;/p&gt;
&lt;h2 id=&#34;what-to-measure&#34;&gt;What to measure&lt;/h2&gt;
&lt;p&gt;The first question to ask in relation to the aspect you want to measure is &lt;em&gt;what&lt;/em&gt; to measure.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example #1: Measure availability&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;How would you determine the availability of a web server?&lt;/p&gt;
&lt;p&gt;You can do this by measuring the number of HTTP calls the server received and the number to which it successfully responded. This ratio of successful calls to total calls gives you an understanding of the server’s reliability. Multiplying the ratio by 100 gives you a percentage.&lt;/p&gt;
&lt;p&gt;$$
\ [\frac {success}{total}] = ratio
$$&lt;/p&gt;
&lt;p&gt;example&lt;/p&gt;
&lt;p&gt;$$
\ [\frac {800}{1000}] = .8
$$&lt;/p&gt;
&lt;p&gt;For example, if the ratio is 0.8 and you multiply by 100, you can conclude that the web server has been available only 80% of the time.&lt;/p&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;p&gt;$$
\ .8 * 100 = 80 percent
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example #2: Measure latency&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To get an idea of the latency of the web service, you can measure the number of operations that were completed in fewer than 10 milliseconds against the number of total operations.&lt;/p&gt;
&lt;p&gt;If, for example, dividing the first number by the second gives you a ratio of 0.8, and you multiply this by 100, your service has an 80% success rate by this measurement.&lt;/p&gt;
&lt;h2 id=&#34;where-to-measure&#34;&gt;Where to measure&lt;/h2&gt;
&lt;p&gt;To have a clear picture of your reliability based on SLIs, you need to know not only what was measured, but also &lt;em&gt;where&lt;/em&gt; the measurement was taken.&lt;/p&gt;
&lt;p&gt;For example, in measuring the availability of the web server above, you need to specify whether the number of calls was measured at the load balancer or at the server. Likewise, when measuring the latency of the web service, you should note that the number of operations was measured at the client.&lt;/p&gt;
&lt;p&gt;Why is this so crucial? If the purpose is to create a feedback loop in your organization, in which you are able to have conversations about reliability using objective data, it is important for the people having these conversations to be using the same data.&lt;/p&gt;
&lt;p&gt;To use a previous example, if you are trying to determine whether a web service is meeting expectations and one person looks at data collected at the server itself while another looks at data collected at the load balancer in front of that web server, you may be looking at radically different sets of numbers. The
information collected from the web server itself only reflects the traffic that actually reached that server. If there was an issue with the load balancer or the network and half the packets never reached the web server, the two people will have a very different picture of the situation, especially when it comes to
total number of requests.&lt;/p&gt;
&lt;p&gt;This leads to the logical question: where is the best place to measure SLIs?&lt;/p&gt;
&lt;p&gt;Unfortunately, there is not a universally “correct” answer. It’s a decision you must make with an understanding that there are tradeoffs either way.&lt;/p&gt;
&lt;p&gt;The key thing to keep in mind is based on our “prime directive” of reliability measurement: &lt;em&gt;Reliability should be measured from the customer’s perspective.&lt;/em&gt;
Thus, most of the time, you should measure at the point that most accurately reflects the customer’s experience.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/building_service_level_indicators_with_log_analytics/&#34;&gt;Building Service Level Indicators (SLIs) with Log Analytics&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Service Level Objectives</title>
      <link>/post/understanding-service-level-objectives/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/understanding-service-level-objectives/</guid>
      <description>&lt;p&gt;Now you know how to measure reliability using SLIs, but the ratios and
percentages that you’ve calculated only get you halfway toward fulfilling the
goal of site reliability engineering. You can now say the web server in our
example is 50% reliable, but is that the &lt;em&gt;appropriate level of reliability&lt;/em&gt; as
discussed in our definition of SRE?&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s also useful to know the period of time to which that reliability level
applied. For how long was that 50% success rate maintained?&lt;/p&gt;
&lt;p&gt;To answer these questions, you need to look at Service Level Objectives. SLOs
are statements of the objective you have from a reliability standpoint, based on
customer expectations. The basic recipe for creating an SLO consists of these
ingredients:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The “thing” you’re going to measure –&lt;/strong&gt; number of requests, storage
checks, operations; &lt;em&gt;what&lt;/em&gt; you’re measuring.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The desired proportion&lt;/strong&gt; – for example, “successful 50% of the time,” “can
read 99.9% of the time,” “return in 10ms 90% of the time.”&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The time&lt;/strong&gt; – what timeframe are you measuring: the last 10 minutes, during
the last quarter, in the previous 30 days, etc.?&lt;/p&gt;
&lt;p&gt;Putting these components together and including the important “where”
information, a sample SLO might look like this:&lt;/p&gt;
&lt;p&gt;“90% of HTTP requests as reported by the load balancer succeeded in the last
30 day window.”&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
&lt;a href=&#34;/post/building-service-level-objectives-with-log-analytics/&#34;&gt;Building Service Level Objectives with Log Analytics&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding the Foundations of Incident Response</title>
      <link>/post/understanding-the-foundations-of-incident-response/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/understanding-the-foundations-of-incident-response/</guid>
      <description>&lt;p&gt;The foundations of building reliable systems including a good incident response plan, have to start with determining “Who is expected to respond to problems?” and “How do let them know?”.&lt;/p&gt;
&lt;p&gt;The best place to start, is to design what is to establish roles, rosters, and rotations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Roles :&lt;/strong&gt; Well defined responsibilities and expectations of individuals on an on-call team (or roster). The &lt;strong&gt;Primary Responder&lt;/strong&gt;, for example.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Rosters :&lt;/strong&gt; A group of individuals, each with their own assigned role and understood responsibilities and expectations. The mobile &lt;em&gt;&amp;ldquo;on-call&amp;rdquo;&lt;/em&gt; team, consisting of multiple members each with thier own assigned role.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Rotations :&lt;/strong&gt; A scheduled shift for individuals where they are &lt;em&gt;&amp;ldquo;on-call&amp;rdquo;&lt;/em&gt; for a defined period of time. A 24 x n rotation where someone is the** Primary Responder**, for example.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s worth looking at each of these groups a little closer, so let&amp;rsquo;s do that now.&lt;/p&gt;
&lt;h2 id=&#34;next-steps&#34;&gt;Next steps&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;/post/establishing-oncall-roles/&#34;&gt;Establishing On-call Roles&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding the Full Lifecycle of an Incident</title>
      <link>/post/understanding-the-full-lifecycle-of-an-incident/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/understanding-the-full-lifecycle-of-an-incident/</guid>
      <description>&lt;p&gt;If we start to think of incidents as a normal part of a system, then we can also build some formality around the patterns and practices we inevitably see when people instinctually do what they do when something goes wrong.&lt;/p&gt;
&lt;p&gt;From the beginning of a problem to analyzing what and how things happened, we can measure them independantly of each other. By doing so, we can look for improvements in each phase.&lt;/p&gt;
&lt;p&gt;For example, monitoring systems may be working as expected but because an alert was sent to a email distribution group, once people were aware of the alert, most assumed someone else was investigating the problem. The problem persisted for hours.&lt;/p&gt;
&lt;p&gt;An incident can be divided into 5 phases. Detection, response, remediation, analysis, and readiness.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://jhandcdn.blob.core.windows.net/blob/LifecycleOfAnIncident.png&#34; alt=&#34;Lifecycle Of An Incident&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Phase 1 - Detection:&lt;/strong&gt;
A problem has been detected through various tooling and practices&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Phase 2 - Response:&lt;/strong&gt;
A coordinated effort to get the right people and tooling in place to diagnose, theorize, and triage.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Phase 3 - Remediation:&lt;/strong&gt;
Efforts made to change the system to either restore service or confirm theories.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Phase 4 - Analysis:&lt;/strong&gt;
Post-incident retrospective exercise to understand the the full the lifecycle of the incident including the human response.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Phase 5 - Readiness:&lt;/strong&gt;
Implementing knew knowledge and changes to improve and shorten the time and effects of future incidents.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s touch on the detection phase just a little more in depth. This is often the best place to start improving your incident response practices. Solid monitoring is the foundation of building reliable systems.&lt;/p&gt;
&lt;h2 id=&#34;next-steps&#34;&gt;Next steps&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;/post/identifying-the-incident-detection-phase/&#34;&gt;Identifying the Detection Phase of an Incident&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding the Measurements of Reliability</title>
      <link>/post/understanding-measurements-of-reliability/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/understanding-measurements-of-reliability/</guid>
      <description>&lt;p&gt;Now that we have a better idea on the different things that we could be measuring, let&amp;rsquo;s talk about what to do with the data.&lt;/p&gt;
&lt;p&gt;Operational practices to create reliability feedback loops known as service level indicators and objectives help transform the reliability discussion.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Service Level Indicators&lt;/strong&gt; are the measurements that you use to determine
whether you have reached those goals; in other words, the &lt;em&gt;indicators&lt;/em&gt; that
your service is behaving reliably.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Service Level Objectives&lt;/strong&gt; are simply the goals that you’ve set, based on
the &lt;em&gt;appropriate level of reliability&lt;/em&gt; that you want to reach.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can use SLIs and SLOs to set appropriate goals based on the different aspects of reliability and to determine whether you are meeting those goals, using simple calculations and a basic recipe.&lt;/p&gt;
&lt;p&gt;Now you’ll find out how to actually measure the success of your reliability efforts using Service Level Indicators (SLIs) and Service Level Objectives (SLOs).&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/understanding-service-level-indicators/&#34;&gt;Understanding Service Level Indicators (SLI)&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Why We Learn From Incidents</title>
      <link>/post/understanding-why-we-learn-from-incidents/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/understanding-why-we-learn-from-incidents/</guid>
      <description>&lt;p&gt;Incident response doesn’t stop when the incident is over. They say those who
don’t study history are doomed to repeat it. Likewise, those who don’t study,
analyze, and learn from the incidents they resolved are doomed to keep repeating
the process, as well.&lt;/p&gt;
&lt;p&gt;Your most important means of learning from incidents is the post-incident
review.&lt;/p&gt;
&lt;p&gt;When an incident occurs, your first reaction probably isn’t, “Hurray – a
learning opportunity!” Your immediate priority is figuring out what went wrong
and fixing it as quickly as possible, to reduce the impact on your customers and
end users – as it should be.&lt;/p&gt;
&lt;p&gt;However, once the incident has been resolved, it’s important to follow up and
benefit from the discovery of whatever mistakes or circumstances led to the
failure. Doing so will help you prevent the same thing from happening again and
will also help you understand what tactics do and don’t work best when
responding to any kind of incident in the future.&lt;/p&gt;
&lt;p&gt;The post-incident review is part of the analysis phase of the incident response
lifecycle. Not all post-incident reviews are created equal. There are different
ways to approach the process, and too much focus on certain aspects of the
problem or framing questions in the wrong way can reduce the value of the
review.&lt;/p&gt;
&lt;p&gt;We want to now explore the best path towards generating actionable introspection about the systems we co-create.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;/post/understanding-how-complex-systems-fail/&#34;&gt;Understanding How Complex Systems Fail&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Updating Stakeholders</title>
      <link>/post/updating-stakeholders/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/updating-stakeholders/</guid>
      <description>&lt;p&gt;It&amp;rsquo;s important that internal teams are aware of what&amp;rsquo;s happening when an incident occurs. If we don’t provide consistent updates, stakeholders will start coming around and asking.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s also important to acknowledge awareness, status, and expectations to stakeholders beyond the internal groups of an organization. Customers, users, fans, and anyone relying on the availability of your service will want to know what&amp;rsquo;s going on when an incident begins to impact them.&lt;/p&gt;
&lt;p&gt;They have every right to this information, but we&amp;rsquo;ve got to find a better way to make them aware of an issue and what is being done about it.&lt;/p&gt;
&lt;h2 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;Just letting others know that someone is aware of the situation and &amp;ldquo;on it&amp;rdquo; can go a long way in easing the anxiety of others who may be impacted by an incident. Hearing from the provider themselves that an issue has been detected and is being investigated is almost always enough to satisfy customers and users when they first discover something isn&amp;rsquo;t working as expected.&lt;/p&gt;
&lt;p&gt;When customers are aware of an issue and it&amp;rsquo;s not clear if the provider knows about it or is doing anything it can sour the relationship.&lt;/p&gt;
&lt;p&gt;Simply acknowledging an incident provides context to everyone else that action is taking place.&lt;/p&gt;
&lt;p&gt;Once we have acknowledge awareness of an incident we need to begin providing additional information as it becomes available. In early moments and minutes of an incident it may not be clear what the problem (or problems) is.&lt;/p&gt;
&lt;p&gt;Even if there isn&amp;rsquo;t any information to share yet, that in and of itself is the message that should be communicated. It&amp;rsquo;s important to be clear in what is known and not known.&lt;/p&gt;
&lt;h2 id=&#34;clear&#34;&gt;Clear&lt;/h2&gt;
&lt;p&gt;We should be clear in presenting what we know, what is being done and what kind of expectations should they have in terms of when they&amp;rsquo;re going to hear back from us?&lt;/p&gt;
&lt;h2 id=&#34;expectations&#34;&gt;Expectations&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;/post/managing-tasks-from-group-chat-chatops/&#34;&gt;ChatOps&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
